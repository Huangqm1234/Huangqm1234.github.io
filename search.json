[{"title":"CPPæ‹¾é—","url":"/2025/02/11/CPP%E6%8B%BE%E9%81%97/","content":"å¤šæ€\nåŠ¨æ€å¤šæ€æ˜¯åŸºäºè™šå‡½æ•°çš„ã€‚æ²¡æœ‰è™šå‡½æ•°ï¼Œå°±æ²¡æœ‰åŠ¨æ€å¤šæ€ã€‚\nå¤šæ€çš„å…·ä½“å®ç°æ˜¯ç”±ç¼–è¯‘å™¨åœ¨ç¼–è¯‘çš„æ—¶å€™å®ç°çš„ï¼Œæ‰€ä»¥è¢«ç§°ä¸ºåŠ¨æ€å¤šæ€ã€‚ä¼ªä»£ç å¦‚ä¸‹ï¼š\nA()&#123;\tvptr = &amp;A::vftable;&#125;\n\n\né—®é¢˜1ï¼šå¤šæ€çš„å…·ä½“å½¢å¼ã€‚\nç­”æ¡ˆï¼šçˆ¶ç±»æŒ‡é’ˆæŒ‡å‘å­ç±»å¯¹è±¡ï¼Œæˆ–è€…å¯¹å­ç±»å¯¹è±¡çš„å¼•ç”¨ã€‚\nclass Base&#123;public:\tvirtual void myvirfunc() &#123;&#125;&#125;;int main()&#123;\tBase* pa = new Base();\tpa-&gt;myvirfunc(); // å¤šæ€\tBase base;\tbase.myvirfunc();// ä¸æ˜¯å¤šæ€\tBase* ybase = &amp;base;\tybase-&gt;myvirfunc();// å¤šæ€\t\treturn 0;&#125;\nbase.myvirfunc()ä¸æ˜¯å¤šæ€çš„åŸå› åœ¨äºï¼Œå…¶å¯¹åº”çš„æ±‡ç¼–ä»£ç å¦‚ä¸‹ï¼š\n00007FF6DB741BDC  lea         rcx,[base]  00007FF6DB741BE0  call        Base::myvirfunc (07FF6DB741073h)  00007FF6DB741BE5  nop  \nè¿™é‡Œçš„è°ƒç”¨æ˜¯ç¡¬ç¼–ç çš„ï¼Œæ„å‘³ç€ç¼–è¯‘æ—¶å·²ç»ç¡®å®šè°ƒç”¨å“ªä¸ªå‡½æ•°ã€‚è€Œybase-&gt;myvirfunc();çš„æ±‡ç¼–ä»£ç å¦‚ä¸‹ï¼š\n00007FF6DB741BEE  mov         rax,qword ptr [ybase]  00007FF6DB741BF2  mov         rax,qword ptr [rax]  00007FF6DB741BF5  mov         rcx,qword ptr [ybase]  00007FF6DB741BF9  call        qword ptr [rax]  00007FF6DB741BFB  nop \nè¿™é‡Œæ˜¯åœ¨åŠ¨æ€çš„é“¾æ¥ï¼Œè°ƒç”¨çš„æ˜¯vptræŒ‡å‘çš„è™šå‡½æ•°è¡¨ã€‚\n\n\n\n\n","categories":["Programming languages","C++"],"tags":["C++"]},{"title":"ClassNote_CIS5190","url":"/2025/10/08/ClassNote_CIS5190/","content":"ML Design Choices\nhypothesis class  +  loss function  +  optimizer  +  hyperparameters  +  features + â€¦â€¦\n\n\nhypothesis class (aka model family):\nlinear functions, polynomial functions, decision tree â€¦â€¦\nFrom which the ML black-box must pick one to fit the data.\n\n\nloss function:\nMSE,\nevaluate the quality of the fit.\n\n\noptimizer:\nhow to search the function that best fits the data.\n\n\nLinear Regression\nLinear Functions\nfÎ²(x)=Î²âŠ¤x=[Î²1â‹¯Î²d][x1â‹®xd]=Î²1x1+â‹¯+Î²dxdxâˆˆRd is called an input or features or covariates.\nÎ²âˆˆRd is called the parameters or parameter vector.\ny^=fÎ²(x) is called predicted (label) or output or response.\nLoss Function\nLoss function is a scalar function. A good loss function should expresses all the performance metrice you cared about and is convenient for machine learning to compute.\nMean Squared Error (MSE)\nL(Î²;Z)=1nâˆ‘i=1n(yiâˆ’Î²âŠ¤xi)2Linear Regression Problem\n\n\nInput: Dataset Z={(x1,y1),Â ...Â ,(xn,yn)}, where xiâˆˆRd and $ y_i \\in \\mathbb{R}$.\n\n\nCompute:\nÎ²^(Z)=argâ¡minÎ²âˆˆRdL(Î²;Z)=argâ¡minÎ²âˆˆRd1nâˆ‘i=1n(yiâˆ’Î²âŠ¤xi)2Finding an appropriate Î² is also a process of finding an appropriate function in the hypothesis class.\n\n\nOutput: A linear function fÎ²^(Z)(x)=Î²^(Z)Tx such that yiâ‰ˆÎ²^(Z)Txi.\n\n\nFrom another point, we are trying to find out a function fÎ², which can approximate the â€˜true functionâ€™ fâˆ— that produces the real date and is unknown, in the model family F.\n\nmost of time, fâˆ— not in our model family F\nLinear Regression With Feature Maps\nFeature Maps\nIn order to enable the linear model to fit nonlinear data, the feature map is introduced.\nÏ•:xâ†’Rdâ€² means transforming the original input x (from the space X with dimentsion d) into a new feature vector Ï•(x) (with a dimension of dâ€²). d and dâ€² can be different.\nIncreasing dâ€² means increasing modelâ€™s capacity and it also means decreasing bias but increasing variance. Thus, we need to construct Î¦ to balance trade-off between bias and variance.\nThere is a rule of thumb: n=dâ€²logdâ€², n is number of samples.\nThen F={fÎ²(x)=Î²TÏ•(x)}. And  $L(\\beta; Z) = \\frac{1}{n} \\sum_{i=1}^{n} \\left( y_i - \\beta^{\\top} \\phi(x_i) \\right)^2 $\n\n\nFor example:\n\n\nxâˆˆR1\nAssume the true function is y=2x2+3x+5.\nDefine the feature map: Ï•(x)=[1xx2]\nApply the linear model to the mapped features: fÎ²(x)=Î²âŠ¤Ï•(x)=Î²0â‹…1+Î²1â‹…x+Î²2â‹…x2\nLoss function: L(Î²;Z)=1nâˆ‘i=1n(yiâˆ’(Î²0+Î²1xi+Î²2xi2))2\nBy minimizing the loss function, the following can be obtained: Î²^=[532]\n\n\nxâˆˆRn n=2\nx=[x1x2]\nThe true function is y=3x12+2x1x2+5x2+4\nÏ•(x)=[1x1x2x12x1x2]\nfÎ²(x)=Î²âŠ¤Ï•(x)=Î²0â‹…1+Î²1â‹…x1+Î²2â‹…x2+Î²3â‹…x12+Î²4â‹…x1x2\nL(Î²;Z)=1nâˆ‘i=1n(yiâˆ’(Î²0+Î²1xi1+Î²2xi2+Î²3xi12+Î²4xi1xi2))2\nÎ²^=[40532]\n\n\nother examples\n\n\nÏ•(x)=[1Â x1Â x2Â ...Â xd]T\nThe first element 1 is for the intercept.\nIn this way, Rdâ†’Rd+1\n\n\nxâˆˆ{â€™high schoolâ€™,\"college\",\"master\"}. Ï•(x) maps to 1,2,3\n\n\nx is the message such as â€œthe food was goodâ€. Ï•(x) maps to [1(\"good\"âˆˆx)Â Â 1(\"bad\"âˆˆx)Â Â ...]T\n\n\n\n\n\n\nAlgorithm for Non-linear Regression\n\n\nSelect an appropriate feature map:\nÏ•(x)=[Ï•1(x)...Ï•dâ€²(x)]The number of features should be constrained, because more features means more expressive hypothesis space. In this way, the model will be overfitting.\n.ywbvbmguudvm{zoom:80%;}\n\n\nCompute Ï•i=Ï•(xi) for each xi in Z.\n\n\nRun linear regression with Zâ€²={(Ï•1,y1),...,(Ï•n,yn)}\n\n\nTraining/Test Split\nRandomly shuffle data first, and then split data Z into Ztrain and Ztest. Run linear regression with Ztrain to obtain Î²^(Ztrain).\nLtrain=L(Î²^(Ztrain);Ztrain)Ltest=L(Î²^(Ztrain);Ztest)For overfitting: Ltrain is small and Ltest is large. For underfitting, they both are pretty large.\nThe reason why it was okay to just randomly sample Ztrain and Ztest from the same Z is Independent and Identically Distributed(i.i.d). That is a strong assumption.\nBias and Variance: way to assess overfitting and underfitting\nConsider you have k training datasets drawn from the same i.i.d over the data, and each time fit your model {fÎ²^1:k} to it. That is to say, you have k different functions.\n\n\nVariance: how much do those fitted functions differ amongst each other, on average?\n\n\nBias: how much does the average of all those fitted functions deviate from the â€œtrueâ€ function over the data distribution?\n\n\n.omjwrwylxooi{zoom:80%;}\nupper right means overfitting, left lower means underfitting\nThat also can be applied to the real functions.\n.wqattfjgvwmj{zoom:80%;}\nLow Variance means model is not sensitive to noises. Then, this model is good for fewer/worse data. Low Bias means model has a high capacity. Then, this model is good for more/better data.\nEffective Bias/Variance Under Varying Dataset Sizes\nWhen the training dataset becomes larger, what will happen to Bias and Variance?\n\n\nBias: Bias will not be impacted by the size of trainning dataset. The essence of bias lies in the limitations of the expression in the hypothesis space. Increasing the amount of data can make the model get closer to the optimal model within  the hypothesis space, but it can not break through the limitation of the space.\n\n\nVariance: Variance will be lower. Increasing data will provide a stronger control to the functions.\n\n\nWhen expanding the hypothesis class (similar to adding more dimensions to the features), what will happen to Bias and Variance?\n\n\nBias: Expanding the hypothesis class means the model has higher capacity. That is to say, Bias will be lower.\n\n\nVariance: Bias being lower means Variance being higher.\n\n\nRegularization: way to fix underfitting and overfitting\n\n\nSolutions to fix underfitting and overfitting\n\nImprove the trainning dataset -&gt; collect more data.\nChoose the right model family -&gt; not too simple, not too complex.\nChoose the right loss function -&gt; MSE + a term which can control the fit complexity.\n\n\n\nLinear regression with L2 regularization:\nL(Î²;Z)=1nâˆ‘i=1n(yiâˆ’Î²Txi)2+Î»âˆ¥Î²âˆ¥22=1nâˆ‘i=1n(yiâˆ’Î²Txi)2+Î»âˆ‘j=1dÎ²j2Î» is a hyperparameter, Î»&gt;=0, and Î» should between [10âˆ’3,103]\nIf using intercept term Ï•(x)=[1,x1,â€¦,xn]T, no penalty on the bias parameter Î²1=1:\nL(Î²;Z)==1nâˆ‘i=1n(yiâˆ’Î²Txi)2+Î»âˆ‘j=2dÎ²j2sum from j=2\nskipping the penalty on the bias term ensures regularization focuses on taming feature-related complexity (to prevent overfitting) without harming the modelâ€™s ability to fit the dataâ€™s overall trend (to prevent underfitting).\nIntuition on L2 Regularization\nConsider a feature map with d=10 features [x1,...,xd]. Suppose during linear regression we forced Î²j=0 for all j&gt;5. This is exactly equal to choosing a smaller-capacity hypothesis class, induced by the smaller feature maps with d=5.\nThus a large Î» will pull coefficients towards 0, which let the function have a low capacity.\n.pgrluverfnbg{zoom:80%;}\nL2 regularized regression amounts to preferring smaller weights according to Gaussian Probability Density Function (Gaussian pdf).\n.neeoymxvzevy{zoom:80%;}\nWhen the Î» becomes larger, the plot of Gaussian pdf becomes much slimmer.\n\nAt the â€œfull lossâ€ point, the â€œpull force of the fitted dataâ€ and the â€œpull force of the simplified weightsâ€ counterbalance each other, achieving stability. When Î» becomes larger, the â€œpull force of the simplified weightsâ€ becomes larger.\n\nâ„¹ï¸ Note\nFor the new parameter Î²newâˆ—=minÎ²Lnew(Î²;Z) (Lnew(Î²;Z)=L(Î²;Z)+Î»R(Î²)), their corresponding value of L(Î²;Z) would be larger than before regularization.\nSince, L(Î²;Z) before regularization is the smallest. Î²newâˆ— will let model be more generalize. Thus, L(Î²newâˆ—;Z) will be larger than L(Î²oldâˆ—;Z) (Î²oldâˆ—=minÎ²Lold(Î²;Z), Lold(Î²;Z)=L(Î²;Z)).\n\nCross-Validation for Hyperparameter Tuning / Model Selection\nHyperparameter Tuning\nNaive strategy to select Î» is by trying a few different candidates Î»i and choose the one that minimizes the test loss. It often make sense to sample uniformly in log-space e.g. 10âˆ’3,10âˆ’2,10âˆ’1,1,101,102,103.\nBut if we directly tune Î» on test dataset, it will contaminate the test dataset. Because test set MSE is meant to be an unbiased noisy estimate of true generalization MSE. But tuning Î» on test set leaks test set info into model. Test set error gets biased to be lower than true generalization error.\n.nwinnvuidiro{zoom:67%;}\noverfitting of hyperparameters on test set\nValidation Set\nThe solution for the problem above is Validation set.\n\n\nNow, the dataset is split into three parts:\n\nTraining set: used to find the optimal parameters.\nValidation set: used to find the optimal Î».\nTesting set: used to get the generalization ability of the model.\n\n\n\nCross Validation\n\n\nBasic algorithm\n\nSplit Z into Ztrian, Zval and Ztest.\nFor tâˆˆ{1,â€¦,h} hyperparameter choices:\n\nRun linear regression with Ztrain and Î»t to obtain Î²^(Ztrain,Î»t).\nEvaluate validation loss Lvalt=L(Î²^(Ztrain,Î»t),Zval)\n\n\nGet the best Î»t\n\nChoose tâ€²=argâ¡mintLvalt with lowest validation loss.\nRe-run linear regression with Ztrain (actually, this Ztrain is not similar with the above Ztrain, this Ztrain means Ztrain+Zvali) and Î»tâ€² to obtain Î²^(Ztrain,Î»t).\n\n\n\n\n\nk-fold algorithm (Z is small)\n.qmhpjxngrlxb{zoom:80%;}\nAssuming k=3, with each Î»t, we can get the validation loss on each Zval. Then, the mean of those loss is the loss of Î»t.\nLvalt=L(Î²^(Ztrain1,Î»t),Zval1)+L(Î²^(Ztrain2,Î»t),Zval2)+L(Î²^(Ztrain3,Î»t),Zval3)3Other steps is similar with basic algorithm.\nAs k becomes bigger, model selection becomes more accurate, but algorithm becomes more computationally expensive.\n\n\nOther Performance Metrics\nEvery loss function is a performance metric, but not every performance metric is a convenient loss function.\n\n\nMean Absolute Error:\n1nâˆ‘i=1n|yi^âˆ’yi|\n\nMean Relative Error:\n1nâˆ‘i=1n|yi^âˆ’yi||yi|\n\nR2 Score:\n1âˆ’MSELabel VarianceHigher is better, R2=1 is perfect.\nVariance=1nâˆ‘i=1N(xiâˆ’Î¼)2\n\nPearson Correlation:\n1nâˆ‘i=1n(yi^âˆ’Î¼^)(yiâˆ’Î¼)Ïƒ^Ïƒ\n\nRank-order correlation:\n\n\nFor any performance  measure, always compare against some simple â€œcommon senseâ€ baselines. The baselines can be: Randomly predicting some sample from the training label; Predicting the mean/median of the training labels; Any other handcoded functions\nOptimizing the MSE Loss\nY=XÎ²Y=[y1â‹®yn]Â Â Â X=[x1â€¦x1,dâ‹®â‹±â‹®xn,1â€¦xn,d]Â Â Â Î²=[Î²1â‹®Î²d]Then, the Loss function is:\nL(Î²;Z)=1nâˆ‘i=1n(yiâˆ’Î²Txi)2=1nâˆ¥Yâˆ’XÎ²âˆ¥22Closed-Form Solution\nThe goal is :\nâˆ‡Î²L(Î²^;Z)=0Then,\nâˆ‡Î²L(Î²;Z)=âˆ‡Î²1nâˆ¥Yâˆ’XÎ²âˆ¥22=âˆ‡Î²1n(Yâˆ’XÎ²)T(Yâˆ’XÎ²)Let A=(Yâˆ’XÎ²), then\nâˆ‡Î²L(Î²;Z)=âˆ‡Î²1nATAThus, L=f(A), A=g(Î²).\nAccording to the the chain rule of scalars against vectors:\nâˆ‡Î²L(Î²;Z)=(âˆ‡Af)Tâˆ‡Î²AFor âˆ‡Af, according to the derivative rule of a scalar with respect to a vector:\nifÂ f=aTa,Â thenÂ âˆ‡af=2a,whereÂ aÂ is a vectorThus,\nâˆ‡Af=2nAFor âˆ‡Î²A, we can get that:\nâˆ‡Î²A=âˆ‡Î²(Yâˆ’XÎ²)=âˆ’XSo,\nâˆ‡Î²L(Î²;Z)=(2nA)T(âˆ’X)=2nAT(âˆ’X)=âˆ’2nX(Yâˆ’XÎ²)TFinally,\nâˆ‡Î²L(Î²;Z)=âˆ’2nXTY+2nXTXÎ²\nğŸ’¡ Tip\nIt also can be written as:\nâˆ‡Î²L(Î²;Z)=1nâˆ‘i=1n2(yiâˆ’Î²Txi)xiFor a single training data sample:\n2n(yiâˆ’Î²Txi)xii.e., the current error yiâˆ’Î²Txi times the feature vector xi.\nThus, Large error samples induce large change to Î². Large feature values induce large change to Î².\n\nSetâˆ‡Î²L(Î²^;Z)=0, we have:\nXTXÎ²^=XTYAssume XTX is invertible, we have:\nÎ²^(Z)=(XTX)âˆ’1XTY\nâ„¹ï¸ Note\nSolution is unique only when XTX is invertible (full rank).\n\n\nwhen feature dimension d is bigger than data examples n.\n\nRemove features or collect more data â†’ nâ‰¥d\n\n\n\nwhen some feature is a linear combination of others.\n\nRemove linearly dependent features.\nUse L2 regularization.\n\n\n\nExample:\n[1122][Î²^1Î²^2]=[24]In this case, any Î²^2=2âˆ’Î²^1 is a solution\n\n\n\nDisadvantages:\n\nComputing (XTX)âˆ’1 is O(d3). (O(nd2) for computing XTX, O(d3) for inverse).\nNumerical accuracy issues due to â€œill-conditioningâ€\n\n\n\nGradient Descent\n\n\nGlobal search: randomly try (Î²t independent of Î²tâˆ’1) â†’ very unstructured, taking a long time.\n\n\nLocal search: Î²t is computed based on Î²tâˆ’1.\n\n\nÎ²t+1=Î²tâˆ’Î±âˆ—âˆ‡Î²L(Î²t;Z)Î± is learning rate (hyperparameter).\nThe gradient is the direction along which L(Î²;Z) changes most quickly as a function of Î².\nFor linear regression loss, it is a convex function, so there is no local minima.\nThe gradient descent will continually repeat until âˆ¥Î²t+1âˆ’Î²tâˆ¥â‰¤Ïµ (convergence).\n\n\nLearning Rate Î± :\nIf Î± is too small, L(Î²;Z) decease slowly. If Î± is too large, L(Î²;Z) stagnates or even increase.\nOne heuristic(å¯å‘æ–¹æ³•) is to decrease Î± over time to force Î² to converge.\n\n\nTime complexity:\nO(nd) for XÎ², O(n) for (XÎ²âˆ’Y), O(nd) for left-multiplying by XT, O(d) for subtracting from Î²t.\nWith T times iteration, the overall time complexity is O(ndT).\n\n\nBatch Gradient Descent &amp; Stochastic Gradient Descent\n\n\nBatch Gradient Descent (BGD)\nÎ²jâ†Î²jâˆ’Î±âˆ‘i=1n2(yiâˆ’Î²âŠ¤xi)xiâˆ€d\nRepeat it for T times, the overall time complexity is O(ndT)\n\n\nStochastic Gradient Descent (SGD)\n\n\nInitialize Î².\n\n\nRandomly shuffle dataset. (To reduce bias and ensure each epoch provides a different, more representative sampling of the data.)\n\n\nRepeat Tâ€² times until convergence{\nFor i=1â€¦n, do\nÎ²jâ†Î²jâˆ’Î±2(yiâˆ’Î²âŠ¤xi)xiâˆ€d\n}\n\n\nThe overall time complexity is O(dTâ€²n). (Repeat Tâ€²n times, O(d) for each update)\n\n\n.fgomvfalemku{zoom:67%;}\nNormally, SGD is faster than BGD because Tâ€²nâ‰ˆ10T.\n\nOptimizing Regularized Linear Regression\nL2-regularized Linear Regression\nThe loss is:\nL(Î²;Z)=1nâˆ‘i=1n(yiâˆ’Î²âŠ¤xi)2+Î»âˆ‘j=1dÎ²j2=1nâˆ¥Yâˆ’XÎ²âˆ¥22+Î»âˆ¥Î²âˆ¥22The gradient is:\nâˆ‡Î²L(Î²;Z)=âˆ’2nXâŠ¤Y+2nXâŠ¤XÎ²+2Î»Î²\n\nClosed form\nSet gradient to be zero, then we can get the optimal parameter:\nÎ²^(Z)=(XâŠ¤X+nÎ»I)âˆ’1XâŠ¤Y\n\nGradient Descent\nThe extra term Î»Î² in the gradient is often interpreted as â€œweight decayâ€. At each update, it decays Î²â†’Î²(1âˆ’2Î±Î»).\n\n\nFeature Preprocessing And Selection\nA data can be measured in centimeters, while it also can be measured in millimeters. Apart from the unit (or to say scaling), there is no different between them.\nWe would like our ML algorithm to not get affected in any substantive / meaningful way by such simple scaling. -&gt; Scale invariance\n\n\nFor unregularized linear regression\nIt is scale invariant. With x is multiplied by k, beta is reduced by 1n.\n\n\nFor regularized linear regression\nThe regularized term depends directly on the parameter scale.\n\n\nFeature Standardization\nTo let solve the problem above, we can use feature standardization, which will rescale all features (except x1=1) to zero mean and unit variance.\nxi,jâ†xi,jâˆ’Î¼jtrainÏƒjtrain\nğŸ“˜ Important\nFor testing dataset, each sample should be scaled by the Î¼jtrain and Ïƒjtrain.\n\nFeature map\nSuch as converting all data to be numeric type.\nFeature Selection\nTo control the size of the hypothesis class\n\n\nSelect features most correlated with label\n\n\n\nGet rid of features that are heavily correlated with each other, to reduce redundancy.\n.hotkesmlgycg{zoom:80%;}\n\n\nHandling Missing Values\n\n\nTodo\n\n\nL1 Regularization (LASSO) And Automatic Feature Selection\n\n\nL0 Regularization\nâˆ¥Î²âˆ¥0=|{j|Î²j=0}|The number of non-zero component.\nAutomatic feature selsection\n\n\nChallenge: âˆ¥Î²âˆ¥0 is not differentiable.\nIf Î² changes from [0,0] to [0.00000001,0], âˆ¥Î²âˆ¥0 changes from 0 to 1. That means a slight change of Î² can cause a huge change in âˆ¥Î²âˆ¥0. Thus we introduce L1 regularization.\n\n\n\n\nL1 Regularization\nâˆ¥Î²âˆ¥1=Î»âˆ‘j=1d|Î²j|\nâ— Caution\nDo not regularize the constant term (intercept).\n\nSolutions arenâ€™t always sparse, but the â€œcornersâ€ on the axes in green contours makes this probable for larger Î». As Î» becomes larger, Î²2 has higher probability to be zero. In practice, if |Î²j|â‰¤Î¼, just set it to zero. Î¼ is threshold.\n.wwbhbckufnch{zoom:80%;}\n\nğŸ’¡ Tip\nQ: when would we get sparse solutions on the positive Î²1 axis? (how would the data fit contours have to change for this?)\nA: Todo\n\nL1 regularization induces a Laplacian prior on the values of Î². Thus, more parameters drawn to 0 and more parameters with large values.\n\n\nâ„¹ï¸ Note\nL2 regularization induces Gaussian prior on the value of Î².\n\n\n\nL1 regularization also can be used to select features:\n\n\nConstruct a lot of features and add to feature map.\n\n\nUse L1 regularization regression to select subset of features. i.e., coefficient Î²jâ‰ 0â†’ feature j is selected.\n\n\nRemove unselected features from the feature map and run vanilla linear regression. (Optional)\n\n\n\n\n\nğŸ“˜ Important\nFor L1 regularized linear regression, gradient descent still works, but there is no closed-form solution.\n\n\n\nSummary\n.jyugiyitlgjl{zoom:67%;}\n\nâ„¹ï¸ Note\nJust to remind, for a ML Design, it has hypothesis class (corresponding to the red part) + loss function (corresponding to the blue part) + optimizer (corresponding to the green part)\n\nLogistic Regression\n\n\nInput: Z={(x1,y1),(x2,y2),â€¦,(xn,yn)},yiâˆˆ{0,1} (Binary Classification)\n\n\nOutput: yiâ‰ˆfÎ²(xi)\n\n\nHypothesis class (model family): linear functions\n\n\nLoss Function: MSE loss\n\n\nOptimizer: Gradient Descent\n\n\nLinear Regression to Binary Classification\nyi=Î²Txiâ†’{PredictÂ yi=classÂ 1Â ifÂ Î²Txiâ‰¥0PredictÂ yi=classÂ 0Â ifÂ Î²Txi&lt;0From continuous to discrete.\n.ccgiesdpvynk{zoom:70%;}\nNeural Networks\n\n\nRepresentation learning: Automatically lean good features for tasks.\ne.g, recognizing casts in photos. Instead of people manually listing features (like â€œfurry tailsâ€), the neural network can lean these useful clues directly from data.\n\n\nDeep learning: Learn multiple levels of representation at increasing levels of complexity.\nThinking it as a tower of understanding.\nThe bottom layer learns simple patterns. The middle layer combines these simple patterns into more complex shapes. The top layer combine those complex shapes into whole object.\n\n\nA Framework Uniting Many ML Hypothesis Class\n\n\nLinear Regression\n\n\nNo bias (actually, there is a bias in input)\n\n\n\nWith bias\n.gwnksalkynsl{zoom:38%;}\n\n\n\n\nLogistic Regression\n\n\nBinary\n.seoheidwxboi{zoom:80%;}\n\n\nMulticlass\n\n\n\n\n\n\nğŸ’¡ Tip\nQ: Is the hypothesis class larger?\nA: No. Because a 2-layers network can be written as:\nfBL=2,BL=1(x)=B1Ã—dâ€²L=2Bdâ€²Ã—dL=1xdÃ—1=W1Ã—dxdÃ—1â€‹    Thus, the set of functions expressible by the 2-layer network is exactly the same as the set expressible by a single linear lager. So the hypothesis class is neither larger nor smaller.\n\nBut, after adding non-linear activation functions, the hypothesis class is changed.\n.lshzpeznrrar{zoom:50%;}\nwhere $g(\\cdot) $ is non-linear activation function. Then the function is changed to:\nfBL=2,BL=1(x)=B1Ã—dâ€²L=2Bdâ€²Ã—dL=1xdÃ—1=W1Ã—dxdÃ—1â†’B1Ã—dâ€²L=2g(Bdâ€²Ã—dL=1xdÃ—1)\n\nAn Example.zbmdndutjjld{zoom:67%;}\nThe input feature x=(x1,x2) is not easy to separate. After the first layer BL=1 and tanh activation, we can get the following:\n.quxyfmonmhfk{zoom:43%;}\nNow, it is much easy to separate red points and blue points. So, the first layers learned a good features from data like manual feature map engineering.\n\n\nNeural Net Building Blocks and Notation\nIn neural nets, it is conventional to refer to parameters as â€œweightsâ€ and denote them as W, w etc. rather than the \\Beta, Î² etc.\nBasic Structure\n.bdiuixxgcpcp{zoom:50%;}\nNeutral networks are made up of nodes or units, connected by links. Links have associated parameters (weights).\n.ouisfxrqevwh{zoom:50%;}\nNeural Networkâ€™s layers are generally homogenous, because within a single layer, the computational units typically share uniform structure, operation rules, or parameter types.\nUnits are organized into layers:\n.jcvcrnpjryjn{zoom:50%;}\nWe can create a deeper network by stacking layer on itself.\nForward Propagation\n.soifzskzdrep{zoom:80%;}\nThe output of the first layer is a1=g1(W(1)xi+b(1)).\nThe output of the second layer is g2(W(2)a1+b(2))=g2(W(2)g1(W(1)xi+b(1))+b(2)) .\nThe final output is $ f(x_{i})=g^{(o)}\\big (â€¦ g_{2}\\big (W^{(2)}\\big (g_{1}\\big (W{(1)}x_{i}+b{(1)}\\big )\\big )+b^{(2)}\\big )+\\cdots b^{(o)} \\big )$.\nSo, each result is computed based on the previous layerâ€™s output.\nLearnable parameter is W=[W(1),b(1),â€¦,W(o),b(o)].\nBackward Pass\nLoss functions\nSame as single-layer models like linear and logistic regression\n\n\nRegression:\n\nMSE\n\n\n\nClassficiation:\n\n\nNegative log-likelihood\n\n\nBinary cross entropy for binary classification.\n\nâ„¹ï¸ Note\nBinary cross entropy is âˆ’[yâˆ—log(p^)+(1âˆ’y)âˆ—log(1âˆ’p^)].\n\n\n\nSoftmax loss for multi-class classification.\n\n\n\n\n\n\nOptimizer\nObjective function:\nWâˆ—=argWâ¡minL(W;Z)\nğŸ’¡ Tip\nQ: Donâ€™t I have to optimize differently for different L(â‹…) ?\nA: No, just use gradient descent. It is most general optimization approach.\nQ: But what if L(â‹…) is non-convex in W?\nA: It almost surely is. Do gradient descent anyway. Just make sure everything is differentiable.\n\nBack Propagation\nBack propagation is for adjusting W. It is based on gradient descent.\n.bvguogycpwcz{zoom:80%;}\nWt+1=Wtâˆ’Î±âˆ‡W=Wtâˆ’Î±dL(fW(x),y)dW|W=WtdL(fW(x),y)dW=[dL(fW(x),y)d[W(1),b(1)],dL(fW(x),y)d[W(2),b(2)],â€¦,dL(fW(x),y)d[W(o),b(o)]]It is difficult to calculate this formula by hand, let alone loss function with regularization term.\nThere is a much easier way to calculate it using computational graph based on chain rule.\nComputational Graph\nLet us take h=logâ¡(x1x2)sinâ¡(x2) as an example.\n\n\n\nBreak down function computation into:\n\n\nData: Input, Output and Intermediate\n\n\nOperator: Addition, Multiplication\nRequirements for each operator: Forward + Backward.\n\n\n\n\nTo update parameters x1,x2, we need to compute the gradient, which is [âˆ‚Lâˆ‚x1,âˆ‚Lâˆ‚x2].\nConsider we use MSE loss, then, we get that: L=(yâˆ’w)2.\nThus,\nâˆ‚Lâˆ‚w=âˆ’2(yâˆ’w)If y=3,w=5, then we get a numerical result âˆ‚Lâˆ‚w=âˆ’2(3âˆ’5)=4.\nThen, according to the chain rule, we can get the expression of âˆ‚Lâˆ‚x1,âˆ‚Lâˆ‚x2:\n{âˆ‚Lâˆ‚x1=âˆ‚Lâˆ‚wâˆ‚wâˆ‚y1âˆ‚y1âˆ‚aâˆ‚aâˆ‚x1âˆ‚Lâˆ‚x2=âˆ‚Lâˆ‚wâˆ‚wâˆ‚y2âˆ‚y2âˆ‚x2+âˆ‚Lâˆ‚wâˆ‚wâˆ‚y1âˆ‚y1âˆ‚aâˆ‚aâˆ‚x2\nğŸ“˜ Important\nThere two paths from w to x2 in the backward graph. The first is wâ†’y1â†’aâ†’x2 and the second is wâ†’y2â†’x2.\n\nSince w=y1y2, then we can get:\nâˆ‚wâˆ‚y1=y2,âˆ‚wâˆ‚y2=y1Since y1=logâ¡(a), then, we can get:\nâˆ‚y1âˆ‚a=1aSince y2=sinâ¡(x2), we can get that:\nâˆ‚y2âˆ‚x2=cosâ¡(x2)Since a=x1x2, we can get that:\nâˆ‚aâˆ‚x1=x2,âˆ‚aâˆ‚x2=x1Thus, we can get the final expression of âˆ‚Lâˆ‚x1,âˆ‚Lâˆ‚x2:\n{âˆ‚Lâˆ‚x1=4âˆ—y2âˆ—1aâˆ—x2âˆ‚Lâˆ‚x2=4âˆ—y1âˆ—cos(x2)+4âˆ—y2âˆ—1aâˆ—x1Since we have got the value of y1,y2,a,x1,x2 from forward propagation, we can get the value of âˆ‚Lâˆ‚x1,âˆ‚Lâˆ‚x2.\nYou also can get an practical example in L09.pdf - page 13\nComputing Rules for Each Nodes\nThe gradient of each nodeâ€™s outputs is with respect to its input.\n\n\nForward pass: y=f(x)\n.clcmlccokhmc{zoom:67%;}\nFor this kind of node (single input, single output), the backward pass is:\nâˆ‡xL=âˆ‡xf(x)âˆ‡yL\n\nForward pass: y=f(x1,x2)\n.qdjvglpdaykl{zoom:70%;}\nFor this kind of node (multiple input, single output), the backward pass is:\n{âˆ‡x1L=âˆ‡x1f(x)âˆ‡yLâˆ‡x2L=âˆ‡x2f(x)âˆ‡yL\n\nForward pass: y=f(x1,x2)\n .rqvfvabyujwy{zoom:67%;}\nFor this kind of node (multiple input, multiple output), the backward pass is:\n{âˆ‡x1L=âˆ‡x1f(x)âˆ‡yLâˆ‡x2L=âˆ‡x2f(x)âˆ‡yLIt is very interesting that we sum the gradient of L from the two paths. The reason for that is as follow.\nLet us consider L=h(g1(y),g2(y)). Then, Î”L is\nÎ”Lâ‰ˆâˆ‚hâˆ‚g1Î”g1+âˆ‚hâˆ‚g2Î”g2Since\ng1â€²(y)=limÎ”yâ†’0Î”g1Î”yâ‡’Î”g1â‰ˆg1â€²(y)Î”ywe can get:\nÎ”g1â‰ˆg1â€²(y)Î”y,Î”g2â‰ˆg2â€²(y)Î”yThen,\nÎ”Lâ‰ˆ(âˆ‚hâˆ‚g1g1â€²(y)+âˆ‚hâˆ‚g2g2â€²(y))Î”yThus,\ndLdy=âˆ‚hâˆ‚g1g1â€²(y)+âˆ‚hâˆ‚g2g2â€²(y)\n\nLet us back to Neural Network, each layer can be seen as a node in a computational graph that receives two kinds of inputs:\n\n\nprevious activations, or to say previous layerâ€™s output\n\n\nparameters\n\n\nand each node (layer) emits one output.\n.gntvicaocdof{zoom:67%;}\n.foxlurhfbriv{zoom:65%;}\nLet us take Layer 4 as an example to analyze the gradient descent in the Neural Network.\nThe input of Layer 4 is a3 (the output of Layer 3) and W(4),b(4). The output of Layer 4 is a3. Thus, it is a multiple input single output node (i.e., y=f(x1,x2)).\n.islsrhbdeswm{zoom:67%;}\nThus,\n{âˆ‡a3L=âˆ‡a3f(x)âˆ‡a4Lâˆ‡W(4),b(4)L=âˆ‡W(4),b(4)f(x)âˆ‡a4LIn practice, we often separate gradient computation and update.\n\nğŸ“˜ Important\nNeural Network layers must:\n\n\nPerform differentiable operations, thus gradient exist.\n\n\n\nTodo Module8_part06_Backpropagation_in_detail_by_hand\nThe Neural Net Toolbox\nOptimization\n\n\nChallenges\n\nLocal minima\nSaddle points\nIll-conditioning\nExploding/ Vanishing gradients\n\n\n\nMini-Batch SGD\nFor the formal SGD, we calculate the gradient using one sampel at each step until traversing the entire dataset (epoch). But, in the mini-batch SGD, we calculate the gradient using a group of samples at each step until traversing the entire dataset (epoch).\nflowchart TD\n    A[Initialize Weights W] --> B[Shuffle Dataset]\n    B --> C{Pass Over Full Dataset?}\n    C -- No --> D[Get Next Batch X_t, y_t]\n    D --> E[Compute Gradient dL/dW]\n    E --> F[Update W: W_new = W_old - \\alpha * dL/dW]\n    F --> C\n    C -- Yes --> G[End Epoch]\nA group of samples can give a more representative result of the loss with respect to the full dataset\nMomentum Gradient Descent\nIn momentum GD, we induces Ï. It is a momentum variable (inertia) that remembers past update. So, we need to update momentum and parameters simultaneously.\nUpdate momentum:\nÏ=Î¼Ïâˆ’Î±âˆ‡W|W=Wtwhere Î¼ is a hyperparameter, which should be tried around 0.9 or 0.99.\nUpdate parameters:\nWt+1=Wt+Ï.rmpllwtuligr{zoom:50%;}\n\n\nRed arrow : âˆ’Î±âˆ‡W|W=Wt\n\n\nGreen arrow: Î¼Ï\n\n\nPurple arrow: Ï\n\n\nMomentum GD uses both the current gradient and history of past gradients (via Ï), so it is less affected by â€œirregular bumpsâ€ in the loss function.\n\n\nNesterov Momentum\nUpdate momentum:\nÏ=Î¼Ïâˆ’Î±âˆ‡W|W=Wt+Î¼ÏThat is different with normal momentum. It adds a â€œlookahead stepâ€. It compute the gradient not at the current position Wt, but at the position youâ€™d be in after applying the momentum from the previous step Wt+Î¼Ï.\nUpdate parameters:\nWt+1=Wt+ÏThat is similar with normal momentum GD.\n\n\n\nGreen arrow: represents the inertia from past updates (Î¼Ï).\n\n\nRed arrow: Instead computing the gradient at the current red dot, Nesterov first moves along the green momentum step to a new temporary position, then computes the gradient there. The red arrow is the gradient at that â€œlookaheadâ€ position. (âˆ’Î±âˆ‡W|W=Wt+Î¼Ï)\n\n\nPurple arrow: The final update.\n\n\nNesterov momentum can avoid overshooting, because Nesterov momentum peeks ahead to the next position, adjust the gradient there, and updates more precisely.\n\n\nAdaptive Learing Rates\n\n\nAdagrad\nAdjusts the learning rate for each parameter based on all past gradients of that parameter.\nAccumulate squared gradients:\nG=G+(âˆ‡W|W=Wt)2Update parameters:\nWt+1=Wtâˆ’Î±âˆ‡W|W=WtG=Wtâˆ’Î±Gâˆ‡W|W=WtIf a parameter has large gradients in the past (G is large), its learning rate is scaled down. If a parameter has small gradients in the past, its learning rate is scaled up.\nIt is great for sparse data (e.g., NLP), but learning rate decays too much over time, which can stall training.\n\n\nRMSProp\nIs based on Adagrad, but give more weight to recent gradients.\nG=Î»G+(1âˆ’Î»)(âˆ‡W|W=Wt)2Wt+1=Wtâˆ’Î±âˆ‡W|W=WtGIt is more stable than Adagrad, especially for long training runs.\n\n\nAdam (Most common)\nCombines RMSProp and Momentum. It is with both the first and second moments of the gradients.\n\nâ„¹ï¸ Note\nMoment is a term in statistic. First moment (ä¸€é˜¶çŸ©) is Mean. Second moment (äºŒé˜¶çŸ©) is Variance-like term\n\nUpdating of G1 is pretty similar with momentum. That can speed up convergence.\nG1=Î»1G1+(1âˆ’Î»1)(âˆ‡W|W=Wt)Updating of G2 is pretty similar with RMSProp. That can adapt learning rate.\nG2=Î»2G2+(1âˆ’Î»2)(âˆ‡W|W=Wt)2Wt+1=Wtâˆ’Î±G1G2Adam can let model converge fast and stably.\n\n\n\nâ„¹ï¸ Note\nCommon use: Adam &gt; Momentum GD &gt; RMSProp\n\n\nCS231n Deep Learning for Computer Vision\n\n\nğŸ’¡ Tip\nQ: Why do some optimizers successfully escape saddle points?\nA: Todo\n\nSet the Learning Rate\n.qryvuvnfwesa{zoom:50%;}\nDropping the learning rate every time validation loss stagnates is extremely useful for training.\n.rkcnsrmrtvws{zoom:80%;}\nActivation Functions\nThresholding functions mimic how biological neurons work: a neuron â€œfiresâ€ if input exceed a threshold, otherwise it silences. But they are not differentiable everywhere (only at threshold). This makes backpropagation impossible. Several Activation functions address this by softening the threshold functions into a continuous, differentiable curve.\n\n\nSigmoid\nS(x)=11+eâˆ’x=exex+1.uptksrwsfsgf{zoom:50%;}\nThe derivative is: S(x)(1âˆ’S(x)).\n.kznzesdamijt{zoom:50%;}\n\n\nTanh\ntanhâ¡(x)=exâˆ’eâˆ’xex+eâˆ’x=2sigmoid(2x)âˆ’1.ymjqujiunoov{zoom:67%;}\nThe derivative is tanhâ€²â¡(x)=ex+eâˆ’xexâˆ’eâˆ’x\n\n\n\nFor Sigmoid and tanh, when the absolute value of  input is pretty large (e.g, 10000 or -100000), the gradient vanishes. Then the update becomes very slow.\nThe gradient of Sigmoid and tanh are non-zero in a narrow regions, approx. zero everywhere else.\n\n\nRelu\nf(x)=max(0,x).sxciurtfgsxf{zoom:67%;}\nThe derivative is:\nfâ€²(x)={1,xâ‰¥00,x&lt;0.myvplwpfxvlw{zoom:70%;}\n\n\nPros:\n\nSolve the problem of vanishing gradients, when the input value is positive.\nIt is easy to calculate.\n\n\n\nCons:\n\nThe gradients is 0, when the input value is negative.\n\n\n\n\n\nLeaky Relu (Parametrix ReLU: PReLU)\nf(x)=max(ax,x).veprfgereluo{zoom:67%;}\nThe derivative is:\nfâ€²(x)={1,xâ‰¥0a,x&lt;0\n\n\nâ„¹ï¸ Note\nReLU and relative activation functions (LeakyReLU, PReLU) are good starting choices of activation functions at the hidden layers of the network. Using thoes functions let training deep nets faster.\n.yciitmxwtvmo{zoom:50%;}\n\nManaging Weights\nInitialization\n\n\nZero Initialization\n\nâš ï¸ Warning\nDo Not initialize weights to all zeros.\n\nIf all weights W in a layer are initialized to 0. Then, every neuron computes the same output for any input X: zi=Wiâˆ—X+bi=0âˆ—X+0=0 (for all neurons i).\nIn the forward propagation, because each neuron has same weights and bias, they produce same output. In the backward propagation, because the gradient for each neurons depends only on its output and upstream gradient from the next layer, their gradients are also identical.\nIn the parameter update, since gradients are all identical and Wold=0, Wnew remains identical across neurons.\nIt is equivalent to just having 1 unit per layer â†’ Symmetry (must be broken!)\n\n\nKaiming Initialization\nDesigned for ReLU.\nWeights are initialized from a normal distribution $W_\\mathrm{layer}\\sim\\mathcal{N}\\left(0,\\frac{2}{D_\\mathrm{in}}\\right) $, where $D_{in} = $ number of inputs to the layer.\n\n\nXavier Initialization\nDesigned for tanh.\nWeights are initialized from a normal distribution Wlayerâˆ¼N(0,1Din+Dout), where Din = inputs to the layer, Dout = output to the layer.\n\n\nBias Initialization\nInitialize the biases to be all zeros.\n\n\n\nâ„¹ï¸ Note\nRecommend Kaiming or Xavier\n\nBatch Normalization\nCovariate shift: As layers 1,2,â€¦,l-1 learn, the distribution of inputs to layer l are shifting.\nObviously, covariant shift slows down learning.\nTo solve this problem, we should standardize the activations of each layer, so that they are always in N(0,1).\nStandardization is differentiable. So gradient descent still works.\nWe can compute mean and std (standard deviation) over mini-batch.\nBatch Normalization can be applied before or after activation layer. (i.e., RuLU(BN(Wx)) or BN(RuLU(Wx))).\n\nâ„¹ï¸ Note\nRecommend BN after activation â†’ BN(RuLU(Wx))\n\nRegularization\n\n\nL2 regularization\nAdd Î»2âˆ¥Wâˆ¥22 to loss function. It is equivalent to decay weight at every update W=Wâˆ’Î»W. (so, it is also called weight decay)\n\n\nL1 regularization\nAdd Î»|W| to loss fucntion.\n\n\n\nâš ï¸ Warning\nDo Not regularize bias term\n\n\nâ„¹ï¸ Note\nRecommend L2 + Dropout\n\nDropout\nRandomly drop a fraction p (usually 0.5) of the units in a layer.\nThere are three intuitions that can help to explain why does dropout work?\n\n\nIntuition 1: Noise-robustness to missing information.\nDuring training, Dropout randomly â€œdrops outâ€ (disables) some neuronsâ€™ outputs. This forces the network to learn to make predictions even when part of the input information is missing.\nJust as humans can recognize objects even if parts are obscured (e.g., a partially hidden cat), the network becomes robust to noise or incomplete inputs â€” it doesnâ€™t rely on any single neuron or feature to make decisions.\n\n\nIntuition 2: Units must work with diverse co-units (leading to disentangled features).\nDropoutâ€™s randomness means a neuron never relies on specific partner neurons (since those partners might be dropped out in any iteration).\nThis forces each neuron to learn features that work with many different sets of â€œcompanionâ€ neurons (instead of relying on a fixed group).\n\n\nIntuition 3: Training a â€œrandom subsetâ€ of networks (implicit ensemble learning)\nWhen Dropout is applied to a layer with N neurons, each training iteration effectively trains a slightly different subnetwork â€” chosen randomly from 2N possible subnetworks (since each neuron has two states: â€œactiveâ€ or â€œdropped outâ€).\nFor example, a layer with 100 neurons has 2100possible subnetworks â€” a huge number. Most subnetworks are never trained in full, and each subnetwork only gets one update per appearance.\nHowever, all these subnetworks share weights: Updating weights for one subnetwork indirectly updates weights for all subnetworks that share those weights.\nThis mirrors ensemble learning (e.g., bagging): Instead of training one large model, Dropout trains thousands of tiny subnetworks, and testing (with all neurons active) acts like an â€œensembleâ€ of these subnetworks â€” improving generalization.\n\n\nDuring the training, we can turn on the dropout to improve the generalization. But, in the testing, we should turn off the dropout. However, now, the output strength of each neuron is twice of what it is in training (consider p = 0.5. In the training, each neuron has 50% contribution to the output. But, in the testing, because the dropout is switched off, each neuron has 100% contribution to the output.).\nThus, we should divide outgoing weights by 2 (or 1/dropout). This approximates the â€œaverageâ€ prediction of all possible sub-models created during training.\nManaging Training\nMonitor your validation losses, and stop training your network when they start rising. Can be thought of as a simple and really efficient way to introduce regularization.\nData Augmentation\nIt is often used when you know that your output is robust to some transformations of your data.\nFor the pictures, you can rotate, shift, flip, crop and combine them. For the language, you can substitute synonyms.\nHyperparameter Optimization\n\n\nKeep the number of hyperparameters as small as possible.\nIf your computation budget lets you optimize only one hyperparameter, let it be the learning rate.\n\n\nParallelize search across many machines.\n\n\nRecord hyperparameters of all runs carefully.\n\n\nUse the same random init for all hyperparameters.\nThat means initializing the weight of the model with same random seed when testing different combinations of hyperparameters, because the randomness of weight may affect the result.\n\n\nFor 2 or 3 hyperparameters, do a systematic â€œgrid searchâ€\n.ycffbsqqsyqn{zoom:50%;}\n\n\n\nâ„¹ï¸ Note\nFor high-dimensional hyperparameter spaces (&gt;3 hyperparameters), random search is more efficient than grid search. It finds better models with fewer training runs by focusing  on impactful hyperparameters.\n.fmvxnyvfqvtn{zoom:50%;}\n\nCoarse-to-fine Search\n\n\nFirst, search over a large (coarse) range of hyperparameters to identify a â€œpromising regionâ€ with good performance.\n\n\nThen, narrow the search window (to a finer scale) around this promising region and repeat â€” refining the search to find the exact optimal hyperparameters.\n\n\nIf best results are not at the boundaries of the current window,\ncenter a smaller window on the best hyperparameters found so far, and repeat the search (to refine the search in this more focused region).\nIf best results are near the boundaries of the current window,\nshift the window to center on the best hyperparameters, then repeat (to ensure the optimal value isnâ€™t â€œcut offâ€ by the windowâ€™s edge).\nThere also an advanced tip: Bayesian Optimization. (Todo)\n.ifufidmttbfp{zoom:50%;}\nCross-validation vs a Single Validation Set\nFull k-fold cross-validation might be computationally too expensive. Thus, a single large held-out validation set suffices.\n.sqtvruidrfci{zoom:67%;}\nPractical Tips for Training Neural Nets\nTodo\n.lptrkuuqmeps{zoom:50%;}\n\nCS231n Deep Learning for Computer Vision\ndeeplearningbook.org/contents/guidelines.html\n\nK-Nearest Neighbors\n.pojmwvfwfgux{zoom:50%;}\nIn K-Nearest Neighbors, we donâ€™t train a model based on the data. On the contrary, the data is the model.\n.mpqmyppqqlvu{zoom:57%;}\n.sktljxfekluz{zoom:57%;}\nAccording to the distribution of red points and blue points, we can explicitly find out two different areas. Each represents either Large or Medium. If a new point has been placed â€œLargeâ€ area, nearing by red points, it has a high probability to be â€œLargeâ€. Same does â€œMediumâ€\n\n\nKNN Classification: To predict category label y of a new point x\n\nFind â€œk nearest neighborsâ€.\nAssign the majority label.\n\n\n\nKNN Regression: To predict value y of a new point x\n\nFind â€œk nearest neighborsâ€.\nAverage the values associated with the neighbors.\n\n\n\nThus, if you change the k, the prediction result may change.\nThe Definition of â€œNearestâ€\nConsider we are calculating the distance between x1,x2.\n\n\nâ„“1 distance\n(âˆ‘j|x1jâˆ’x2j|1)11=âˆ‘j|x1jâˆ’x2j|\n\nâ„“2 distance\n(âˆ‘j|x1jâˆ’x2j|2)12Euclidean distance\n\n\nâ„“âˆ distance\n(âˆ‘j|x1jâˆ’x2j|â†’âˆ)â†’0=maxj(|x1jâˆ’x2j|)Why? Todo\n\n\nDifferent distance produce different outcomes.\nLet us consider k= 1, then\n\nâ€œNon-parametricâ€ Machine Learning Approches\n\nğŸ’¡ Tip\nQ: In linear regression / logistic regression / neural networks, what do we need to make predictions?\nA: Parameters or weights\n\nFor the KNN model, there are implicit parameters, which is the full training dataset.\nThus there is no explicit training phase. The moment we have the dataset, we are ready to produce predictions for new input data.\n\nâš ï¸ Warning\nDataset preprocessing phase that may be thought of as training.\n\nHyperparameters in KNN\n\n\nChoice of distance function\nMost of time, we choose â„“p=2,1,\\infin distance. But, sometimes an â„“p distance after transforming inputs x to some f(x).\n\n\nChoice of k\nSmall value of k means model may be easily affected by noisy data. Large value of k makes it difficult to model sharp changes in the true function. For binary classification, usually an odd number to avoid ties.\n\n\n\nğŸ’¡ Tip\nQ: What functions can k-NN classifiers / regressors represent? Or perhaps  easier to think about: what functions can k-NN models not represent?\nA: k-NN models are only limited in expressivity by the training data, so with  the right training dataset, k-NN models can represent any function.\n\n","categories":["Artificial Intelligence","Machine Learning"]},{"title":"Gitä½¿ç”¨","url":"/2025/03/10/Git%E4%BD%BF%E7%94%A8/","content":"å‚è€ƒè§†é¢‘ï¼šååˆ†é’Ÿå­¦ä¼šæ­£ç¡®çš„githubå·¥ä½œæµï¼Œå’Œå¼€æºä½œè€…ä»¬ä½¿ç”¨åŒä¸€å¥—æµç¨‹_å“”å“©å“”å“©_bilibili\nåŸºæœ¬æ“ä½œæµ\nå…ˆå°†è¿œç¨‹ä»“åº“å¤åˆ¶åˆ°æœ¬åœ°\n.lqqsguolmddl{zoom:50%;}\n\n\næœ¬åœ°åˆ†æˆä¸¤ä¸ªéƒ¨åˆ†\n\nLocalï¼šæœ¬åœ°çš„ä»“åº“ï¼Œå³å°†è¦å‘Šè¯‰gitçš„ä¿¡æ¯\nDiskï¼šæœ¬åœ°çš„ç£ç›˜ï¼Œæºæ–‡ä»¶çœŸæ­£æ‰€åœ¨çš„ä½ç½®\n\n\n\næ–°å»ºä¸€ä¸ªæ–°çš„åˆ†æ”¯\n.zbdbbtbxokod{zoom:50%;}\nç¼–è¾‘æ–°çš„ä»£ç \n.bfcletpsvmlq{zoom:50%;}\nå¯ä»¥ä½¿ç”¨git diffæŸ¥çœ‹diskä¸‹çš„æ–‡ä»¶ä¸localä¸­æ–‡ä»¶çš„åŒºåˆ«\nä¿å­˜æ–°çš„ä»£ç \n.kqoveulrfcge{zoom:50%;}\ngit addå‘½ä»¤å°†ä¼šè®©gitçŸ¥é“ç”¨æˆ·æƒ³è¦æäº¤çš„æ–‡ä»¶æœ‰å“ªäº›ï¼Œè¿™äº›æ–‡ä»¶å°†è¢«ä¿å­˜åœ¨æš‚å­˜åŒº\næäº¤æ–°çš„ä»£ç \n.fukccfdxjssb{zoom:50%;}\né€šè¿‡git commitï¼Œgitå°†ä¼šæŠŠä¿®æ”¹çš„å†…å®¹æ”¾åˆ°gitä¸­ã€‚ä½†æ˜¯æ­¤æ—¶ï¼Œgithubè¿˜æ˜¯ä¸çŸ¥é“æ›´æ–°çš„\næ¨é€ä»£ç \n.ujeonydfxmrw{zoom:50%;}\næ¨é€åï¼Œgithubä¼šå‡ºç°ä¸€ä¸ªæ–°çš„åˆ†æ”¯\néšä¸»åˆ†æ”¯æ›´æ–°\n.beypworkavqu{zoom:50%;}\nåœ¨ç”¨æˆ·æäº¤äº†ä¸€ä¸ªæ–°çš„ä¿¡æ¯ï¼ˆf-commitï¼‰åï¼Œä¸»åˆ†æ”¯å¯èƒ½ä¼šæœ‰æ›´æ–°ï¼ˆupdateï¼‰ï¼Œè¿™æ—¶å€™å°±éœ€è¦ç¡®è®¤my-featureä¸‹çš„æ›´æ–°æ˜¯å¦å¯ä»¥åœ¨æ›´æ–°åçš„ä¸»åˆ†æ”¯ä¸‹è¿è¡Œã€‚\næ›´æ–°local branch\nåˆ‡æ¢åˆ°mainåˆ†æ”¯\n.tbxidslqxieu{zoom:50%;}\næ‹‰å–mainåˆ†æ”¯çš„æ›´æ–°\n.kyvlewloliml{zoom:50%;}\nåˆ‡æ¢åˆ°my-featureåˆ†æ”¯\n.dljdyrbagmgk{zoom:50%;}\nåŒæ­¥updateä¿®æ”¹\n.xzopqrcsjlge{zoom:50%;}\ngit rebaseæ„å‘³ç€å…ˆæŠŠç”¨æˆ·çš„ä¿®æ”¹æ”¾åˆ°ä¸€éï¼ŒæŠŠmainæœ€æ–°çš„ä¿®æ”¹æ‹¿è¿‡æ¥ï¼Œåœ¨è¿™ä¸ªæœ€æ–°çš„ä¿®æ”¹åŸºç¡€ä¹‹ä¸Šï¼Œå°è¯•æŠŠf-commitæ”¾ä¸Šå»ã€‚\næ­¤æ—¶å¯èƒ½ä¼šå‡ºç°rebase conflictï¼Œè¿™æ—¶å€™å°±éœ€è¦æ‰‹åŠ¨é€‰æ‹©ã€‚\nç›¸æ¯”è¾ƒäºmergeï¼Œrebaseæ˜¯åœ¨ä¸»åˆ†æ”¯æœ€æ–°çš„æäº¤ä¸Šåšå‡ºäº†ä¿®æ”¹ã€‚ä¹Ÿå°±æ˜¯è¯´ï¼Œç›¸æ¯”è¾ƒäºmergerä¼šäº§ç”Ÿä¸€ä¸ªé¢å¤–çš„merge commitï¼Œrebaseä¼šè®©æäº¤æ›´çº¿æ€§ï¼Œæ›´å¹²å‡€ã€‚ä½†æ˜¯å…¬å…±åˆ†æ”¯ä¸è¦ä½¿ç”¨ã€‚\næäº¤åˆ†æ”¯çš„æ–°ä¿®æ”¹\n.bvdqmeevgjdj{zoom:50%;}\n\næ³¨æ„\næ­¤æ—¶æäº¤å¿…é¡»åŠ ä¸Š-f\n\nå°†ä»£ç åˆå¹¶åˆ°ä¸»åˆ†æ”¯\n.hjoicdstdmiv{zoom:50%;}\nå‘é¡¹ç›®ç®¡ç†è€…æå‡ºpull request\né¡¹ç›®ç®¡ç†è€…åˆå¹¶ä»£ç \n.vutuaksulsrm{zoom:50%;}\nsquashæ“ä½œæ˜¯ä¸ºäº†å°†æäº¤çš„åˆ†æ”¯ä¸Šçš„æ‰€æœ‰æ”¹å˜éƒ½åˆå¹¶æˆä¸€ä¸ªæ”¹å˜ï¼Œè¿™æ ·å¯ä»¥ä½¿ä¸»åˆ†æ”¯çš„commit historyå°½å¯èƒ½ç®€æ´ã€‚ç®€å•æ¥è®²å°±æ˜¯æ‰€æœ‰çš„æ”¹å˜éƒ½è¢«æ”¾å…¥åˆ°äº†update2ä¸­ï¼Œä½†æ˜¯commitçš„ç»“æ„æ•°é‡å’Œåå­—æ”¹å˜äº†\nåˆ é™¤åˆ†æ”¯\nå½“æäº¤ç»“æŸåï¼Œè¿œç«¯çš„åˆ†æ”¯å°±å¯ä»¥é€šè¿‡delete branchåˆ æ‰äº†ã€‚ä½†æ˜¯æ­¤æ—¶æœ¬åœ°çš„gitä¸Šè¿˜æœ‰è¿™ä¸ªåˆ†æ”¯ã€‚\n.jgkgruasjtdh{zoom:50%;}\næœ¬åœ°åˆ‡æ¢å›ä¸»åˆ†æ”¯ã€‚\n.apyzzkcobpku{zoom:50%;}\nåˆ é™¤æ‰my-featureåˆ†æ”¯ã€‚\næ‹‰å–æ–°çš„ä¸»åˆ†æ”¯\n.xnftmbsfrglq{zoom:50%;}\n","categories":["Git"],"tags":["Git"]},{"title":"Machine Learning","url":"/2025/09/22/Machine-Learning/","content":"çº¿æ€§ç®—æ³•ï¼ˆlinear algorithmï¼‰\né€»è¾‘å›å½’ï¼ˆLogistic Regressionï¼‰\nè™½ç„¶åå­—ä¸ºå›å½’ï¼Œä½†æ˜¯å®é™…ä¸Šå®ç°çš„æ˜¯åˆ†ç±»ã€‚\næ‰€è°“åˆ†ç±»ï¼Œå°±æ˜¯å¤šå¤§æ¦‚ç‡åˆ†åœ¨ä¸€èµ·ï¼Œè¿™ä¸ªæ¦‚ç‡æ˜¯è¿ç»­çš„ï¼Œä»è€Œå°±å¯ä»¥å®ç°å›å½’ã€‚\np=f(x)y^={1,pâ‰¥0.50,p&lt;0.5ä½†æ˜¯ï¼Œè¿™ä¼šå¼•å‡ºä¸€ä¸ªé—®é¢˜ï¼Œf(x)çš„å–å€¼èŒƒå›´æ˜¯(âˆ’âˆ,+âˆ)ï¼Œè€Œæ¦‚ç‡pçš„å–å€¼èŒƒå›´æ˜¯[0,1]ã€‚\nä¸ºäº†è§£å†³è¿™ä¸ªé—®é¢˜ï¼Œä½¿ç”¨sigmoidå‡½æ•°å¯¹f(x)è¿›è¡Œå¤„ç†ï¼Œå°†(âˆ’âˆ,+âˆ)çš„æ•°æ®è½¬åŒ–ä¸º[0,1]çš„æ•°æ®ã€‚\n.joehynnclool{zoom: 50%;}\nç°åœ¨ï¼Œæ–°çš„å…¬å¼å°±å˜æˆï¼š\nf(x)=wTxp=g(wTx)=11+eâˆ’wTxy^={1,pâ‰¥0.50,p&lt;0.5ä¸ºäº†åº”å¯¹æ–°çš„è¿™ç§å¸¦æœ‰æ¦‚ç‡çš„å…¬å¼ï¼ŒæŸå¤±å‡½æ•°ä¹Ÿè¦åšå‡ºæ”¹å˜ã€‚\n\n\næŸå¤±å‡½æ•°\n\nå½“çœŸå€¼y=1çš„æ—¶å€™ï¼Œè‹¥pè¶Šå°ï¼Œè¯¯å·®è¶Šå¤§ï¼Œåä¹‹è¶Šå°ã€‚\nå½“çœŸå€¼y=0çš„æ—¶å€™ï¼Œè‹¥pè¶Šå¤§ï¼Œè¯¯å·®è¶Šå¤§ï¼Œåä¹‹è¶Šå°ã€‚\n\n.ducwwcwrmxek{zoom: 50%;}\nå·¦è¾¹çš„å›¾å°±å·²ç»åˆæ­¥æ»¡è¶³äº†æŸå¤±å‡½æ•°çš„è¦æ±‚ï¼Œä½†æ˜¯ä½¿ç”¨å³è¾¹çš„å¸¦æœ‰logçš„æ›´å¥½ï¼Œè¿™æ˜¯å› ä¸ºå‡½æ•°åœ¨é€¼è¿‘0å’Œé€¼è¿‘1çš„æ—¶å€™ï¼Œyå€¼å°±ä¼šå˜æˆæ— ç©·å¤§ï¼Œè¿™æœ‰ç›Šäºæƒ©ç½šã€‚åŒæ—¶ï¼Œlogå‡½æ•°æ›´æ–¹ä¾¿æ±‚å¯¼ã€‚\nå¯¹äºâˆ’ylog(p)âˆ’(1âˆ’y)log(1âˆ’p)è¿™ä¸ªå…¬å¼ï¼Œyçš„å–å€¼åªæœ‰ä¸¤ç§ã€‚å½“y=0çš„æ—¶å€™ï¼ŒåŸå¼å˜ä¸ºâˆ’log(1âˆ’p)ï¼Œå½“y=1æ—¶å€™ï¼ŒåŸå¼å˜ä¸ºâˆ’log(p)ã€‚\næœ€ç»ˆï¼ŒæŸå¤±å‡½æ•°å¦‚ä¸‹ï¼š\nL(w)=âˆ’1mâˆ‘i=1m{yilog(pi)+(1âˆ’yi)log(1âˆ’pi)}pi=g(wTxi)ç°åœ¨ï¼Œæˆ‘ä»¬çš„ç›®æ ‡è½¬åŒ–ä¸ºï¼š\nargminwL(w)å› ä¸ºé€»è¾‘å›å½’çš„æŸå¤±å‡½æ•°ä¸­å­˜åœ¨logå’Œsigmoidï¼Œå¹¶ä¸”å…¶æ˜¯åˆ†æ®µçš„ï¼Œæ‰€ä»¥å…¶æ²¡æœ‰è§£æè§£ï¼ˆè¶…è¶Šæ–¹ç¨‹æ²¡æœ‰è§£æè§£ï¼‰ã€‚æ‰€ä»¥ï¼Œåªæœ‰æ¢¯åº¦ä¸‹é™èƒ½æ±‚è§£é€»è¾‘å›å½’çš„æŸå¤±å‡½æ•°ï¼ˆæœ€ä¼˜åŒ–ç†è®ºï¼‰ã€‚\n\n\næ­£åˆ™åŒ–ï¼ˆregularizationï¼‰\nGeneralï¼š\nL(w)=âˆ’1mâˆ‘i=1m{yilog(pi)+(1âˆ’yi)log(1âˆ’pi)}+Î”å…¶ä¸­ï¼ŒÎ”å°±æ˜¯æ­£åˆ™åŒ–é¡¹ï¼Œå…¶æœ‰ä¸åŒçš„å½¢å¼ã€‚\n\n\nL1èŒƒå¼ä¸LASSOå›å½’\nÎ”=Î±âˆ‘j=1n|wj|ä¸ºæ¨¡å‹æ·»åŠ äº†å…ˆéªŒçŸ¥è¯†ã€‚å…¶å‘Šè¯‰æ¨¡å‹ï¼Œå‚æ•°ç¬¦åˆé›¶å‡å€¼æ‹‰æ™®æ‹‰æ–¯åˆ†å¸ƒã€‚\n.chvloyovdhzk{zoom:67%;}\n\n\nL2èŒƒå¼ä¸Ridgeå›å½’\nÎ”=12Ïƒ2wTw=12Ïƒ2âˆ¥wâˆ¥22è®¡ç®—æ‰€æœ‰å‚æ•°åˆ†é‡çš„å¹³æ–¹å’Œã€‚å…¶è®©å‚æ•°ç¬¦åˆé›¶å‡å€¼æ­£æ€åˆ†å¸ƒã€‚Ïƒ2æ˜¯æ­£æ€åˆ†å¸ƒçš„æ–¹å·®ã€‚\n.colsdsatbldb{zoom:67%;}\n\n\nå¼¹æ€§ç½‘ç»œï¼ˆElasticNetï¼‰\nÎ”=Î»Ïâˆ¥wâˆ¥1+Î»(1âˆ’Ï)2âˆ¥wâˆ¥22å…¼å®¹LASSOå›å½’å’ŒRidgeå›å½’ï¼Œé€šè¿‡Î»å’ŒÏæ¥è°ƒèŠ‚ä¸¤ä¸ªæ¯”ä¾‹ã€‚\n\n\n\n\n\n\nå¤šåˆ†ç±»é—®é¢˜\né€»è¾‘å›å½’é™¤äº†èƒ½è§£å†³äºŒåˆ†ç±»é—®é¢˜ï¼Œè¿˜å¯ä»¥è§£å†³å¤šåˆ†ç±»é—®é¢˜ã€‚æ ¸å¿ƒçš„åŸç†å°±æ˜¯å¤æ‚é—®é¢˜ç®€å•åŒ–ã€‚æŠŠå¤šåˆ†ç±»è½¬åŒ–ä¸ºäºŒåˆ†ç±»ã€‚\n\n\nè½¬åŒ–ç­–ç•¥ï¼š\n\n\nOne vs Oneï¼ˆOvOï¼‰ï¼šä¸€å¯¹ä¸€\nå°†å¤šåˆ†ç±»æ‹†å¼€æˆè‹¥å¹²ä¸ªäºŒåˆ†ç±»ã€‚\n.edewtnesqold{zoom: 25%;}\nç°åœ¨æœ‰4ç±»ï¼Œå¯ä»¥æ‹†æˆC42=6ç»„äºŒåˆ†ç±»ã€‚éšåå°±å¯ä»¥è®­ç»ƒå‡º6ä¸ªåˆ†ç±»å™¨ã€‚\n.fhkkfaodildv{zoom: 33%;}\nåœ¨é¢„æµ‹é˜¶æ®µï¼ŒæŠŠæ•°æ®åˆ†åˆ«æ‰”ç»™è¿™å…­ä¸ªåˆ†ç±»å™¨ï¼Œæ¯ä¸ªåˆ†ç±»å™¨éƒ½ä¼šç»™å‡ºä¸€ä¸ªç»“æœï¼Œå¯¹è¿™äº›ç»“æœè¿›è¡Œç»Ÿè®¡ï¼ŒæŠ•ç¥¨æœ€é«˜çš„å°±æ˜¯æœ€ç»ˆç»“æœã€‚\næ¨å¹¿åˆ°nä¸ªç±»ï¼Œå°±æ„å»ºCn2=n(nâˆ’1)2ä¸ªåˆ†ç±»å™¨ã€‚\n\n\nOne vs Rest ï¼ˆOvRï¼‰ï¼šä¸€å¯¹å‰©ä½™\n.gqspimgpgfra{zoom:25%;}\nç°åœ¨æœ‰4ç±»ï¼Œä»¥æ¯ä¸€ç±»ä¸ºè§†è§’ï¼Œå…¶æœ¬èº«æ˜¯ä¸€ç±»ï¼Œå‰©ä¸‹çš„ä¸‰ç±»æ˜¯ä¸€ç±»ã€‚è¿™æ ·å°±å¯ä»¥æ‹†å¼€æˆ4ä¸ªäºŒåˆ†ç±»é—®é¢˜ã€‚\n.bjfwdpcwilfj{zoom:33%;}\nä»¥æ­¤è®­ç»ƒå¤„4ä¸ªåˆ†ç±»å™¨ã€‚åœ¨é¢„æµ‹é˜¶æ®µï¼Œå°†æ•°æ®åˆ†åˆ«æ”¾å…¥å››ä¸ªåˆ†ç±»å™¨ï¼Œç»Ÿè®¡4ä¸ªåˆ†ç±»å™¨è¾“å‡ºçš„ç»“æœï¼Œç¥¨æ•°æœ€é«˜çš„å°±æ˜¯æœ€ç»ˆç»“æœã€‚\næ¨å¹¿åˆ°nä¸ªç±»ï¼Œå°±æ„å»ºnä¸ªåˆ†ç±»å™¨\n\n\næ€»ç»“æ¥çœ‹ï¼ˆå‡è®¾äºŒåˆ†ç±»çš„æ—¶é—´å¤æ‚åº¦ä¸ºTï¼‰ï¼š\n\n\n\n\n\n\næ—¶é—´å¤æ‚åº¦\nå‡†ç¡®ç‡\n\n\n\n\nOvO\nCn2âˆ—T=n(nâˆ’1)2âˆ—T\nå‡†ç¡®åº¦é«˜\n\n\nOvR\nnâˆ—T\nå­˜åœ¨ç±»åˆ«æ··æ·†\n\n\n\nOvOçš„æ—¶é—´å¤æ‚åº¦é«˜ï¼ˆå› ä¸ºå…¶éœ€è¦æ›´å¤šçš„åˆ†ç±»å™¨ï¼‰ï¼Œä½†æ˜¯å‡†ç¡®åº¦é«˜ã€‚\n\n\n\n\n2\n3\n\n\n\n\n4\nCn2âˆ—T=n(nâˆ’1)2âˆ—T\n6\n\n\n7\nnâˆ—T\n9\n\n\n\n","categories":["Artificial Intelligence"]},{"title":"Linuxä¸Šä½¿ç”¨C++","url":"/2025/07/18/Linux%E4%B8%8A%E4%BD%BF%E7%94%A8CPP/","content":"GCCç¼–è¯‘å™¨\ngccå¯ä»¥ç”¨äºç¼–è¯‘Cä»£ç ï¼ˆgç”¨äºç¼–è¯‘Cä»£ç ï¼‰\nç¼–è¯‘è¿‡ç¨‹\n.okoojkleagcf{zoom: 50%;}\nåŸå§‹çš„ test.cpp æ–‡ä»¶\n\n\né¢„å¤„ç†ï¼šç”Ÿæˆ.iæ–‡ä»¶\nåœ¨è¿™ä¸€æ­¥éª¤ä¸­ï¼Œç¼–è¯‘å™¨ä¼šå¯¹å®å®šä¹‰ã€å¸¸é‡è¿›è¡Œå¤„ç†ï¼Œæ¯”å¦‚å¯¹å¸¸é‡è¿›è¡Œæ›¿æ¢æ“ä½œã€‚\n# -E è¡¨ç¤ºä»…è¿›è¡Œé¢„å¤„ç†ã€‚g++ -E test.cpp -o test.i\n.jvsdelqmgshs{zoom:50%;}\nCONST_NUMè¢«æ›¿æ¢æˆäº†40ï¼ŒåŒæ—¶å‰é¢çš„iostreamåº“ä¹Ÿè¢«å¯¼å…¥è¿‘æ¥ï¼Œæ‰€ä»¥ä¼šçªå¢åˆ°28650è¡Œã€‚\n\n\nç¼–è¯‘ï¼šç”Ÿæˆ.sæ–‡ä»¶\nå°†ä»£ç è½¬åŒ–ä¸ºæ±‡ç¼–è¯­è¨€ã€‚\n# -S è¡¨ç¤ºä»…è¿›è¡Œæ±‡ç¼–g++ -S test.i -o test.s\n.nddrlofnhfty{zoom: 50%;}\næ±‡ç¼–ç¨‹åºã€‚\n\n\næ±‡ç¼–ï¼šç”Ÿæˆ.oæ–‡ä»¶\nå°†æ±‡ç¼–ä»£ç è½¬åŒ–ä¸ºè®¡ç®—æœºèƒ½ç†è§£çš„äºŒè¿›åˆ¶ä»£ç ã€‚\n# -C è¡¨ç¤ºä»…è¿›è¡Œæ±‡ç¼–g++ -c test.s -o test.o\n.sdcfrgcjdlcq{zoom:50%;}\näºŒè¿›åˆ¶ä»£ç ã€‚\n\n\né“¾æ¥ï¼šç”Ÿæˆå¯æ‰§è¡Œçš„äºŒè¿›åˆ¶ç¨‹åºã€‚\nå°†ç³»ç»Ÿä¸­çš„åº“é“¾æ¥åˆ°å½“å‰ç¨‹åºä¸­ã€‚\n# -o å°†å¯æ‰§è¡Œæ–‡ä»¶å‘½åä¸ºæŒ‡å®šçš„åå­—g++ test.o -o test\n\n\nåœ¨å®é™…ä½¿ç”¨è¿‡ç¨‹ä¸­ï¼Œå¯ä»¥ç›´æ¥ä½¿ç”¨ä¸‹é¢çš„è¿™ä¸€æ­¥ã€‚å…¶éšå¼çš„åŒ…å«äº†å‰é¢å››æ­¥ã€‚\ng++ test.cpp -o test\ng++ç¼–è¯‘å‚æ•°\n\n\n-gç¼–è¯‘å¸¦è°ƒè¯•ä¿¡æ¯çš„å¯æ‰§è¡Œæ–‡ä»¶ã€‚è¿™æ„å‘³ç€ä»£ç å¯ä»¥è¢«GDBè°ƒè¯•\ng++ -g test.cpp -o test\n\n\n-O[n]ä¼˜åŒ–æºä»£ç ã€‚ç¼–è¯‘å™¨ä¼šå¯¹ä»£ç è¿›è¡Œä¼˜åŒ–ï¼Œæ¯”å¦‚å»é™¤æ²¡ç”¨è¿‡çš„å˜é‡ã€‚æé«˜å¯æ‰§è¡Œæ–‡ä»¶çš„è¿è¡Œé€Ÿåº¦ã€‚\nä¸€èˆ¬æƒ…å†µä¸‹-O2å°±å·²ç»è¶³å¤Ÿäº†ã€‚-Oä¸-O1éƒ½æ˜¯åˆæ­¥ä¼˜åŒ–ã€‚\n\n\n-lå’Œ-Léƒ½ç”¨äºæŒ‡å®šé“¾æ¥åº“ã€‚\n-lä¸ºåº“åã€‚\n-Lä¸ºåº“çš„ä½ç½®ã€‚\né»˜è®¤æƒ…å†µä¸‹ï¼Œç¨‹åºéƒ½æ˜¯åœ¨/libï¼Œ/usr/libå’Œ/usr/local/libä¸­å¯»æ‰¾åº“ï¼Œæ­¤æ—¶æ— éœ€å‚æ•°-Lã€‚\ng++ -lOfiicialLib test.cpp # é“¾æ¥çš„åº“ä¸ºOfficialLib\nä½†å¦‚æœæ˜¯è‡ªå·±å†™çš„åº“ï¼Œéœ€è¦æŒ‡å®šä½ç½®ï¼Œé‚£ä¹ˆå°±éœ€è¦ä½¿ç”¨-Lã€‚\ng++ -L/home/abc/MyLibFolder -lMyLib test.cpp\n\n\n-IæŒ‡å®šå¤´æ–‡ä»¶ä½ç½®ã€‚\ng++ test.cpp -Iinclude # includeä¸‹ä¿å­˜äº†test.cppä¸­éœ€è¦çš„.hæ–‡ä»¶\n\n\ng++å¤šæ–‡ä»¶ç¼–è¯‘\n.jgidxmvctfdo{zoom:50%;}\næ–‡ä»¶ç»“æ„å¦‚å›¾æ‰€ç¤ºã€‚\n\n\næœ€ç®€å•ç›´æ¥çš„æ–¹å¼\n\n\n# æœ€ç®€å•çš„æ–¹å¼g++ main.cpp src/swap.cpp -Iinclude -o result.out # -IæŒ‡æ˜äº†å¤´æ–‡ä»¶æ‰€åœ¨æ–‡ä»¶å¤¹\n\n\nç”Ÿæˆåº“åå†è¿è¡Œ\n\n\né™æ€åº“ç‰ˆæœ¬ï¼šå°†src.cppç”Ÿæˆä¸€ä¸ªé™æ€åº“ï¼Œéšåå†é“¾æ¥åˆ°main.cppä¸Šã€‚\ncd src # åœ¨srcæ–‡ä»¶å¤¹ä¸‹#åœ¨./src/ä¸‹#å¼€å§‹ç”Ÿæˆé™æ€åº“g++ swap.cpp -c -I../include # -cè¡¨ç¤ºç”Ÿæˆä¸€ä¸ªäºŒè¿›åˆ¶æ–‡ä»¶ -Iåˆ¶å®šäº†swap.hä¸­å¤´æ–‡ä»¶æ‰€åœ¨ä½ç½®\t# éšåï¼Œä¼šè¾“å‡ºä¸€ä¸ªswap.oçš„äºŒè¿›åˆ¶æ–‡ä»¶ar rs libSwap.a swap.o # å°†äºŒè¿›åˆ¶æ–‡ä»¶å½’æ¡£ä¸ºé™æ€åº“æ–‡ä»¶libSwap.a\t# éšåï¼Œä¼šè¾“å‡ºä¸€ä¸ªlisSwap.a \t# æ­¤æ—¶ srcå†…ä¿å­˜äº†ï¼šlibSwap.a  swap.cpp  swap.ocd .. # å›åˆ°æ ¸å¿ƒç›®å½•ä¸‹# æ­¤å¤„åœ¨./æ–‡ä»¶å¤¹ä¸‹# é“¾æ¥é™æ€åº“g++ main.cpp -Iinclude -Lsrc -lSwap -o static_main.out # æ³¨æ„ï¼Œè¿™é‡Œçš„-låé¢å†™çš„æ˜¯Swapæ˜¯å› ä¸ºï¼Œ-læ“ä½œä¼šè‡ªåŠ¨çš„åœ¨åæ¥ä¸Šçš„å†…å®¹ï¼ˆSwapï¼‰å‰åŠ libååŠ .aã€‚æ­¤æ—¶-lä¼šå»å¯»æ‰¾libSwap.a\n.vkwdzqnboyqi{zoom:50%;}\n\n\nåŠ¨æ€åº“ç‰ˆæœ¬ï¼š\ncd src#åœ¨./srcä¸‹è¿›è¡Œg++ swap.cpp -I../include -fPIC -shared -o libSwap.so # -fPIC è¡¨ç¤ºç”Ÿæˆä¸ä½ç½®æ— å…³æ–‡ä»¶ï¼ŒåŠ¨æ€é“¾æ¥åº“å¿…å¤‡ã€‚ -sharedè¡¨ç¤ºç”ŸæˆåŠ¨æ€é“¾æ¥åº“cd ../#åœ¨./ä¸‹è¿›è¡Œ#é“¾æ¥åŠ¨æ€åº“g++ main.cpp -Iinclude -Lsrc -lSwap -o share_main.out # åŒç†-lä¼šåŠ ä¸Šä¸œè¥¿ï¼Œä½†æ˜¯ç¼–è¯‘å™¨ä¼šä¼˜å…ˆå°è¯•åŠ¨æ€åº“ï¼ˆlibSwap.so)\n.sopqgydvehxx{zoom:50%;}\n\n\n\n\n\nGDBè°ƒè¯•å™¨\nåœ¨Vscodeä¸­è°ƒè¯•ä»£ç \n.tldfiogmzgxm{zoom:50%;}\nç‚¹å‡»å›¾ç‰‡ä¸­çš„â€åˆ›å»ºlaunch.jsonæ–‡ä»¶&quot;\nlaunch.jsonæ–‡ä»¶çš„ç¼–å†™å¦‚ä¸‹\n// launch.json&#123;    // ä½¿ç”¨ IntelliSense äº†è§£ç›¸å…³å±æ€§ã€‚     // æ‚¬åœä»¥æŸ¥çœ‹ç°æœ‰å±æ€§çš„æè¿°ã€‚    // æ¬²äº†è§£æ›´å¤šä¿¡æ¯ï¼Œè¯·è®¿é—®: https://go.microsoft.com/fwlink/?linkid=830387    &quot;version&quot;: &quot;0.2.0&quot;,    &quot;configurations&quot;: [        &#123;            &quot;name&quot;: &quot;my_cmake_exe&quot;,            &quot;type&quot;: &quot;cppdbg&quot;,            &quot;request&quot;: &quot;launch&quot;,            &quot;program&quot;: &quot;$&#123;workspaceFolder&#125;/build/my_cmake_exe&quot;, //å¯æ‰§è¡Œæ–‡ä»¶æ‰€åœ¨ä½ç½®            &quot;args&quot;: [],            &quot;stopAtEntry&quot;: false,            &quot;cwd&quot;: &quot;$&#123;workspaceFolder&#125;&quot;,            &quot;environment&quot;: [],            &quot;externalConsole&quot;: false,            &quot;MIMode&quot;: &quot;gdb&quot;,            &quot;setupCommands&quot;: [                &#123;                    &quot;description&quot;: &quot;ä¸º gdb å¯ç”¨æ•´é½æ‰“å°&quot;,                    &quot;text&quot;: &quot;-enable-pretty-printing&quot;,                    &quot;ignoreFailures&quot;: true                &#125;            ],            &quot;preLaunchTask&quot;: &quot;Build&quot; // ä¼šè°ƒç”¨tasks.jsonä¸­ç›¸å¯¹çš„æŒ‡ä»¤ï¼Œå®Œæˆè‡ªåŠ¨ç¼–è¯‘        &#125;    ]&#125;\næ³¨æ„ä¿®æ”¹programé€‰é¡¹ï¼ŒæŒ‡å®šä¸ºmy_cmake_exeã€‚åŒæ—¶ï¼Œåœ¨æ ¹ç›®å½•çš„CMakeLists.txtä¸­åº”è¯¥ç¼–å†™set(CMAKE_BUILD_TYPE Debug)ï¼Œå¹¶ä¸”ç¡®ä¿set(CMAKE_CXX_FLAGS &quot;$&#123;CMAKE_CXX_FLAGS&#125; -Wall&quot;)ä¸­æ²¡æœ‰è®¾ç½®-O2é€‰é¡¹ï¼Œé¿å…å› ä¸ºä¼˜åŒ–ä»£ç ï¼Œå¯¼è‡´è°ƒè¯•çš„ä¸å‡†ç¡®ã€‚\néšåé…ç½®tasks.jsonæ–‡ä»¶ã€‚\n.eenxyytvtqus{zoom:50%;}\nç‚¹å‡»â€œé…ç½®é»˜è®¤ç”Ÿæˆä»»åŠ¡â€ï¼Œç”Ÿæˆä¸€ä¸ªtasks.json\n// tasks.json&#123;\t&quot;version&quot;: &quot;2.0.0&quot;,\t&quot;options&quot;: &#123;\t\t&quot;cwd&quot;: &quot;$&#123;workspaceFolder&#125;/build&quot; // ç›¸å½“äº cd $&#123;workspaceFolder&#125;/build\t&#125;,\t&quot;tasks&quot;: [\t\t&#123;\t\t\t&quot;type&quot;: &quot;shell&quot;,\t\t\t&quot;label&quot;: &quot;cmake&quot;, // è¿™ä¸€éƒ¨åˆ†çš„åç§°\t\t\t&quot;command&quot;: &quot;cmake&quot;, // è¿™ä¸€éƒ¨åˆ†è¦æ‰§è¡Œçš„å‘½ä»¤\t\t\t&quot;args&quot;: [\t\t\t\t&quot;..&quot;\t\t\t]\t\t&#125;,\t\t&#123;\t\t\t&quot;label&quot;: &quot;make&quot;,\t\t\t&quot;group&quot;: &#123;\t\t\t\t&quot;kind&quot;: &quot;build&quot;,\t\t\t\t&quot;isDefault&quot;: true\t\t\t&#125;,\t\t\t&quot;command&quot;: &quot;make&quot;,\t\t\t&quot;args&quot;: []\t\t&#125;,\t\t&#123;\t\t\t&quot;label&quot;: &quot;Build&quot;, // è¿™ä¸ªlableåº”è¯¥è¢«è¾“å…¥åˆ°launch.json\t\t\t&quot;dependsOrder&quot;: &quot;sequence&quot;, // æŒ‰ç…§åˆ—å‡ºçš„é¡ºåºæ‰§è¡Œä¾èµ–\t\t\t&quot;dependsOn&quot;: [ //ä¾èµ–é¡¹çš„åå­—å°±æ˜¯å‰é¢çš„label\t\t\t\t&quot;cmake&quot;,\t\t\t\t&quot;make&quot;\t\t\t]\t\t&#125;\t]&#125;\néšåï¼Œæ— éœ€å†æ‰‹åŠ¨ç¼–è¯‘ï¼Œç›´æ¥æ‰§è¡Œè°ƒè¯•å°±å¯ä»¥å¯¹ç¨‹åºè‡ªåŠ¨çš„è¿›è¡Œç¼–è¯‘ã€‚\nå†æ¬¡å±•ç¤ºæ–‡ä»¶ç»“æ„ã€‚\n.bogpxntiuofv{zoom:50%;}\nCMake\nè·¨å¹³å°çš„å®‰è£…ç¼–è¯‘å·¥å…·ï¼Œå¯ä»¥ç”¨äºæè¿°æ‰€æœ‰å¹³å°çš„ç¼–è¯‘è¿‡ç¨‹ã€‚æ²¡æœ‰Cmakeï¼Œä½ å°±éœ€è¦ç»™æ¯ä¸€ä¸ªå¹³å°éƒ½å†™ä¸€ä¸ªå·¥ç¨‹æ„å»ºæ–‡ä»¶ï¼ˆæ¯”å¦‚Makefileã€Xcodeç­‰ï¼‰ã€‚æœ‰äº†Cmakeï¼Œä½ åªéœ€è¦ä¸Cmakeæ‰“äº¤é“å°±å¯ä»¥äº†ï¼Œåªéœ€è¦ä¿®æ”¹CMakeList.txtã€‚\nåŸºæœ¬è¯­æ³•\næŒ‡ä»¤(å‚æ•°1 å‚æ•°2 å‚æ•°3)# ä¸¾ä¾‹set(HELLO hello.cpp)add_executable(hello main.cpp $&#123;HELLO&#125;)IF(HELLO) # IF($&#123;HELLO&#125;)æ˜¯é”™è¯¯çš„ï¼ï¼ï¼\n\n\nå‚æ•°ä¹‹é—´éœ€è¦ç”¨ç©ºæ ¼æˆ–åˆ†å·åˆ†å¼€ï¼Œé€—å·ç”¨ä¸äº†ã€‚\n\n\næŒ‡å®šä¸å¤§å°å†™æ— å…³ï¼Œå‚æ•°å’Œå˜é‡æ˜¯å¤§å°å†™ç›¸å…³çš„ã€‚\n\n\n$&#123;&#125;ç”¨äºå˜é‡å–å€¼ã€‚åœ¨IFè¯­å¥ä¸­å¯ä»¥ç›´æ¥ä½¿ç”¨å˜é‡åã€‚\n\n\nå¸¸ç”¨æŒ‡ä»¤\ncmake_minimum_required(VERSION 2.8.3) # è®¾ç½®CMakeæœ€å°ç‰ˆæœ¬å·ä¸º2.8.3project(HELLOWORLD) # å®šä¹‰å·¥ç¨‹å#æ˜¾å¼å®šä¹‰å˜é‡set(SRC hello.cpp Nihao.cpp) # å®šä¹‰SRCå˜é‡å¹¶è®¾ç½®ä¸ºhello.cpp Nihao.cpp# æ·»åŠ å¤´æ–‡ä»¶æœç´¢è·¯å¾„ ç±»ä¼¼g++çš„-Iinclude_directories(/usr/include ./include) # å°†/usr/include ./include ä¸¤ä¸ªæ–‡ä»¶å¤¹åŠ å…¥åˆ°å¤´æ–‡ä»¶æœç´¢è·¯å¾„# æ·»åŠ åº“æ–‡ä»¶æœç´¢è·¯å¾„ ç±»ä¼¼g++çš„-Llink_directories(/usr/lib ./lib) # å°†/usr/lib ./lib ä¸¤ä¸ªæ–‡ä»¶å¤¹åŠ å…¥åˆ°åº“æ–‡ä»¶æœç´¢è·¯å¾„# ç”Ÿæˆåº“æ–‡ä»¶add_library(hello SHARED $&#123;SRC&#125;) # ä»SRCå–å€¼ï¼Œç”Ÿæˆä¸€ä¸ªå«helloçš„åŠ¨æ€åº“ï¼Œæœ€ç»ˆç”Ÿæˆçš„æ–‡ä»¶æ˜¯libhello.soåº“æ–‡ä»¶add_library(hello STATIC $&#123;SRC&#125;) # ç”Ÿæˆé™æ€åº“# å¢åŠ ç¼–è¯‘é€‰é¡¹add_compile_options(-Wall -std=c++11 -o2) # ç¼–è¯‘çš„æ—¶å€™è¿½åŠ -Wall -std=c++11 -o2è¿™ä¸‰ä¸ªå‘½ä»¤#ç”Ÿæˆå¯æ‰§è¡Œæ–‡ä»¶add_executable(main main.cpp) # å°†main.cppç”Ÿæˆmainçš„å¯æ‰§è¡Œæ–‡ä»¶add_executable(exename source1 source2 sourceN) # æ ‡å‡†è¯­æ³•# æ·»åŠ åŠ¨æ€é“¾æ¥åº“ ç±»ä¼¼g++çš„-ltarget_link_libraries(main hello) # å°†helloè¿™ä¸ªåº“é“¾æ¥åˆ°mainä¸Š# å‘å·¥ç¨‹ä¸­æ·»åŠ å­˜æ”¾æºæ–‡ä»¶çš„å­ç›®å½•add_subdirectory(src) # æ·»åŠ srcå­ç›®å½•ï¼Œéœ€è¦æ³¨æ„çš„æ˜¯å­ç›®å½•ä¸‹å¿…é¡»è¦æœ‰ä¸€ä¸ªCMakeLists.txtï¼ï¼ï¼Œå“ªæ€•æ˜¯ç©ºçš„# ç»™ç›®å½•å†…æ‰€æœ‰çš„æºæ–‡ä»¶å–ä¸€ä¸ªå˜é‡åaux_source_directory(dir VARIABLE) # æ ‡å‡†è¯­æ³•aux_source_directory(. SRC) # æ¡ˆä¾‹,å°†å½“å‰ç›®å½•ä¸‹æ‰€æœ‰çš„æºä»£ç å–åä¸ºSRCadd__executable(main $&#123;SRC&#125;) # å°†SRCä»£æŒ‡çš„æ‰€æœ‰æºæ–‡ä»¶ç”Ÿæˆå¯æ‰§è¡Œæ–‡ä»¶\nå¸¸ç”¨å˜é‡\nCMAKE_C_FLAGS # gccç¼–è¯‘é€‰é¡¹CMAKE_CXX_FLAGS # g++ç¼–è¯‘é€‰é¡¹# ä¸¾ä¾‹ï¼Œåœ¨ç¼–è¯‘é€‰é¡¹åè¿½åŠ ä¸€äº›å†…å®¹set(CMAKE_CXX_FLAGS &quot;&#123;$CMAKE_CXX_FLAGS&#125; -std=c++11&quot;)CMAKE_BUILD_TYPE # è®¾ç½®ç¼–è¯‘ç±»å‹# ä¸¾ä¾‹set(CMAKE_BUILD_TYPE Debug)set(CMAKE_BUILD_TYPE Release)\nç¼–è¯‘æµç¨‹\n\n\nç¼–å†™ä¸€ä¸ªCMakeList.txtã€‚\n\n\næ‰§è¡Œcmake PATHç”ŸæˆMakefileï¼ˆlinuxä¸‹ï¼‰ã€‚\nâ€‹\tPATHæ˜¯é¡¶å±‚CMakeList.txtæ‰€åœ¨çš„ç›®å½•ï¼Œæ¯”å¦‚åœ¨ä¸»ç›®å½•ä¸­ç›´æ¥ç¼–è¯‘å†™å¥½çš„CMakeList.txtï¼Œå°±å¯ä»¥å†™cmake .ã€‚\n\n\næ‰§è¡Œmakeè¿›è¡Œç¼–è¯‘ã€‚\n\n\nåœ¨ç¼–è¯‘è¿‡ç¨‹ä¸­ï¼Œæœ‰ä¸¤ç§æ„å»ºæ–¹å¼ã€‚ç¬¬ä¸€ç§æ˜¯å†…éƒ¨æ„å»ºï¼Œå…¶æ„æ€æ˜¯æŒ‡ç›´æ¥åœ¨å·¥ç¨‹ä¸»ç›®å½•ä¸‹è¿›è¡ŒcmakeæŒ‡ä»¤ï¼Œç›´æ¥å°±æ˜¯cmake .+makeã€‚ç¬¬äºŒç§æ˜¯å¤–éƒ¨ç¼–è¯‘ï¼Œå…¶å«ä¹‰ä¸ºç°åˆ›å»ºä¸€ä¸ªbuildç›®å½•ï¼Œè¿›å…¥åˆ°buildç›®å½•åï¼Œåœ¨ç›®å½•å†…éƒ¨æ‰§è¡Œcmake ..çš„å‘½ä»¤æ¥ç¼–è¯‘ä½äºä¸Šçº§ç›®å½•ä¸­çš„CMakeList.txtï¼Œéšåçš„makeæŒ‡ä»¤ä¹Ÿåœ¨buildç›®å½•ä¸­æ‰§è¡Œã€‚\n","categories":["Programming languages","C++"],"tags":["C++"]},{"title":"Paper Reading: Development of the relationship between visual selective attention and auditory change detection","url":"/2025/02/17/Paper-Reading%EF%BC%9ADevelopment-of-the-relationship-between-visual-selective-attention-and-auditory-change-detection/","content":"Development of the relationship between visual selective attention and auditory change detection\næ–‡ç« é“¾æ¥ï¼šDevelopment of the relationship between visual selective attention and auditory change detection - ScienceDirect\n\n\nè§†è§‰æ‹¥æœ‰æ›´é«˜çš„ç©ºé—´åˆ†è¾¨ç‡ã€å¬è§‰æ‹¥æœ‰æ›´é«˜çš„è§†è§‰åˆ†è¾¨ç‡ -&gt; visual selective attention and auditory change detection.\n\n\nå¬è§‰åœ¨å­•æœŸå°±å¼€å§‹å‘æŒ¥ä½œç”¨ï¼Œè§†è§‰åˆ™åœ¨å‡ºç”Ÿåå¼€å§‹å‘æŒ¥ä½œç”¨ã€‚æ—©èµ·å¬è§‰å¤„ç†å ä¸»å¯¼ï¼Œéšç€å¹´é¾„å¢å¤§ï¼Œæˆå¹´äººè½¬åŒ–ä¸ºè§†è§‰ä¸»å¯¼ã€‚\n\n\n\n\n\nAå›¾ï¼šæˆäººåœ¨è§†è§‰å®šä½ä»»åŠ¡ä¸­çš„ N2 pc æŒ¯å¹…å¤§äºå„¿ç«¥ï¼Œè¡¨æ˜æˆäººåœ¨è§†è§‰é€‰æ‹©æ€§æ³¨æ„ç›¸å…³çš„ç¥ç»åŠ å·¥å¼ºåº¦ä¸Šé«˜äºå„¿ç«¥ã€‚\n\n\nBå›¾ï¼šå„¿ç«¥å’Œæˆäººçš„ MMN æŒ¯å¹…æ— æ˜¾è‘—å·®å¼‚ï¼Œè¯´æ˜åœ¨å¬è§‰å˜åŒ–æ£€æµ‹æ–¹é¢ï¼ŒäºŒè€…åœ¨è¯¥æˆåˆ†åæ˜ çš„ç¥ç»æœºåˆ¶ä¸Šæ— æ˜æ˜¾å¹´é¾„å·®å¼‚ã€‚\n\n\n","categories":["Paper Reading"],"tags":["Paper"]},{"title":"Paper Reading: Interactions between auditory statistics processing and visual experience emerge only in late development","url":"/2025/02/17/Paper-Reading%EF%BC%9AInteractions-between-auditory-statistics-processing-and-visual-experience-emerge-only-in-late-development/","content":"Interactions between auditory statistics processing and visual experience emerge only in late development\næ–‡ç« ç½‘å€ï¼šInteractions between auditory statistics processing and visual experience emerge only in late development: iScience (cell.com)\n\n\nå…ˆå¤©æ€§å’Œè¿Ÿå‘æ€§è§†è§‰å‰¥å¤ºå¯¹æŸäº›é«˜é˜¶å¬è§‰åŠŸèƒ½éƒ½æœ‰è¡¥å¿ä½œç”¨ï¼Œä»è€Œæé«˜äº†ç›¸å…³è¡¨ç°ã€‚\n\n\nå¯¹äºå¬è§‰å¤„ç†çš„æ¨¡å¼ï¼š\n\nå±€éƒ¨ç‰¹å¾åˆ†æï¼šå£°éŸ³é¢‘ç‡ä¸Šçš„ç»†å¾®å·®åˆ«ï¼Œæ¯”å¦‚è½¿è½¦çš„å¯åŠ¨å£°éŸ³å’Œç†„ç«å£°éŸ³\nå…¨å±€è¡¨å¾è®¡ç®—ï¼šé•¿æ—¶é—´çš„ç»Ÿè®¡å¹³å‡ã€‚å¯ä»¥ç†è§£ä¸ºå¯¹è½¿è½¦æ‰€æœ‰æ“ä½œçš„å£°éŸ³å–å¹³å‡åå¾—åˆ°çš„ä¸€ä¸ªå£°éŸ³ï¼Œè¿™ä¸ªå£°éŸ³å¯ä»¥ä»£è¡¨æ±½è½¦\n\n\n\n\n\n\nAå±•ç¤ºäº†å¦‚ä½•å¯¹å£°éŸ³è¿›è¡Œå¤„ç†ï¼Œä»è€Œäº§ç”Ÿä¸åŒçš„æ ·æœ¬ï¼Œä¸ºBå’ŒCæä¾›æ•°æ®æ”¯æŒ\n\n\nDå±•ç¤ºäº†æ•°æ®çš„ç‰¹å¾ï¼Œæ¨ªè½´è¡¨ç¤ºæ—¶é•¿ï¼Œçºµè½´è¡¨ç¤ºæ•°æ®é—´çš„å·®å¼‚æ€§ã€‚å·®å¼‚æ€§è¶Šå¤§ï¼Œè¶Šå®¹æ˜“åˆ†è¾¨ã€‚\n\n\n\n\n\nDï¼ˆæ ·æœ¬è¾¨åˆ«å®éªŒï¼‰ï¼šå±•ç¤ºäº†è§†åŠ›æ­£å¸¸å¯¹ç…§ç»„ï¼ˆé»‘è‰²ï¼‰ã€å…ˆå¤©æ€§å¤±æ˜ç»„ï¼ˆçº¢è‰²ï¼‰å’Œè¿Ÿå‘æ€§å¤±æ˜ç»„ï¼ˆè“è‰²ï¼‰åœ¨æ ·æœ¬è¾¨åˆ«å®éªŒä¸­çš„æ­£ç¡®ç‡ã€‚è¿Ÿå‘æ€§å¤±æ˜ç»„åœ¨éƒ¨åˆ†æ—¶é•¿ä¸‹çš„è¡¨ç°æ˜¾è‘—å·®äºå…¶ä»–ä¸¤ç»„ã€‚\n\n\nEï¼ˆçº¹ç†è¾¨åˆ«å®éªŒï¼‰ï¼šå‘ˆç°äº†ä¸‰ç»„åœ¨çº¹ç†è¾¨åˆ«å®éªŒä¸­çš„æ­£ç¡®ç‡ã€‚ä¸‰ç»„è¡¨ç°éšæ—¶é•¿å¢åŠ çš„è¶‹åŠ¿ç›¸ä¼¼ã€‚\n\n\nF éƒ¨åˆ†ï¼šè®¡ç®—äº†ä¸¤ä¸ªå®éªŒï¼ˆæ ·æœ¬è¾¨åˆ«å‡å»çº¹ç†è¾¨åˆ«ï¼‰çš„ç›¸å¯¹å·®å¼‚ï¼Œå±•ç¤ºäº†ä¸åŒç»„åˆ«è¯¥å·®å¼‚éšå£°éŸ³ç‰‡æ®µæ—¶é•¿çš„å˜åŒ–ã€‚è¿Ÿå‘æ€§å¤±æ˜ç»„åœ¨è¯¥ç›¸å¯¹å·®å¼‚ä¸Šä¸å…¶ä»–ä¸¤ç»„å­˜åœ¨æ˜¾è‘—å·®å¼‚ã€‚\n\n\nDiscussion\nAuditory statistics processing develops regardless of visual input availability\n\n\nåŸºäºè¿‡å¾€ç ”ç©¶ï¼Œå­˜åœ¨ä¸¤ç§é¢„æœŸï¼šä¸€æ˜¯è‹¥è§†è§‰ç»éªŒå¯¹å¬è§‰å¤„ç†èƒ½åŠ›å‘å±•è‡³å…³é‡è¦ï¼Œé‚£ä¹ˆ CB ï¼ˆå…ˆå¤©å¤±æ˜ï¼‰å‚ä¸è€…åœ¨å®éªŒä¸­çš„è¡¨ç°ä¼šé€Šäºè§†åŠ›æ­£å¸¸è€…ï¼ˆSCï¼‰ï¼›äºŒæ˜¯é‰´äºå…ˆå¤©æ€§æˆ–æ—©æœŸå¤±æ˜ä¸ªä½“å­˜åœ¨å¬è§‰å¢å¼ºçš„ç°è±¡ï¼ŒCB ç»„å¯èƒ½ä¼šå‡ºç°ç‰¹å®šçš„è¡¥å¿æœºåˆ¶ï¼Œè¿›è€Œåœ¨å®éªŒä¸­å±•ç°å‡ºæ›´ä¼˜çš„è¡¨ç°\n\n\nå®éªŒç»“æœæ˜¾ç¤ºï¼ŒCB ç»„å’Œ SC ç»„åœ¨å®éªŒä¸­çš„è¡¨ç°å¹¶æ— æ˜¾è‘—å·®å¼‚ã€‚åœ¨æ ·æœ¬è¾¨åˆ«å®éªŒä¸­ï¼Œä¸¤ç»„åœ¨çŸ­æ—¶é•¿ï¼ˆ40ã€91msï¼‰æ—¶ï¼Œæ ·æœ¬è¾¨åˆ«è¡¨ç°å‡ä¼˜äºçº¹ç†è¾¨åˆ«ï¼›åœ¨é•¿æ—¶é•¿ï¼ˆ478ã€1093ã€2500msï¼‰æ—¶ï¼Œçº¹ç†è¾¨åˆ«è¡¨ç°å‡æ›´ä¼˜ã€‚è¿™ä¸€ç»“æœæœ‰åŠ›åœ°è¯æ˜äº†å¬è§‰ç»Ÿè®¡å¤„ç†çš„å‘å±•å¹¶ä¸ä¾èµ–äºæ—©æœŸè§†è§‰ç»éªŒã€‚\n\n\nFunctional interplay between selective auditory computations and vision\n\n\né€šè¿‡å¯¹æ¯”å…ˆå¤©æ€§å¤±æ˜ï¼ˆCBï¼‰ã€è§†åŠ›æ­£å¸¸å¯¹ç…§ç»„ï¼ˆSCï¼‰å’Œè¿Ÿå‘æ€§å¤±æ˜ï¼ˆLBï¼‰ä¸‰ç»„çš„è¡¨ç°ï¼Œå‘ç°è™½ç„¶æ—©æœŸè§†è§‰å¯¹å¬è§‰ç»Ÿè®¡å¤„ç†çš„å…¸å‹å‘å±•å¹¶éå¿…éœ€ï¼Œä½†åœ¨åæœŸå‘å±•é˜¶æ®µï¼Œå¤±æ˜ä»ä¼šå½±å“å±€éƒ¨ç‰¹å¾çš„å¤„ç†ã€‚\n\nå¬è§‰æ„ŸçŸ¥åŠŸèƒ½ï¼ˆä¾‹å¦‚é¢‘ç‡æ£€æµ‹å’ŒæŒ¯å¹…è°ƒåˆ¶ï¼‰å¯èƒ½åœ¨ç‹¬ç«‹äºè§†è§‰è¾“å…¥çš„æƒ…å†µä¸‹å‘å±•ï¼Œåªæœ‰åœ¨åŠŸèƒ½å‘è‚²å®Œæˆåï¼Œæ„Ÿå®˜ä¹‹é—´æ‰ä¼šäº§ç”Ÿäº¤äº’ä½œç”¨ã€‚\nåå¤©å¤±æ˜çš„äººå¯èƒ½æ›´æ³¨é‡ç»Ÿè®¡å¹³å‡è€Œéæ—¶é—´ç»†èŠ‚ï¼Œå› ä¸ºå…¶æœ‰åŠ©äºåœ¨æ—¥å¸¸ç¯å¢ƒä¸­è¯†åˆ«å£°éŸ³å¯¹è±¡ã€‚\n\n\n\nConclusions\n\n\nå¬è§‰å’Œè§†è§‰éƒ½æ˜¯å…ˆç‹¬è‡ªå‘å±•ï¼Œå¾…å„è‡ªæˆç†Ÿåæ‰äº¤äº’ä½œç”¨å…±åŒå‘å±•ã€‚\n\n\n","categories":["Paper Reading"],"tags":["Paper"]},{"title":"Paper Reading: Multisensory Causal Inference in the Brain","url":"/2025/02/20/Paper-Reading%EF%BC%9AMultisensory-Causal-Inference-in-the-Brain/","content":"Multisensory Causal Inference in the Brain\næ–‡ç« é“¾æ¥ï¼šMultisensory Causal Inference in the Brain | PLOS Biology\n\n\nèåˆçš„ä¸¤ä¸ªæ ¸å¿ƒå†…å®¹ï¼š\n\nå¦‚ä½•åˆ¤æ–­ä¸¤ä¸ªæˆ–å¤šä¸ªæ¨¡æ€çš„è¾“å…¥æ¥è‡ªåŒä¸€ä¸ªobjectï¼Ÿ\né’ˆå¯¹æ¥è‡ªåŒä¸€ä¸ªobjectçš„å¤šæ¨¡æ€è¾“å…¥ï¼Œè¯¥å¦‚ä½•èåˆï¼Ÿ\n\n\n\né’ˆå¯¹è¿™ä¸ªé—®é¢˜ï¼Œoptimal probabilistic inference åˆç§° æ˜¯æ ¸å¿ƒçš„è§£å†³æ–¹æ¡ˆ\n","categories":["Paper Reading"],"tags":["Paper"]},{"title":"Paper Reading: PERCEIVER IO: A GENERAL ARCHITECTURE FOR STRUCTURED INPUTS & OUTPUTS","url":"/2025/05/29/Paper-Reading%EF%BC%9APERCEIVER%20IO%20A%20GENERAL%20ARCHITECTURE%20For%20Structured%20Input%20and%20Output/","content":"\nAbstract\nPerceiver IOçš„ç‰¹ç‚¹ï¼š\n\n\nåŸºäºPerceiverï¼ˆ[ç»†è¯»ç»å…¸+ä»£ç è§£æ]Perceiver: General Perception with Iterative Attention - çŸ¥ä¹ (zhihu.com)ï¼‰æ¨¡å‹ï¼Œä½†æ˜¯å¯¹äºoutputçš„è¾“å‡ºè¿›è¡Œä¼˜åŒ–ï¼Œä½¿å…¶åœ¨sizeå’Œå«ä¹‰ä¸Šå¯ä»¥çµæ´»æ”¹å˜ã€‚\n\ninputçš„å°ºå¯¸ä¹Ÿæ˜¯å¯ä»¥çµæ´»æ”¹å˜çš„ã€‚\n\n\n\né€šç”¨æ€§å¼ºï¼Œå¯ä»¥å®Œæˆå¤šç§ä»»åŠ¡ï¼Œä¸æ‹˜æ³¥äºç‰¹å®šåœºæ™¯ã€‚\n\n\nIntroduction\nOutputçš„è·å–æ˜¯æ¥è‡ªäºè¾“å…¥åˆ°Decodeä¸­çš„Output query arrayçš„ï¼Œå¹¶ä¸”éœ€è¦æ³¨æ„åˆ°æ˜¯ï¼ŒOutput query arrayæ˜¯é’ˆå¯¹Outputè¿›è¡Œè®¾è®¡çš„ã€‚å…¶åœ¨è®¾è®¡ä¸Šï¼Œä¸Outputçš„å«ä¹‰é«˜åº¦ç›¸å…³ã€‚\n","categories":["Paper Reading"],"tags":["Paper"]},{"title":"Paper Reading: Rethinking Latent Redundancy in Behavior Cloning: An Information Bottleneck Approach for Robot Manipulation","url":"/2025/07/16/Paper-Reading%EF%BC%9ARethinking%20Latent%20Redundancy%20in%20Behavior%20Cloning/","content":"è®ºæ–‡é¢˜ç›®ï¼šRethinking Latent Redundancy in Behavior Cloning: An Information Bottleneck Approach for Robot Manipulation\nå½“å‰Behavior Cloning(BC)ä¸ºäº†æå‡é€šç”¨æ€§ï¼Œä¼šå¢å¤§æ•°æ®é›†ã€å¢åŠ æ›´å¤šæ¨¡æ€ã€‚ä½†è¿™æ ·çš„é—®é¢˜æ˜¯ï¼Œæ— æ³•å¾—çŸ¥å› å˜é‡ä¸­æ˜¯å¦ä¼šåŒ…å«å¤§é‡çš„å†—ä½™ä¿¡æ¯ã€‚\n","categories":["Paper Reading"],"tags":["Paper"]},{"title":"Paper Reading: Visual Infuences on Auditory Behavioral, Neural, and Perceptual Processes: A Review","url":"/2025/02/04/Paper-Reading%EF%BC%9AVisual-Infuences-on-Auditory-Behavioral,-Neural,-and-Perceptual-Processes-A-Review/","content":"Visual Infuences on Auditory Behavioral, Neural, and Perceptual Processes: A Review\npaper link: Visual Influences on Auditory Behavioral, Neural, and Perceptual Processes: A Review (springer.com)\næ–‡ç« ä¸»è¦çš„ç›®æ ‡ï¼šè§£é‡Šä¿¡æ¯æ˜¯å¦‚ä½•æ•´åˆçš„ã€‚æ•´åˆçš„ç›®çš„æ˜¯å¯¹å†—ä½™å’Œäº’è¡¥ä¿¡æ¯è¿›è¡Œå¤„ç†\nååŠéƒ¨åˆ†æ›´å¤šçš„æ˜¯åœ¨è¯´è§†è§‰å¦‚ä½•å¸®åŠ©å¬è§‰ï¼šæ¯”å¦‚å¸®åŠ©å¬åŠ›æŸå¤±çš„äºº\nVisual Influences on Auditory Perception: Psychophysical Evidence in Humans\nVision Can Enhance Auditory Perceptual Performance\n\n\nä¸€ç§æ„Ÿè§‰æ¨¡æ€çš„ä¿¡æ¯ï¼Œæ— è®ºä¸ä»»åŠ¡æ˜¯å¦ç›¸å…³ï¼Œéƒ½èƒ½å¯¹å¦ä¸€ç§æ„Ÿè§‰æ¨¡æ€çš„æ„ŸçŸ¥åˆ¤æ–­äº§ç”Ÿå½±å“ã€‚è¿™è¯´æ˜ä¸åŒæ„Ÿè§‰æ¨¡æ€ä¹‹é—´å­˜åœ¨ç€å¯†åˆ‡çš„è”ç³»å’Œç›¸äº’ä½œç”¨ï¼Œä¸æ˜¯å­¤ç«‹è¿ä½œçš„ã€‚\n\n\nEnhancement of Auditory Perceptual Performance by Taskâ€‘Irrelevant Visual Information\n\n\nä¸ä»»åŠ¡æ— å…³çš„è§†è§‰èƒ½å¤Ÿå¢å¼ºå¯¹äºå¬è§‰çš„æ„ŸçŸ¥ã€‚ä¸¾ä¾‹è€Œè¨€å°±æ˜¯å½“ä¸€ä¸ªäººåœ¨åˆ¤åˆ«å£°éŸ³å¼ºåº¦çš„ä»»åŠ¡æ—¶ï¼ŒåŠ å…¥ç¯å…‰ä¿¡æ¯ä¼šè®©å—è¯•è€…è¯¯è®¤ä¸ºå£°éŸ³å˜å¼ºäº†ã€‚\n\n\nå½“è§†è§‰åˆºæ¿€é¢†å…ˆäºå¬è§‰åˆºæ¿€æ—¶ï¼Œäººä»¬æœ€å®¹æ˜“æ„ŸçŸ¥åˆ°ä¸¤è€…æ˜¯åŒæ—¶å‘ç”Ÿçš„ã€‚\n\n\nSpatial Ventriloquismï¼šç©ºé—´è…¹è¯­æ•ˆåº”\nè§£é‡Šï¼šå°±æ˜¯åœ¨ç©ºé—´ä¸­ï¼Œå°½ç®¡å£°éŸ³æ˜¯ä»Bç‚¹å‘å‡ºçš„ï¼Œä½†æ˜¯å› ä¸ºè§†è§‰çœ‹åˆ°Aç‚¹æ›´ç¬¦åˆå£°éŸ³çš„å‘å‡ºæ¡ä»¶ï¼ˆæ¯”å¦‚ç”µè§†é‡Œä¸€ä¸ªäººåœ¨è¯´è¯ï¼‰ï¼Œé‚£ä¹ˆå°±ä¼šä½¿äººè®¤ä¸ºå£°éŸ³æ¥è‡ªAç‚¹ã€‚\n\n\nåªæœ‰å½“è§†è§‰ä¿¡æ¯å¯é æ€§è¶³å¤Ÿé«˜çš„æ—¶å€™ï¼Œè¿™ç§æ•ˆåº”æ‰ä¼šäº§ç”Ÿï¼šå³è§†è§‰å¯¹å¬è§‰å ä¸»å¯¼åœ°ä½\n\nå¯é æ€§è¶³å¤Ÿé«˜ï¼šå¬è§‰ä¿¡æ¯å’Œè§†è§‰ä¿¡æ¯åœ¨ç©ºé—´å’Œæ—¶é—´ä¸Šçš„ä¸€è‡´æ€§å¼º\n\n\n\n::: tips\næç¤º\nå½“å¬è§‰ä¿¡æ¯ä¸åœ¨ç©ºé—´å’Œæ—¶é—´ä¸Šä¸€è‡´çš„è§†è§‰ä¿¡æ¯é…å¯¹æ—¶ï¼Œå¬è§‰æ„ŸçŸ¥è¡¨ç°èƒ½å¤Ÿå¾—åˆ°æå‡ï¼Œæ— è®ºè¿™äº›è§†è§‰ä¿¡æ¯ä¸ä»»åŠ¡æ˜¯å¦ç›¸å…³ã€‚\n:::\nMechanistic Principles of Visual Influences on Auditory Perception\n\nè‡ªä¸‹è€Œä¸Šæ˜¯æ­£å‘ä¼ æ’­ï¼Œè‡ªä¸Šè€Œä¸‹åˆ™æ˜¯åå‘ä¼ æ’­\n\n\nå¯¹äºComputationséƒ¨åˆ†çš„è§£é‡Šï¼šè“è‰²æ˜¯è§†è§‰æ„ŸçŸ¥çš„æ¦‚ç‡åˆ†å¸ƒï¼Œçº¢è‰²æ˜¯å¬è§‰æ„ŸçŸ¥çš„æ¦‚ç‡åˆ†å¸ƒï¼Œç´«è‰²æ˜¯èåˆåçš„æ¦‚ç‡åˆ†å¸ƒï¼Œå‘ä¸‹çš„é»‘è‰²ç®­å¤´æ˜¯å¯¹ç»“æœçš„æœ€ä¼˜ä¼°è®¡ã€‚\n\n\né»‘è‰²å‚ç›´çº¿è¡¨ç¤ºcausal inferenceã€fusionå’Œsensory processingæœ‰å¯èƒ½å‘ç”Ÿçš„ä½ç½®\n\n\n\næ³¨æ„\nsensory processingå›¾ä¸­é»‘è‰²ç®­å¤´ä¸ºå•¥ä¸æŒ‡æ¦‚ç‡æœ€é«˜çš„è§†è§‰ï¼Ÿ\n\n","categories":["Paper Reading"],"tags":["Paper"]},{"title":"Pythonæ‹¾é—","url":"/2025/02/21/Python%E6%8B%BE%E9%81%97/","content":"ä»£ç ä»“åº“ï¼šhttps://gitee.com/qianminghuang/python-learning.git\n\næç¤º\nPythonä¸­ä¸‡ç‰©çš†æ˜¯å¯¹è±¡\n\n\nâ„¹ï¸ Note\nPythonä¸­ä¸‡ç‰©çš†æ˜¯å¯¹è±¡\n\nå¯å˜ç±»å‹ä¸ä¸å¯å˜ç±»å‹\n\n\nå¯å˜ç±»å‹ï¼šåœ¨å‡½æ•°å‚æ•°ä¼ é€’æ—¶ï¼Œç±»ä¼¼äºå¼•ç”¨ã€‚å¦‚å­—å…¸ã€åˆ—è¡¨ã€é›†å’Œã€‚\n\n\nä¸å¯å˜ç±»å‹ï¼šåœ¨å‡½æ•°å‚æ•°ä¼ é€’æ—¶ï¼Œç±»ä¼¼äºå€¼ä¼ é€’ã€‚å¦‚æ•´æ•°ã€å­—ç¬¦ä¸²ã€å…ƒç´ ã€‚\n\n\nå¯ä»¥ä½¿ç”¨.copy()æ¥å®ç°å¯¹äº å¯å˜å¯¹è±¡ çš„å¤åˆ¶ï¼Œè€Œä¸è¦ä½¿ç”¨=ã€‚=ä¼šä½¿è¿ä¸ªå˜é‡æŒ‡å‘åŒä¸€ä¸ªå¯å˜ç±»å‹å¯¹è±¡çš„åœ°å€\nlist1 = [1, 2, 3]list2 = list1.copy()list3 = list1 list1.append(4)print(list1, f&quot;åœ°å€ä¸º&#123;id(list1)&#125;&quot;)print(list2, f&quot;åœ°å€ä¸º&#123;id(list2)&#125;&quot;)print(list3, f&quot;åœ°å€ä¸º&#123;id(list3)&#125;&quot;)\nè¾“å‡ºï¼š\n[1, 2, 3, 4] åœ°å€ä¸º2227037551360[1, 2, 3] åœ°å€ä¸º2227037228800[1, 2, 3, 4] åœ°å€ä¸º2227037551360\nä½†æ˜¯.copy()æœ¬è´¨ä¸Šè¿˜æ˜¯æµ…æ‹·è´ï¼Œå½“å¯¹è±¡ä¸­å«æœ‰æŒ‡é’ˆæ—¶ï¼Œæµ…æ‹·è´å°±ä¼šäº§ç”Ÿé£é™©ã€‚è¿™æ—¶å°±éœ€è¦ä½¿ç”¨æ·±æ‹·è´.deepcopy()ã€‚.deepcopy()æ˜¯ä¾èµ–äºcopy()åº“çš„ã€‚\nimport copyoriginal = [1,2,3, [4, 5]] # [4, 5]æœ¬è´¨ä¿å­˜çš„æ˜¯æŒ‡å‘å…¶çš„æŒ‡é’ˆlist_copy = original.copy() # æµ…æ‹·è´list_deepcopy = copy.deepcopy(original) # æ·±æ‹·è´original[3].append(6) # ä¿®æ”¹åŸåˆ—è¡¨ä¸­çš„åµŒå¥—åˆ—è¡¨print(f&quot;original = &#123;original&#125;&quot;)print(f&quot;list_copy = &#123;list_copy&#125;&quot;)print(f&quot;list_deepcopy = &#123;list_deepcopy&#125;&quot;)\nè¾“å‡ºï¼š\noriginal = [1, 2, 3, [4, 5, 6]]list_copy = [1, 2, 3, [4, 5, 6]]list_deepcopy = [1, 2, 3, [4, 5]]\nå‡è®¾originalæŒ‡å‘äº† [1,2,3,æŒ‡é’ˆ1] ï¼Œç”±äºlist_copyæ˜¯æµ…æ‹·è´è€Œæ¥çš„ï¼Œæ‰€ä»¥å…¶æŒ‡å‘çš„æ˜¯å¦ä¸€ä¸ªå†…å­˜å•å…ƒçš„[1,2,3,æŒ‡é’ˆ1]ã€‚è€Œå› ä¸ºlist_deepcopyæ˜¯æ·±æ‹·è´è€Œæ¥ï¼Œæ‰€ä»¥å…¶å¯¹åˆ—è¡¨å†…éƒ¨çš„å…ƒç´ ä¸ä»…ä»…æ˜¯ç®€å•çš„å¤åˆ¶å€¼ï¼Œä½†é‡åˆ°æŒ‡é’ˆæ—¶ï¼Œå…¶ä¼šå¼€è¾Ÿä¸€ä¸ªæ–°çš„å†…å­˜å•å…ƒï¼Œç„¶åæŒ‡å‘æ–°çš„å†…å­˜å•å…ƒã€‚å¦‚ä¸‹å›¾æ‰€ç¤ºã€‚\n.yvrnkrzjukuw{zoom:60%;}\næ‰€ä»¥å½“originalä¸­æŒ‡é’ˆ1æ‰€ç¤ºæŒ‡å‘çš„ç©ºé—´ä¸­çš„å†…å®¹è¢«ä¿®æ”¹åï¼Œlist_copyä¸­æŒ‡é’ˆ1æ‰€æŒ‡å‘çš„ç©ºé—´ä¸­çš„å†…å®¹åŒæ ·è¢«ä¿®æ”¹ï¼Œå…¶äºŒè€…æŒ‡å‘çš„æ˜¯åŒä¸€ä¸ªå†…å­˜ç©ºé—´ã€‚\néœ€è¦æ³¨æ„åˆ°æ˜¯ï¼Œè¿™å¼ å›¾ä¸­åªæ˜¯å±•ç¤ºæ¦‚å¿µã€‚çœŸæ­£çš„æ•°ç»„ä¿å­˜ä¸å¯èƒ½æ˜¯ä¸€ä¸ªå†…å­˜å•å…ƒå­˜ä¸‹äº†æ‰€æœ‰å†…å®¹ï¼Œè€Œæ˜¯å¼€è¾Ÿä¸€æ®µè¿ç»­çš„å†…å­˜ç©ºé—´ä¿å­˜è¿™äº›å€¼ï¼Œoriginalåˆ™æŒ‡å‘äº†è¿™æ®µè¿ç»­çš„å†…å­˜ç©ºé—´çš„èµ·å§‹åœ°å€ã€‚\nå‡½æ•°\n::: tips\næç¤º\nå½“å‡½æ•°æ²¡æœ‰è¿”å›å€¼çš„æ—¶å€™ï¼Œå‡½æ•°é»˜è®¤è¿”å›Noneã€‚\n:::\nNoneçš„åº”ç”¨\n\n\nå¯ä»¥ç”¨äºifåˆ¤æ–­ï¼ŒNoneç­‰ä»·äºFalse\n\n\nç”¨äºå£°æ˜æ— åˆå§‹å†…å®¹çš„å˜é‡\nname = None # è¡¨ç¤ºåç»­å†ç»™nameèµ‹å€¼\n\n\nå˜é‡çš„ä½œç”¨åŸŸ\nåœ¨å‡½æ•°å†…éƒ¨ï¼Œå¯ä»¥ä½¿ç”¨globalå…³é”®è¯æ¥è®¾ç½®å…¨å±€å˜é‡ã€‚\ndef test():    global a # å°†å‡½æ•°å†…éƒ¨çš„å˜é‡å£°æ˜ä¸ºå…¨å±€å˜é‡    a = 10    print(f&quot;aåœ¨å‡½æ•°ä¸­çš„åœ°å€:&#123;id(a)&#125;,å€¼ä¸º&#123;a&#125;&quot;)a = 20print(f&quot;aåœ¨å‡½æ•°å¤–çš„åœ°å€:&#123;id(a)&#125;,å€¼ä¸º&#123;a&#125;&quot;)test()\nè¾“å‡ºï¼š\n.yhjftspcqshh{zoom: 67%;}\nLambdaåŒ¿åå‡½æ•°\nå‡½æ•°çš„å®šä¹‰æœ‰ä¸¤ç§ï¼š\nâ€‹\tdefå…³é”®å­—çš„æ–¹å¼ï¼Œå®šä¹‰æœ‰åç§°çš„å‡½æ•°ï¼Œå¯ä»¥é‡å¤ä½¿ç”¨ã€‚\nâ€‹\tlambdaå…³é”®å­—ï¼Œå®šä¹‰åŒ¿åå‡½æ•°ï¼Œåªèƒ½ä¸´æ—¶ä½¿ç”¨ä¸€æ¬¡ã€‚\nåŒ¿åå‡½æ•°çš„å®šä¹‰è¯­æ³•ï¼š\nlambda ä¼ å…¥å‚æ•°: å‡½æ•°ä½“\næ³¨æ„ï¼Œæ­¤å¤„å‡½æ•°ä½“åªèƒ½ä¸€è¡Œã€‚\nå…·ä½“æ¡ˆä¾‹ï¼š\ndef test_func(compute):    result = compute(1, 2)    print(result)def compute(x, y):    return x + ytest_func(compute)# åŒ¿åå‡½æ•°test_func(lambda x, y: x + y)\nè¾“å‡ºï¼š\nâ€‹\t\nlambdaå‡½æ•°å…·ä½“åº”ç”¨åœºæ™¯ï¼š\n\n\nä¸´æ—¶æ„å»ºä¸€ä¸ªå‡½æ•°ï¼Œåªç”¨ä¸€æ¬¡çš„åœºæ™¯\n\n\nå‡½æ•°ä½“åªç”¨ä¸€è¡Œ\n\n\nå‡½æ•°å‚æ•°\nä¸å®šé•¿å‚æ•°\nä½ç½®ä¼ é€’çš„ä¸å®šé•¿å‚æ•°\nä»¥*å¼€å¤´çš„å‚æ•°ï¼Œå…¶ä¼šæ ¹æ®ä¼ è¿›å‚æ•°çš„ä½ç½®åˆå¹¶å‡ºä¸€ä¸ªå…ƒç¥–ã€‚\ndef user_info_1(*args):    print(args)user_info_1(1, 2, 3, 4, 5)user_info_1(&quot;test&quot;, 10)\nè¾“å‡ºï¼š\n.ifgpphukrvpx{zoom: 50%;}\nå…³é”®å­—ä¼ é€’çš„ä¸å®šé•¿å‚æ•°\nä»¥**å¼€å¤´ï¼Œå‚æ•°æ˜¯é”®å€¼å¯¹çš„å½¢å¼ï¼Œè¿”å›çš„æ˜¯ä¸€ä¸ªå­—å…¸ã€‚\ndef user_info_2(**kwargs):    print(kwargs)    user_info_2(name=&quot;test&quot;, age=10, sex = &quot;ç”·&quot;)user_info_2(10,20) # æŠ¥é”™ï¼Œä¸æ¥å—ä½ç½®å‚æ•°\nè¾“å‡ºï¼š\n.dtnwvopsdbou{zoom: 67%;}\nç»“åˆä½¿ç”¨ï¼š\ndef user_info_3(*args, **kwargs):    print(args)    print(kwargs)user_info_3(1, 2, 3, 4, 5, name=&quot;test&quot;, age=10, sex = &quot;ç”·&quot;)user_info_3(10,20)user_info_3(1, 2, name=&quot;test&quot;, 4, age=10, sex = &quot;ç”·&quot;) # æŠ¥é”™ï¼Œä½ç½®å‚æ•°ä¸èƒ½åœ¨å…³é”®å­—å‚æ•°åé¢\nè¾“å‡ºï¼š\n.pndqgawovlbx{zoom:67%;}\nå‡½æ•°ä½œä¸ºå‚æ•°\ndef test_func(compute_func, a, b):    result = compute_func(a, b) + 10    print(result)def compute(x, y):    return x + ytest_func(compute, 1, 2)\nè¾“å‡ºï¼š\n.nhwllfhbmjwu{zoom: 67%;}\næœ¬è´¨ä¸Šä¼ é€’çš„æ˜¯ä»£ç çš„æ‰§è¡Œé€»è¾‘\næ•°æ®ç±»å‹\n.pudttvlpwtzc{zoom: 67%;}\n\n\nåˆ—è¡¨ï¼š[]ï¼Œä¸€æ‰¹æ•°æ®ï¼Œå¯ä¿®æ”¹\n\n\nå…ƒç»„ï¼š()ï¼Œä¸€æ‰¹æ•°æ®ï¼Œä¸å¯ä¿®æ”¹\n\n\né›†å’Œï¼šset()  &#123;&#125;ï¼Œç”¨äºå»é‡æ“ä½œ\n\n\nå¼‚å¸¸\näº†è§£å¼‚å¸¸\nå¼‚å¸¸ï¼šç¨‹åºè¿è¡Œçš„æ—¶å€™æ£€æµ‹åˆ°ä¸€ä¸ªé”™è¯¯ï¼Œè¿™ä¸ªé”™è¯¯å°†å¯¼è‡´Pythonè§£é‡Šå™¨æ— æ³•ç»§ç»­æ‰§è¡Œã€‚\nä¹Ÿå¯ä»¥ç†è§£ä¸ºä¸€ä¸ªBugã€‚\nå¼‚å¸¸çš„æ•è·æ–¹å¼\n\n\nå¼‚å¸¸æ•è·çš„ç›®çš„\nå¯¹å¯èƒ½å‡ºç°çš„å¼‚å¸¸ï¼ˆbugï¼‰ï¼Œæå‰åšå‡†å¤‡ã€‚å…·ä½“è€Œè¨€å°±æ˜¯å¯¹Bugè¿›è¡Œæé†’ï¼Œå¹¶ä¸”è®©æ•´ä¸ªç¨‹åºç»§ç»­è¿è¡Œã€‚è¿™æ ·å°±å¯ä»¥é¿å…å› ä¸ºä¸€ä¸ªBugè€Œå¯¼è‡´çš„æ•´ä¸ªç¨‹åºçš„åœæ­¢ã€‚\n\n\næ•è·å¸¸è§„å¼‚å¸¸\nåŸºæœ¬è¯­æ³•ï¼š\n  try:\tå¯èƒ½é”™è¯¯çš„ä»£ç except:\tå¦‚æœå‡ºç°å¼‚å¸¸æ‰§è¡Œçš„ä»£ç \næ¡ˆä¾‹ï¼š\n  try:\tf = open(&quot;text.txt&quot;,&#x27;r&#x27;)except:    print(&quot;å‡ºç°äº†å¼‚å¸¸ï¼Œä½¿ç”¨æ–°çš„æ–¹æ¡ˆ&quot;)\tf = open(&quot;linux.txt&quot;,&#x27;w&#x27;)\nå½“tryè¯­å¥ä¸­å‡ºç°å¼‚å¸¸åï¼Œç¨‹åºä¸ä¼šæŠ›å‡ºå¼‚å¸¸ï¼Œè€Œæ˜¯ä½¿ç”¨exceptæ¥è®©ç¨‹åºç»§ç»­è¿è¡Œ\næ•è·æŒ‡å®šçš„å¼‚å¸¸\nåŸºæœ¬è¯­æ³•ï¼š\ntry:\tå¯èƒ½é”™è¯¯çš„ä»£ç except æŒ‡å®šçš„å¼‚å¸¸ as å¼‚å¸¸çš„åˆ«å:\tå¦‚æœå‡ºç°å¼‚å¸¸æ‰§è¡Œçš„ä»£ç \næ¡ˆä¾‹ï¼š\ntry:    print(name) # æ³¨æ„ï¼Œå‰æ–‡å¹¶æ²¡æœ‰å®šä¹‰nameexcept NameError as e:     print(&quot;nameæœªå®šä¹‰&quot;)    print(e) # æŠŠå¼‚å¸¸æ‰“å°å‡ºæ¥\nç»“æœï¼š\n.axlxiaooobno{zoom:80%;}\næ•è·å¤šä¸ªå¼‚å¸¸\nåŸºæœ¬è¯­æ³•ï¼š\ntry:    å¯èƒ½é”™è¯¯çš„ä»£ç except (å¼‚å¸¸1, å¼‚å¸¸2): # ä¹Ÿå¯ä»¥ï¼šexcept (å¼‚å¸¸1, å¼‚å¸¸2) as e\tå¦‚æœå‡ºç°å¼‚å¸¸æ‰§è¡Œçš„ä»£ç \næ•è·æ‰€æœ‰å¼‚å¸¸\næœ€ç®€å•çš„try   exceptè¯­å¥å°±å¯ä»¥æ•è·æ‰€æœ‰å¼‚å¸¸äº†ï¼Œä½†æ˜¯æ²¡æ³•å¯¹æ‰€æ•è·åˆ°çš„å¼‚å¸¸è¿›è¡Œè¾“å‡ºã€‚\nåŸºæœ¬è¯­æ³•ï¼š\ntry:    å¯èƒ½é”™è¯¯çš„ä»£ç except Exception as å¼‚å¸¸çš„åˆ«å: # Exceptionå¯ä»¥è¢«è®¤ä¸ºæ˜¯æ‰€æœ‰å¼‚å¸¸çš„çˆ¶ç±»    å¦‚æœå‡ºç°å¼‚å¸¸æ‰§è¡Œçš„ä»£ç \né€šè¿‡è¾“å‡ºå¼‚å¸¸çš„åˆ«åå°±å¯ä»¥çœ‹åˆ°æ•è·çš„å¼‚å¸¸æ˜¯ä»€ä¹ˆã€‚\nelse ä¸ finally\nelseç”¨äºè¡¨ç¤ºåœ¨æ²¡æœ‰å¼‚å¸¸çš„æ—¶å€™è¯¥æ‰§è¡Œä»€ä¹ˆè¡Œä¸ºã€‚\nåŸºæœ¬è¯­æ³•ï¼š\ntry:    å¯èƒ½é”™è¯¯çš„ä»£ç except Exception as å¼‚å¸¸çš„åˆ«å:    å¦‚æœå‡ºç°å¼‚å¸¸æ‰§è¡Œçš„ä»£ç else:    æ²¡æœ‰å¼‚å¸¸æ—¶æ‰§è¡Œçš„ä»£ç finally:    æœ‰æ²¡æœ‰å¼‚å¸¸éƒ½æ‰§è¡Œçš„ä»£ç \n::: tips\næç¤º\nfinallyä¸­ä¸€èˆ¬éƒ½æ˜¯ç”¨æ¥æ‰§è¡Œèµ„æºå…³é—­æ“ä½œï¼Œæ¯”å¦‚æ–‡ä»¶çš„å…³é—­\n:::\næ¡ˆä¾‹ï¼š\ntry:    print(&quot;æ­£å¸¸æ‰§è¡Œ&quot;)except Exception as e:    print(e)else:    print(&quot;æ²¡æœ‰å¼‚å¸¸&quot;)\nè¾“å‡ºï¼š\n.nqokvvonxesg{zoom:67%;}\nå¼‚å¸¸çš„ä¼ é€’\nå‡è®¾æœ‰å‡½æ•°è°ƒç”¨å…³ç³»å¦‚ä¸‹\nmain()|--func01()   |--func02()\nå½“func02()çš„ä»£ç ä¸­å‡ºç°å¼‚å¸¸åï¼Œå¼‚å¸¸ä¼šå‘ä¸Šé€çº§ä¼ é€’ï¼ˆä»func02() -&gt; func01() -&gt; main() ï¼‰ï¼Œè¿™æ¡ä¼ é€’çº¿ä¸Šçš„ä»»æ„ä¸€ä¸ªå‡½æ•°æœ‰å¯¹å¼‚å¸¸çš„æ•è·éƒ½å°†ä½¿ç¨‹åºå¯ä»¥ç»§ç»­æ‰§è¡Œã€‚\næŠ›å‡ºå¼‚å¸¸\nä½¿ç”¨raiseå¯ä»¥æŠ›å‡ºä¸€ä¸ªæˆ‘ä»¬æƒ³è¦çš„å¼‚å¸¸ã€‚\nåŸºç¡€è¯­æ³•ï¼š\nraise å¼‚å¸¸\næŠ›å‡ºçš„å¼‚å¸¸ä¸€å®šè¦æ˜¯å¼‚å¸¸çš„å®ä¾‹æˆ–è€…ä½¿å¼‚å¸¸çš„ç±»ï¼Œå³Exceptionçš„å­ç±»ã€‚ä¾‹å¦‚å¯ä»¥æ˜¯NameError('that is a name error')ã€‚å› ä¸ºNameErroræ˜¯Exceptionçš„ä¸€ä¸ªå­ç±»ï¼Œæ‰€ä»¥è¿™é‡Œåˆ›å»ºäº†ä¸€ä¸ªå¼‚å¸¸çš„åŒ¿åå¯¹è±¡ã€‚\nä¸¾ä¾‹ï¼š\nx = 4if x &lt; 5:    raise Exception(&quot;xå°äº5&quot;)\nè¾“å‡ºï¼š\n  File &quot;c:/Users/QianmingHuang/Desktop/Python-Learning/å¼‚å¸¸.py&quot;, line 32, in raise_func    raise Exception(&quot;xå°äº5&quot;)Exception: xå°äº5\nè‡ªå®šä¹‰çš„å¼‚å¸¸\nç”¨æˆ·éœ€è¦åˆ›å»ºä¸€ä¸ªç»§æ‰¿äº†Exceptionç±»çš„ç±»ã€‚\n::: tips\næç¤º\næœ‰æ—¶å€™éœ€è¦ç»™ä¸€ä¸ªæ¨¡å—å†™è‹¥å¹²ä¸ªè‡ªå®šä¹‰çš„å¼‚å¸¸\n:::\nä¸¾ä¾‹ï¼š\nclass ExamError(Exception):    &quot;&quot;&quot;åŸºäºè€ƒè¯•çš„è‡ªå®šä¹‰å¼‚å¸¸ç±»&quot;&quot;&quot;    passclass SubmissionError(ExamError):    &quot;&quot;&quot;æäº¤æ—¶å‘ç”Ÿçš„é”™è¯¯&quot;&quot;&quot;    def __init__(self, message=&quot;Cannot submit after the exam has ended&quot;):        self.message = message        super().__init__(self.message)def submit_exam(student, answers, exam_end_time):    from datetime import datetime    current_time = datetime.now()    if current_time &gt; exam_end_time:        raise SubmissionError(&quot;You cannot submit the exam after the deadline.&quot;)    else:        # å¤„ç†æäº¤é€»è¾‘        print(&quot;Exam submitted successfully!&quot;)# å‡è®¾è€ƒè¯•ç»“æŸæ—¶é—´æ˜¯ä¹‹å‰çš„æŸä¸ªæ—¶é—´from datetime import datetime, timedeltaexam_end_time = datetime.now() - timedelta(hours=1)try:    submit_exam(&quot;John Doe&quot;, [&quot;A&quot;, &quot;B&quot;, &quot;C&quot;], exam_end_time)except SubmissionError as e:    print(f&quot;Submission failed: &#123;e&#125;&quot;)\nè¾“å‡ºç»“æœï¼š\nSubmission failed: You cannot submit the exam after the deadline.\nåœ¨ä¸Šé¢è¿™ä¸ªæ¡ˆä¾‹ä¸­å°±å±•ç¤ºäº†è‡ªå®šä¹‰å¼‚å¸¸çš„ä¸€ä¸ªåœºæ™¯ã€‚å¯¹äºæäº¤çš„å¼‚å¸¸å¯ä»¥ä¸“é—¨å†™ä¸€ä¸ªç±»SubmissionErrorï¼Œå¯¹äºåˆ«çš„å¼‚å¸¸ï¼Œæ¯”å¦‚ç­”é¢˜ä¸­çš„å¼‚å¸¸ï¼Œä¹Ÿå¯ä»¥ä¸“é—¨å†™ä¸€ä¸ªç±»æ¥åº”å¯¹ã€‚\né¢å‘å¯¹è±¡\nç±»\nå˜é‡å = ç±»å() # ç±»çš„å®ä¾‹åŒ–\nç±»å()åˆ›å»ºå‡ºäº†ä¸€ä¸ªåŒ¿åå¯¹è±¡ï¼Œå˜é‡åä¹Ÿæ˜¯ä¸€ä¸ªå¯¹è±¡ï¼Œå…¶æŒ‡å‘äº†è¿™ä¸ªåŒ¿åå¯¹è±¡ã€‚æ›´ç¡®åˆ‡çš„è¯´ï¼Œå˜é‡åæ˜¯é€šè¿‡ç±»å()åˆ›å»ºå‡ºçš„åŒ¿åå¯¹è±¡çš„å¼•ç”¨ã€‚\nå¯¹äºæ¯ä¸€ä¸ªå¯¹è±¡æ¥è¯´ï¼Œå…¶éƒ½æ‹¥æœ‰è‡ªå·±çš„å†…å­˜ç©ºé—´ï¼Œä¿å­˜ç€è‡ªå·±çš„å±æ€§ã€‚ä½†æ˜¯æ¥è‡ªåŒä¸€ä¸ªç±»çš„ä¸åŒå¯¹è±¡å…±äº«ç±»çš„æ–¹æ³•ï¼Œä¹Ÿå°±æ˜¯è¯´ç±»çš„æ–¹æ³•åœ¨å†…å­˜ä¸­åªæœ‰ä¸€ä»½ï¼Œåœ¨ä½¿ç”¨ç±»çš„æ–¹æ³•æ—¶ï¼Œè¦æŠŠå¯¹è±¡çš„å¼•ç”¨ï¼ˆå¯ä»¥ç†è§£ä¸ºå½“å‰è°ƒç”¨æ­¤æ–¹æ³•çš„å®ä¾‹çš„åœ°å€ï¼Œç±»æ¯”C++ä¸­çš„thisæŒ‡é’ˆï¼‰ä¼ é€’åˆ°æ–¹æ³•å†…éƒ¨ã€‚\nåœ¨pythonä¸­æœ‰ä¸¤ç§ç±»ã€‚åˆ†åˆ«æ˜¯æ–°å¼ç±»å’Œæ—§å¼ç±»ï¼Œä¸»è¦åŒºåˆ«æ˜¯æ–°å¼ç±»ç»§æ‰¿è‡ªobjectç±»ï¼Œè€Œæ—§å¼ç±»ä¸æ˜¯ã€‚è‡ªpython3å¼€å§‹ï¼Œä¸€åˆ‡çš„ç±»éƒ½æ˜¯æ–°å¼ç±»\n\n\n__del__æ–¹æ³•\nå¦‚æœè¯´__init__æ–¹æ³•ç±»æ¯”äº C++ çš„æ„é€ å‡½æ•°ï¼Œé‚£ä¹ˆ__del__æ–¹æ³•å°±å¯ä»¥ç±»æ¯”äº C++ çš„ææ„å‡½æ•°ã€‚\n\n\n__str__æ–¹æ³•\næ­¤æ–¹æ³•çš„ä½œç”¨æ˜¯å’±è¾“å‡ºå¯¹è±¡å˜é‡çš„æ—¶å€™ï¼Œå¯ä»¥è¾“å‡ºè‡ªå®šä¹‰çš„å†…å®¹ã€‚\nclass ç±»å1ï¼š    def __str__(self):    \treturn æƒ³è¾“å‡ºçš„å­—ç¬¦ä¸²å¯¹è±¡å˜é‡å1 = ç±»å1()print(å¯¹è±¡å˜é‡å1)\n\næ³¨æ„\n__str__æ–¹æ³•çš„è¿”å›å€¼å¿…é¡»æ˜¯ å­—ç¬¦ä¸²\n\n\n\nç§æœ‰å±æ€§å’Œç§æœ‰æ–¹æ³•\n\n\nå®šä¹‰æ–¹æ³•ï¼š\nåœ¨å±æ€§æˆ–è€…æ–¹æ³•å‰é¢åŠ ä¸Šä¸¤ä¸ªä¸‹åˆ’çº¿\nclass Women:    def __init__(self, name):        self.name = name        self.__age = 18  # ç§æœ‰å±æ€§ï¼Œå¤–éƒ¨ä¸å¯è®¿é—®            def __secret(self):  # ç§æœ‰æ–¹æ³•ï¼Œå¤–éƒ¨å¯ä¸è®¿é—®        print(&quot;%s çš„å¹´é¾„æ˜¯ %d&quot; % (self.name, self.__age))\n\n\npythonä¸­æ²¡æœ‰çœŸæ­£æ„ä¹‰ä¸Šçš„ç§æœ‰\n\n\nåœ¨ç»™å±æ€§ã€æ–¹æ³•å‘½åæ˜¯ï¼Œå®é™…ä¸Šæ˜¯å¯¹åç§°ä½œäº†ä¸€äº›ç‰¹æ®Šå¤„ç†ï¼Œä½¿å¤–ç•Œæ— æ³•è®¿é—®åˆ°\n\n\né€šè¿‡_ç±»å__ç§æœ‰æ–¹æ³•\\å±æ€§å¯ä»¥è®¿é—®ç§æœ‰æ–¹æ³•å’Œå±æ€§\nxiaohong = Women(&quot;xiaohong&quot;)xiaohong._Women__secret() # æ­¤æ—¶å°±å¯ä»¥è®¿é—®ç§æœ‰æ–¹æ³•å•¦\n\n\n\n\nç±»ä¹Ÿæ˜¯å¯¹è±¡\nå¼€ç¯‡ç¬¬ä¸€å¥è¯æ˜¯ï¼špythonä¸­ä¸‡ç‰©çš†ä¸ºå¯¹è±¡ã€‚ç±»ä¹Ÿä¸æ„å¤–ï¼Œå¯ä»¥æŠŠå…¶çœ‹åšç±»å¯¹è±¡ã€‚\nç±»å¯¹è±¡åœ¨å†…å­˜ä¸­åªæœ‰ä¸€ä¸ªï¼Œä¸€ä¸ªç±»å¯¹è±¡å¯ä»¥åˆ›å»ºå¤šä¸ªå®ä¾‹ã€‚\næ­£å› ä¸ºç±»æ˜¯ä¸€ä¸ªå¯¹è±¡ï¼Œæ‰€ä»¥å…¶æ‹¥æœ‰è‡ªå·±çš„ ç±»å±æ€§ å’Œ ç±»æ–¹æ³•ã€‚äºŒè€…éƒ½å¯ä»¥é€šè¿‡ç±»åÂ·çš„æ–¹å¼æ¥è®¿é—®\n\n\nç±»å±æ€§\nä»…ä»…ç”¨äºè®°å½•ç±»çš„ç‰¹å¾ï¼Œæ— æ³•ç”¨äºè®°å½•å®ä¾‹ç‰¹å¾ã€‚\nclass Counter:    # ç±»å±æ€§ï¼šç”¨äºç»Ÿè®¡å®ä¾‹æ•°é‡    instance_count = 0    def __init__(self):        Counter.instance_count += 1 # é€šè¿‡ ç±»å.ç±»å±æ€§ çš„æ–¹å¼è°ƒç”¨\nä¹Ÿå¯ä»¥ä½¿ç”¨å®ä¾‹.ç±»å±æ€§çš„æ–¹å¼è°ƒç”¨ç±»å±æ€§ï¼ˆå‘ä¸ŠæŸ¥æ‰¾æœºåˆ¶ï¼‰ï¼Œä½†æ˜¯ååˆ†ä¸æ¨èã€‚æ¯”å¦‚ å®ä¾‹.ç±»å±æ€§ = 1 çš„æ—¶å€™ï¼Œå®é™…ä¸Šæ˜¯ç»™å®ä¾‹å¢åŠ äº†ä¸€ä¸ªå±æ€§ï¼Œè€Œä¸æ˜¯å¯¹ç±»å±æ€§çš„å€¼è¿›è¡Œäº†ä¿®æ”¹ã€‚\n\n\nç±»æ–¹æ³•\nç±»æ–¹æ³•éœ€è¦ç”¨åˆ°ä¿®é¥°å™¨@classmethodï¼Œå…¶åªèƒ½è®¿é—® ç±»å±æ€§ å’Œ ç±»æ–¹æ³•ã€‚\nclass ParentClass:    class_variable = &quot;è¿™æ˜¯çˆ¶ç±»çš„ç±»å˜é‡&quot;    @classmethod # å¿…é¡»è¦æœ‰@classmethodä¿®é¥°    def class_method(cls): # ç±»æ–¹æ³•çš„ç¬¬ä¸€ä¸ªå‚æ•°å¿…é¡»æ˜¯cls        return f&quot;è°ƒç”¨äº† &#123;cls.__name__&#125; çš„ç±»æ–¹æ³•ï¼Œç±»å˜é‡å€¼ä¸º: &#123;cls.class_variable&#125;&quot;class ChildClass(ParentClass):    class_variable = &quot;è¿™æ˜¯å­ç±»çš„ç±»å˜é‡&quot;# å­ç±»è°ƒç”¨çˆ¶ç±»çš„ç±»æ–¹æ³•result = ChildClass.class_method()print(result)\nè¾“å‡ºï¼š\nè°ƒç”¨äº† ChildClass çš„ç±»æ–¹æ³•ï¼Œç±»å˜é‡å€¼ä¸º: è¿™æ˜¯å­ç±»çš„ç±»å˜é‡\nå› ä¸ºclsçš„å­˜åœ¨ï¼Œç±»æ–¹æ³•çŸ¥é“å…¶æ˜¯è¢«å“ªä¸ªç±»å¯¹è±¡è°ƒç”¨çš„ã€‚å½“ChildClassç±»è°ƒç”¨äº†ç±»æ–¹æ³•class_method()åï¼Œclsæ”¶åˆ°äº†ChildClassç±»çš„å¼•ç”¨ï¼ˆå¯ä»¥ç†è§£ä¸ºclsæ­¤æ—¶æˆä¸ºäº†ChildClassï¼‰ï¼Œä¹Ÿå°±è°ƒç”¨äº†ChildClassçš„ç±»å±æ€§ã€‚\nclass Student:    def __init__(self, name, age):        self.name = name        self.age = age    @classmethod    def from_string(cls, info):        name, age = info.split(&#x27; &#x27;)        return cls(name, age)  # æ­¤å¤„ç­‰ä»·äºStudent(name, age)ï¼Œå› ä¸ºclsæŒ‡å‘Studentç±»s1 = Student(&quot;å°æ˜&quot;, 10)s2 = Student(&quot;å°æ&quot;, 20)s3 = Student.from_string(&quot;å°çº¢ 14&quot;)  # è°ƒç”¨ç±»æ–¹æ³•å¤„ç†ç‰¹æ®Šè¾“å…¥for s in [s1, s2, s3]:    print(f&quot;s.name = &#123;s.name&#125;, s.age = &#123;s.age&#125;&quot;)\nè¾“å‡ºï¼š\ns.name = å°æ˜, s.age = 10s.name = å°æ, s.age = 20s.name = å°çº¢, s.age = 14\n\n\né™æ€æ–¹æ³•\né™æ€æ–¹æ³•éœ€è¦ç”¨åˆ°ä¿®é¥°å™¨@staticmethodã€‚å…¶æ—¢ä¸éœ€è¦clsä¹Ÿä¸éœ€è¦selfä½œä¸ºè¾“å…¥ï¼Œå¥½ä¼¼ç‹¬ç«‹äºç±»è€Œå­˜åœ¨ã€‚ä½†æ˜¯å…¶ä¾æ—§å¯ä»¥ç”¨ç±»å.çš„æ–¹å¼æ¥è®¿é—®ã€‚\nclass MathUtils:    @staticmethod    def add(a, b):        return a + b# è°ƒç”¨é™æ€æ–¹æ³•result = MathUtils.add(3, 5)print(f&quot;3 å’Œ 5 ç›¸åŠ çš„ç»“æœæ˜¯: &#123;result&#125;&quot;)print(id(MathUtils))print(id(MathUtils.add))\nè¾“å‡ºï¼š\n3 å’Œ 5 ç›¸åŠ çš„ç»“æœæ˜¯: 821535934431682153611265360\nå¯ä»¥å‘ç°ï¼Œé™æ€æ–¹æ³•å¹¶ä¸ä¿å­˜åœ¨ç±»å¯¹è±¡ä¸­ã€‚\nåµŒå¥—ç±»\nç±»ä¸­æ˜¯å¯ä»¥å†å®šä¹‰ç±»çš„ã€‚\nclass A:    class B:        pass\nè¿™ç­‰ä»·äºä¸‹é¢çš„å†™æ³•ï¼š\nclass B:    passclass A:    b = B()\nè¿™å°±æ˜¯è¯´ç±»Bçš„å®ä¾‹bæ˜¯ç±»Açš„ç±»æ–¹æ³•\nç»§æ‰¿\né™¤äº†ç»§æ‰¿çˆ¶ç±»çš„å±æ€§å’Œæ–¹æ³•å¤–ï¼Œè¿˜å¯ä»¥å¯¹çˆ¶ç±»çš„æ–¹æ³•è¿›è¡Œé‡å†™ã€‚é‡å†™å¯ä»¥åˆ†ä¸ºä¸¤ç§æ–¹å¼ã€‚\n\n\nè¦†ç›–ï¼šåœ¨å­ç±»ä¸­å®šä¹‰äº†ä¸€ä¸ªå’Œçˆ¶ç±»åŒåçš„æ–¹æ³•å¹¶ä¸”å®ç°ã€‚\n\n\næ‰©å±•ï¼šåœ¨å­ç±»ä¸­å®šä¹‰äº†ä¸€ä¸ªå’Œçˆ¶ç±»åŒåçš„æ–¹æ³•ï¼Œåœ¨å…¶ä¸­ä½¿ç”¨äº†çˆ¶ç±»åŒåçš„æ–¹æ³•ã€‚è¿™æ—¶å°±éœ€è¦superç±»å‡ºåœºäº†ã€‚\né€šè¿‡super()çš„æ–¹å¼ç”Ÿæˆä¸€ä¸ªåŒ¿åå¯¹è±¡ï¼Œå…¶ä¼šæŒ‰ç…§__mro__å±æ€§çš„é¡ºåºæ¥è®¿é—®çˆ¶ç±»ã€‚\nclass A:    def method(self):        print(&quot;A çš„æ–¹æ³•&quot;)class B(A):    def method(self):        print(&quot;B çš„æ–¹æ³•&quot;)        super().method()class C(A):    def method(self):        print(&quot;C çš„æ–¹æ³•&quot;)        super().method()class D(B, C):    def method(self):        print(&quot;D çš„æ–¹æ³•&quot;)        super().method()d = D()d.method()print(D.__mro__)\nè¾“å‡ºï¼š\nD çš„æ–¹æ³•B çš„æ–¹æ³•C çš„æ–¹æ³•A çš„æ–¹æ³•(&lt;class &#x27;__main__.D&#x27;&gt;, &lt;class &#x27;__main__.B&#x27;&gt;, &lt;class &#x27;__main__.C&#x27;&gt;, &lt;class &#x27;__main__.A&#x27;&gt;, &lt;class &#x27;object&#x27;&gt;)\nå¯ä»¥çœ‹å‡ºï¼Œçˆ¶ç±»æ–¹æ³•è°ƒç”¨çš„é¡ºåºå°±æ˜¯__mro__ä¸­è®¾ç½®çš„é¡ºåºã€‚å› æ­¤ï¼Œ__mro__å°†ä¼šå¸®åŠ©ç”¨æˆ·æ¥è§£å†³æ¥è‡ªä¸åŒçˆ¶ç±»çš„åŒåå‡½æ•°çš„è°ƒç”¨é—®é¢˜ã€‚\n::: danger\nè­¦å‘Š\nä½†æ˜¯ï¼Œæœ€å¥½ä¸è¦é€šä¿—çš„è®¤ä¸ºsuper()å°±æ˜¯ä¸€ä¸ªçˆ¶ç±»å¯¹è±¡ã€‚å¯ä»¥å…·ä½“çœ‹åé¢çš„è¿™ä¸¤ç»„å®éªŒã€‚\n:::\nclass ParentClass:    @classmethod    def class_method(cls):        print(f&quot;è°ƒç”¨äº† &#123;cls.__name__&#125; çš„ç±»æ–¹æ³•&quot;)        print(id(cls))class ChildClass(ParentClass):    @classmethod    def class_method(cls):        print(f&quot;è°ƒç”¨äº† &#123;cls.__name__&#125; çš„ç±»æ–¹æ³•ï¼Œç°åœ¨è¦è°ƒç”¨çˆ¶ç±»çš„ç±»æ–¹æ³•&quot;)        print(id(cls))        super().class_method()# è°ƒç”¨å­ç±»çš„ç±»æ–¹æ³•ChildClass.class_method()\nè¾“å‡ºï¼š\nè°ƒç”¨äº† ChildClass çš„ç±»æ–¹æ³•ï¼Œç°åœ¨è¦è°ƒç”¨çˆ¶ç±»çš„ç±»æ–¹æ³•2153593471488è°ƒç”¨äº† ChildClass çš„ç±»æ–¹æ³•2153593471488\nç°åœ¨ï¼Œåªå°†super()æ”¹æˆParentClassï¼Œå³çˆ¶ç±»ç±»åã€‚\nclass ParentClass:    @classmethod    def class_method(cls):        print(f&quot;è°ƒç”¨äº† &#123;cls.__name__&#125; çš„ç±»æ–¹æ³•&quot;)        print(id(cls))class ChildClass(ParentClass):    @classmethod    def class_method(cls):        print(f&quot;è°ƒç”¨äº† &#123;cls.__name__&#125; çš„ç±»æ–¹æ³•ï¼Œç°åœ¨è¦è°ƒç”¨çˆ¶ç±»çš„ç±»æ–¹æ³•&quot;)        print(id(cls))        ParentClass.class_method() # æ­¤å¤„åšå‡ºäº†ä¿®æ”¹ï¼# è°ƒç”¨å­ç±»çš„ç±»æ–¹æ³•ChildClass.class_method()\nè¾“å‡ºï¼š\nè°ƒç”¨äº† ChildClass çš„ç±»æ–¹æ³•ï¼Œç°åœ¨è¦è°ƒç”¨çˆ¶ç±»çš„ç±»æ–¹æ³•2153593459216è°ƒç”¨äº† ParentClass çš„ç±»æ–¹æ³•2153593465824\nå¯ä»¥çœ‹å‡ºï¼Œæ­¤æ—¶çš„clsçš„åœ°å€æ˜¯ä¸ä¸€æ ·çš„ã€‚è€Œä½¿ç”¨super()çš„æ—¶å€™ï¼Œclsçš„åœ°å€åˆ™æ˜¯ç›¸åŒçš„ã€‚\n\n\nå¤šæ€\nåœ¨C++ä¸­ï¼Œå¤šæ€çš„å®ç°æ˜¯é€šè¿‡çˆ¶ç±»æŒ‡é’ˆæˆ–å¼•ç”¨æŒ‡å‘å­ç±»å¯¹è±¡å®ç°çš„ã€‚è€Œpythonä¸­ä¹Ÿæ˜¯å¦‚æ­¤ï¼Œåªä¸è¿‡å› ä¸ºpythonä¸­æ²¡æœ‰æŒ‡é’ˆçš„æ˜¾å¼æ¦‚å¿µï¼Œæ‰€ä»¥å¯ä»¥è¿›ä¸€æ­¥ç†è§£ä¸ºï¼šä¸åŒçš„å­ç±»å¯¹è±¡è°ƒç”¨åŒåçš„çˆ¶ç±»æ–¹æ³•ï¼Œäº§ç”Ÿä¸åŒçš„æ‰§è¡Œç»“æœã€‚\n# å®šä¹‰ä¸€ä¸ªåŸºç±»class Animal:    def speak(self):        pass# å®šä¹‰ Dog ç±»ï¼Œç»§æ‰¿è‡ª Animal ç±»class Dog(Animal):    def speak(self):        return &quot;æ±ªæ±ªæ±ªï¼&quot;# å®šä¹‰ Cat ç±»ï¼Œç»§æ‰¿è‡ª Animal ç±»class Cat(Animal):    def speak(self):        return &quot;å–µå–µå–µï¼&quot;# å®šä¹‰ä¸€ä¸ªå‡½æ•°ï¼Œæ¥æ”¶ä¸€ä¸ª Animal ç±»å‹çš„å¯¹è±¡def animal_speak(animal):    print(animal.speak())# åˆ›å»º Dog å’Œ Cat çš„å®ä¾‹dog = Dog()cat = Cat()# è°ƒç”¨ animal_speak å‡½æ•°ï¼Œä¼ å…¥ä¸åŒçš„å¯¹è±¡animal_speak(dog)animal_speak(cat)\nè¾“å‡ºï¼š\næ±ªæ±ªæ±ªï¼å–µå–µå–µï¼\nå¤šçº¿ç¨‹\næ¯ä¸ªçº¿ç¨‹éƒ½æœ‰å±äºå…¶è‡ªå·±çš„ä¸€ç»„CPUå¯„å­˜å™¨ï¼ˆæ¯”å¦‚æŒ‡ä»¤æŒ‡é’ˆå’Œå †æ ˆæŒ‡é’ˆå¯„å­˜å™¨ï¼‰ï¼Œè¿™ç»„CPUå¯„å­˜å™¨è¢«ç§°ä¸ºâ€œçº¿ç¨‹çš„ä¸Šä¸‹æ–‡â€ã€‚çº¿ç¨‹çš„ä¸Šä¸‹æ–‡å¯ä»¥è¢«æƒ³è±¡ä¸ºä¸€ä¸ªçº¿ç¨‹çš„â€œä¸ªäººä¿¡æ¯åŒ…â€ï¼Œå®ƒåŒ…å«äº†çº¿ç¨‹è¿è¡Œæ—¶éœ€è¦çš„å„ç§çŠ¶æ€ä¿¡æ¯ã€‚\nå½“çº¿ç¨‹è¢«è°ƒåº¦æ‰§è¡Œæ—¶ï¼ŒCPU ä¼šæ ¹æ®è¯¥çº¿ç¨‹çš„ä¸Šä¸‹æ–‡ä¿¡æ¯æ¥è®¾ç½® CPU å¯„å­˜å™¨çš„çŠ¶æ€ï¼Œä½¿å…¶æ¢å¤åˆ°ä¸Šæ¬¡è¯¥çº¿ç¨‹è¿è¡Œæ—¶çš„çŠ¶æ€ã€‚è¿™æ ·ï¼Œçº¿ç¨‹å°±å¯ä»¥ä»ä¸Šæ¬¡åœæ­¢çš„åœ°æ–¹ç»§ç»­æ‰§è¡Œä¸‹å»ã€‚æ¯”å¦‚ï¼Œçº¿ç¨‹åœ¨æ‰§è¡Œä¸€ä¸ªå¤æ‚çš„æ•°å­¦è®¡ç®—ï¼Œç”±äºæŸç§åŸå› ï¼ˆå¦‚æ—¶é—´ç‰‡ç”¨å®Œï¼‰æš‚åœäº†æ‰§è¡Œï¼Œå½“æ—¶ CPU å¯„å­˜å™¨ä¸­ä¿å­˜äº†è®¡ç®—çš„ä¸­é—´ç»“æœå’ŒæŒ‡ä»¤æŒ‡é’ˆç­‰ä¿¡æ¯ã€‚å½“è¯¥çº¿ç¨‹å†æ¬¡è¢«è°ƒåº¦æ‰§è¡Œæ—¶ï¼Œé€šè¿‡å…¶ä¸Šä¸‹æ–‡æ¢å¤ CPU å¯„å­˜å™¨çš„çŠ¶æ€ï¼Œå°±èƒ½å¤Ÿç»§ç»­è¿›è¡Œä¹‹å‰æœªå®Œæˆçš„è®¡ç®—ï¼Œè€Œä¸ä¼šä¸¢å¤±ä¹‹å‰çš„å·¥ä½œè¿›åº¦ã€‚\n\næ³¨æ„\nå¤šçº¿ç¨‹æ˜¯å®è§‚ä¸Šçš„å¹¶è¡Œï¼Œå¾®è§‚ä¸Šçš„å¹¶å‘ã€‚\n\nçº¿ç¨‹æ˜¯å¯ä»¥è¢«ä¸­æ–­çš„ã€‚å½“åˆ«çš„çº¿ç¨‹æ­£åœ¨è¿è¡Œæ—¶ï¼Œçº¿ç¨‹å¯ä»¥æš‚æ—¶è¿›å…¥ç¡çœ çŠ¶æ€ï¼Œå³çº¿ç¨‹çš„é€€è®©ã€‚\nåŒä¸€ä¸ªè¿›ç¨‹ä¸‹çš„å¤šä¸ªçº¿ç¨‹æ˜¯å…±äº«å†…å­˜ç©ºé—´çš„ã€‚\n\n\nçº¿ç¨‹å¯ä»¥è¢«åˆ†ä¸ºä¸¤ç§ï¼š\n\nå†…æ ¸çº¿ç¨‹ï¼šæ“ä½œç³»ç»Ÿå†…æ ¸è¿›è¡Œç®¡ç†\nç”¨æˆ·çº¿ç¨‹ï¼šç”¨æˆ·åœ¨ç¨‹åºä¸­ç®¡ç†\n\n\n\nthreadingæ¨¡å—\nthreadingæ¨¡å—ä¸­çš„Threadç±»å°±æ˜¯ç”¨äºåˆå§‹åŒ–çº¿ç¨‹çš„ã€‚å¯ä»¥é€šè¿‡å…¶ç›´æ¥ç”Ÿæˆä¸€ä¸ªçº¿ç¨‹ç¤ºä¾‹ã€‚\nimport threadingimport timedef print_numbers():    for i in range(5):        time.sleep(1)        print(i)# åˆ›å»ºçº¿ç¨‹thread = threading.Thread(target=print_numbers)# å¯åŠ¨çº¿ç¨‹thread.start()# ç­‰å¾…çº¿ç¨‹ç»“æŸthread.join()\nè¾“å‡ºï¼š\n01234\n\n\nThreadç±»çš„__init__å‡½æ•°æœ‰ç”¨çš„å‚æ•°ï¼š\n\n\ntargetï¼šçº¿ç¨‹è¦æ‰§è¡Œçš„ç›®æ ‡å‡½æ•°\n\n\nnameï¼šçº¿ç¨‹åå­—\n\n\nargsï¼šç›®æ ‡å‡½æ•°çš„å‚æ•°ï¼ŒæŒ‰ä½ç½®ä¼ é€’\n\n\nkwargsï¼šç›®æ ‡å‡½æ•°çš„å‚æ•°ï¼ŒæŒ‰å…³é”®å­—ä¼ é€’\nimport threadingdef print_info(a = 10, b = 20, c = 30, d = 40, e = 50):    print(f&quot;a: &#123;a&#125;, b: &#123;b&#125;, c: &#123;c&#125;, d: &#123;d&#125;, e: &#123;e&#125;&quot;)# åˆ›å»ºçº¿ç¨‹å¹¶åŒæ—¶ä¼ é€’ä½ç½®å‚æ•°å’Œå…³é”®å­—å‚æ•°thread = threading.Thread(target=print_info, args=(1,), kwargs=&#123;&quot;c&quot; : 5, &quot;e&quot;: 4&#125;)# å¯åŠ¨çº¿ç¨‹thread.start()# ç­‰å¾…çº¿ç¨‹ç»“æŸthread.join()\nè¾“å‡ºï¼š\na: 1, b: 20, c: 5, d: 40, e: 4\n\n\ndaemonï¼šçº¿ç¨‹æ˜¯å¦ä¸ºå®ˆæŠ¤çº¿ç¨‹ã€‚\n::: tips\næç¤º\nå®ˆæŠ¤çº¿ç¨‹æ˜¯ä¸€ç§ç‰¹æ®Šçš„çº¿ç¨‹ï¼Œå…¶ä¼šåœ¨ä¸»çº¿ç¨‹é€€å‡ºæ—¶è‡ªåŠ¨ç»ˆæ­¢ã€‚è¿™å°±æ„å‘³ç€ï¼Œå½“ç¨‹åºä¸­åªå‰©ä¸‹å®ˆæŠ¤çº¿ç¨‹çš„æ—¶å€™ï¼Œä¸»çº¿ç¨‹ä¼šç›´æ¥ç»“æŸã€‚ä¸ä¹‹ç›¸å¯¹çš„æ˜¯éå®ˆæŠ¤çº¿ç¨‹ï¼Œä¸»çº¿ç¨‹ä¼šç­‰å¾…æ‰€æœ‰éå®ˆæŠ¤çº¿ç¨‹æ‰§è¡Œå®Œæ¯•åæ‰ä¼šé€€å‡ºç¨‹åºã€‚\n:::\nimport threadingimport timedef daemon_function():    print(&quot;Daemon thread started&quot;)    time.sleep(5)    print(&quot;Daemon thread finished&quot;)def non_daemon_function():    print(&quot;Non-daemon thread started&quot;)    time.sleep(2)    print(&quot;Non-daemon thread finished&quot;)# åˆ›å»ºå®ˆæŠ¤çº¿ç¨‹daemon_thread = threading.Thread(target=daemon_function, daemon=True)# åˆ›å»ºéå®ˆæŠ¤çº¿ç¨‹non_daemon_thread = threading.Thread(target=non_daemon_function)# å¯åŠ¨çº¿ç¨‹daemon_thread.start()non_daemon_thread.start()print(&quot;\\nMain thread continues...&quot;)# ä¸»çº¿ç¨‹ä¸åšé¢å¤–ç­‰å¾…ï¼Œç›´æ¥ç»“æŸ\nè¾“å‡ºï¼š\nDaemon thread startedNon-daemon thread startedMain thread continues...Non-daemon thread finished\n::: danger\nè­¦å‘Š\nåœ¨jupyter notebookä¸­çš„ç»“æœå¯èƒ½ä¸åŒã€‚å…¶å¯èƒ½ä¼šè¾“å‡ºåˆ° Daemon thread finishedã€‚\n:::\n\n\nä½¿ç”¨åœºæ™¯ï¼š\n\nåå°ä»»åŠ¡ï¼šå½“ç”¨æˆ·éœ€è¦åœ¨ç¨‹åºè¿è¡Œè¿‡ç¨‹ä¸­æ‰§è¡Œä¸€äº›åå°ä»»åŠ¡ï¼Œä¸”è¿™äº›ä»»åŠ¡ä¸éœ€è¦ä¿è¯ä¸€å®šæ‰§è¡Œå®Œæ¯•æ—¶ï¼Œå¯ä»¥ä½¿ç”¨å®ˆæŠ¤çº¿ç¨‹ã€‚ä¾‹å¦‚ï¼Œæ—¥å¿—è®°å½•ã€ç›‘æ§ç³»ç»ŸçŠ¶æ€ç­‰ä»»åŠ¡ã€‚\né¿å…èµ„æºæ³„éœ²ï¼šå¦‚æœçº¿ç¨‹æ‰§è¡Œçš„ä»»åŠ¡åœ¨ä¸»çº¿ç¨‹é€€å‡ºæ—¶æ²¡æœ‰å¿…è¦ç»§ç»­æ‰§è¡Œï¼Œå°†å…¶è®¾ç½®ä¸ºå®ˆæŠ¤çº¿ç¨‹å¯ä»¥é¿å…èµ„æºæ³„æ¼ã€‚æ¯”å¦‚ï¼Œä¸€äº›ä¸´æ—¶æ–‡ä»¶çš„æ¸…ç†å·¥ä½œï¼Œå¦‚æœä¸»çº¿ç¨‹å·²ç»é€€å‡ºï¼Œè¿™äº›æ¸…ç†å·¥ä½œä¹Ÿå°±æ²¡æœ‰å¿…è¦ç»§ç»­æ‰§è¡Œäº†ã€‚\n\n\n\n\n\n\n\n\n&nbsp;Threadç±»ä¸­çš„å…¶ä»–æ–¹æ³•ã€å±æ€§\n\nstart(self)ï¼šå¯åŠ¨çº¿ç¨‹ï¼Œå°†è°ƒç”¨çº¿ç¨‹çš„ã€‚\n\n\nrunæ–¹æ³•ã€‚run(self)ï¼šçº¿ç¨‹åœ¨æ­¤æ–¹æ³•ä¸­å®šä¹‰éœ€è¦æ‰§è¡Œçš„ä»£ç ã€‚\n\n\njoin(self, timeout=None)ï¼šç­‰å¾…çº¿ç¨‹ç»ˆæ­¢ã€‚æ­¤æ–¹æ³•ä¼šä¸€ç›´é˜»å¡ï¼Œç›´åˆ°è¢«è°ƒç”¨çš„çº¿ç¨‹ç»ˆæ­¢ã€‚\n\n\ntimeoutå‚æ•°å®šä¹‰äº†æœ€å¤šç­‰å¤šå°‘ç§’ã€‚\n\n\nis_alive(self)ï¼šåˆ¤æ–­çº¿ç¨‹æ˜¯å¦è¿˜åœ¨è¿è¡Œã€‚\n\n\ngetName(self)ï¼šè·å–çº¿ç¨‹åå­—ã€‚\n\n\nsetName(self)ï¼šè®¾ç½®åå­—ã€‚\n\n\nidentï¼šçº¿ç¨‹çš„å”¯ä¸€æ ‡è¯†ç¬¦ã€‚\n\n\nisDaemon(self)ï¼šè¿”å›æ˜¯å¦ä¸ºå®ˆæŠ¤çº¿ç¨‹ã€‚\n\n\n\né™¤äº†ç›´æ¥ä½¿ç”¨Threadç±»æ¥åˆå§‹åŒ–ä¸€ä¸ªçº¿ç¨‹å®ä¾‹ï¼Œè®¾ç½®ä¸€ä¸ªç»§æ‰¿å…¶çš„å­ç±»ï¼Œå¹¶é‡å†™å…¶runæ–¹æ³•ä¹Ÿå¯ä»¥å®ç°å¤šçº¿ç¨‹ã€‚\n\n","categories":["Programming languages","Python"],"tags":["Python"]},{"title":"What is a standard CPP","url":"/2025/10/06/What-is-a-standard-CPP/","content":"From this Blog, I will start writing blogs in English. Since this is my first attempt to write English version blogs and I am not a native speaker of English, there might be some issues with grammar, spelling, or the naturalness of expression. If you ever find anything unclear or awkward, I sincerely apologize and I would really appreciate it if you could tell me the problem by email (brad_huang@outlook.com). Thank you for your understanding and support! Let us get start!\n\nLet us take a look of a piece of code:\n#include &lt;bits/stdc++.h&gt;using namespace std;int main()&#123;    cout &lt;&lt; &quot;hello world&quot; &lt;&lt; endl;    return 0;&#125;\nAt first glance, how many problems â€” that is , instance of nonstandard code â€” can you find? Well, except for int main() , which is standard, everything else is nonstandard. Let me explain it step by step.\nFirstly, the first line #include &lt;bits/stdc++.h&gt; is a universal header. but it is not part of the C++ standard library. Many compilers donâ€™t support it (except gcc). Moreover, it may cause serious naming conflicts and introduce a large number of macros, both of which may lead to unexpected erros â€” a real disaster for a big project with thousands of lines of code. In addition, this nonstandard header may increase the time spent for compiling. In short,  avoid using bits/stdc++.h !\nSecondly, using namespace std should not be used here at all, because it may cause undesirable name collision. For example, if you define your own class string and include using namespace std, the compiler will be confused when it encounters string. It doesnâ€™t know whether to use your class or the one from the standard library. Thus, do not use using namespace std;. On the contrary, prefix names with std:: (as shown in the example below).\nusing std::cout;using std::endl;std::cout &lt;&lt; &quot;hello world&quot; &lt;&lt; std::endl; // Recommoand !! \nThirdly,  themain function doesnâ€™t need a return statement. Although it is okay for you to write return 0; at the end of main function and I believe many students were taught to include it when they first learned C++, the compiler can automatically insert return 0; at the end of main function. But let me emphasize it again, it is still completely okay for you to write return 0; at end of main function (even I still like to include it myself).\n","categories":["Programming languages","C++"],"tags":["C++"]},{"title":"Hello World","url":"/2025/01/15/hello-world/","content":"Welcome to Hexo! This is your very first post. Check documentation for more info. If you get any problems when using Hexo, you can find the answer in troubleshooting or you can ask me on GitHub.\nQuick Start\nCreate a new post\n$ hexo new &quot;My New Post&quot;\nMore info: Writing\nRun server\n$ hexo server\nMore info: Server\nGenerate static files\n$ hexo generate\nMore info: Generating\nDeploy to remote sites\n$ hexo deploy\nMore info: Deployment\n"},{"title":"æ³¨æ„åŠ›å¯è§†åŒ–é¢„å¤‡","url":"/2025/05/02/%E6%B3%A8%E6%84%8F%E5%8A%9B%E5%8F%AF%E8%A7%86%E5%8C%96%E9%A2%84%E5%A4%87/","content":"å›¾è®ºå‰æ\n\n\nåº¦ï¼šä¸ç»“ç‚¹è¿æ¥çš„è¾¹æ•°ã€‚\n\nå¯¹äºæœ‰å‘å›¾\nå‡ºåº¦ï¼šä»ç»“ç‚¹å‡ºå»çš„è¾¹ã€‚\nå…¥åº¦ï¼šè¿›å…¥ç»“ç‚¹çš„è¾¹ã€‚\n\n\n\né‚»æ¥çŸ©é˜µï¼š\n\n\næ¯ä¸€ä¸ªä½ç½®ï¼šï¼ˆa,bï¼‰=1è¡¨ç¤ºaåˆ°bæœ‰ä¸€æ¡è¾¹\n\n\næ¯ä¸€è¡Œçš„å’Œï¼šè¡¨ç¤ºè¡Œå¤´ï¼ˆaï¼‰çš„æ€»å‡ºåº¦ã€‚\n\n\næ¯ä¸€åˆ—çš„å’Œï¼šè¡¨ç¤ºåˆ—å¤´ï¼ˆaï¼‰çš„æ€»å…¥åº¦ã€‚\n\n\né‚»æ¥çŸ©é˜µç›¸ä¹˜ï¼šç›¸ä¹˜å‡ æ¬¡è¡¨ç¤ºè·¯å¾„é•¿åº¦ï¼Œï¼ˆaï¼Œbï¼‰çš„å€¼è¡¨ç¤ºå½“å‰è·¯å¾„é•¿åº¦ä¸‹çš„è¾¹æ•°ã€‚\n\næ³¨æ„\nä½¿ç”¨æƒé‡è¡¨ç¤ºçš„é‚»æ¥çŸ©é˜µç›¸ä¹˜åçš„ç»“æœä¸å…·å¤‡ç±»ä¼¼å¯è§£é‡Šæ€§ã€‚\n\n\n\n\n\nè®ºæ–‡å‚è€ƒ\nQuantifying Attention Flow in Transformers\n\n\n\nSince in Transformer decoder, future tokens are masked, naturally there is more attention toward initial tokens in the input sequence, and both attention rollout and attention flow will be biased toward these tokens. Hence, to apply these methods on a Transformer decoder, we should first normalize based on the receptive field of attention.\n\n\n\n\n\nå­˜åœ¨çš„é—®é¢˜ï¼šæœ‰æ©ç æ—¶ï¼Œæ­¤æ–¹æ³•å¯èƒ½ä¼šå¤±æ•ˆã€‚\n\n\nè§£å†³æ–¹æ¡ˆï¼šæ ¹æ®æ¥æ”¶åŸŸæ¥å½’ä¸€åŒ–ã€‚\n\n\næ³¨æ„åŠ›çŸ©é˜µï¼šé‚»æ¥çŸ©é˜µçš„è½¬ç½®ã€‚\n\n\næœ¬æ–‡å¯¹äºå…¶å†…éƒ¨å­˜åœ¨çš„å±€é™æ€§ä¸Š\n\n\nä½¿ç”¨effective attention weights æ›¿ä»£raw attention(A=0.5Watt+0.5I)è¿›è¡Œé€’å½’è®¡ç®—\neffective attention weightsçš„ç›®çš„æ˜¯ä»raw attentionä¸­æ‰¾å‡ºçœŸæ­£å½±å“ç»“æœçš„é‚£éƒ¨åˆ†ï¼Œè§£å†³raw attentionå¸¦æ¥çš„åå·®ã€‚\n\n\næ¥æºï¼šå½“è¾“å…¥çš„tokenæ•° å¤§äº æ³¨æ„åŠ›å¤´çš„ç»´åº¦çš„æ—¶å€™ï¼Œæ³¨æ„åŠ›çŸ©é˜µAå°±ä¼šå­˜åœ¨éé›¶é›¶ç©ºé—´ï¼Œè¿™æ„å‘³Aæ˜¯ä¸å”¯ä¸€çš„ã€‚å½“è¾“å…¥çš„tokenæ•° å°äºç­‰äº æ³¨æ„åŠ›å¤´çš„ç»´åº¦æ—¶ï¼Œeffective attention weights = raw attentionã€‚\n\n\næ ¸å¿ƒæ€æƒ³ï¼šæ‰¾å‡ºçœŸæ­£ä¸ç»“æœç›¸å…³çš„éƒ¨åˆ†ï¼Œè€Œå‰©ä¸‹çš„éƒ¨åˆ†å°±æ˜¯å¯¼è‡´Aä¸å”¯ä¸€çš„éƒ¨åˆ†ã€‚ï¼ˆç±»ä¼¼äºå»å™ªæçº¯ï¼‰\nAâ‹…T=(A+AÂ¯)â‹…T\n\nç®—æ³•åˆæ­¥æ¨å¯¼ï¼š\nå…ˆæ„å»ºT\nT=EWVHï¼Œå…¶ä¸­ï¼š\n\n\nEæ˜¯è¾“å…¥åµŒå…¥çŸ©é˜µç»´åº¦760Ã—512ï¼›\n\n\nWVæ˜¯å€¼æŠ•å½±çŸ©é˜µç»´åº¦ 512Ã—dvï¼Œå‡è®¾å¤šå¤´æ³¨æ„åŠ›ä¸­ dv=64\n\n\nH æ˜¯å¤´éƒ¨æ··åˆçŸ©é˜µç»´åº¦ dvÃ—512ã€‚\nrank(T)â‰¤min(ds,dv)=2ï¼Œå› æ­¤ T çš„é›¶ç©ºé—´ç»´åº¦ä¸º dsâˆ’dv=760âˆ’64=696ã€‚\n\n\nAâŠ¥=Aâˆ’Aâˆ¥\nå…¶ä¸­AâŠ¥æ˜¯æœ€åè·å¾—çš„ç»“æœï¼ŒAâˆ¥é€šè¿‡Aâˆ¥T=0è·å¾—\n\n\n\n\nåŸºäºgradientç®— -&gt; Gradient-Based Attribution Methods | SpringerLink\n\n\n\n\nON IDENTIFIABILITY IN TRANSFORMERS\nqueryï¼šQâˆˆRdsÃ—dq\nkeyï¼šKâˆˆRdsÃ—dq\nvalue: VâˆˆRdsÃ—dv\n\nç”±å…¬å¼1è¾“å‡ºçš„ç»“æœå«åšcontextual word embeddingã€‚\nxiâˆˆRdè¡¨ç¤ºè¾“å…¥Tokenï¼›eilæ˜¯ç¬¬lå±‚è¾“å‡ºçš„contextual word embeddingçš„ç¬¬iè¡Œã€‚\nè‹¥å¹²ä¸ªxiç»„æˆäº†è¾“å…¥çŸ©é˜µ XâˆˆRdsÃ—d ï¼›è‹¥å¹²ä¸ª eil ç»„æˆäº†embedding matrix EâˆˆRdsÃ—dã€‚\n\nHâˆˆRdvÃ—dã€‚å…¬å¼2çš„å·¦ä¾§å…¬å¼è¡¨ç¤ºçš„æ„æ€æ˜¯ï¼šè‹¥å¹²ä¸ªå¤´æœ‰è‹¥å¹²ç»„Q\\K\\Vï¼Œè¿™äº›å¤´è¾“å‡ºçš„ç»“æœçš„å°ºå¯¸ä¸ºdsÃ—dvï¼Œå…¶ä¸­dv=dhï¼Œ hä¸ºå¤´çš„æ•°é‡ã€‚\n"},{"title":"å¦‚ä½•é…ç½®Hexo","url":"/2025/02/01/%E5%A6%82%E4%BD%95%E9%85%8D%E7%BD%AEHEXO/","content":"é…ç½®å›¾åºŠ\nå‚è€ƒé“¾æ¥ï¼šGithub+PicGoæ­å»ºä¸ªäººå…è´¹å›¾åºŠ - misakivv - åšå®¢å›­ (cnblogs.com)\nå¯¹äºåšå®¢é¡µçš„ç…§ç‰‡ä¸Šä¼ \nå‚è€ƒè§£å†³é“¾æ¥ï¼š[2024] hexoå›¾ç‰‡æ— æ³•åŠ è½½ç©¶æè§£å†³æ–¹æ¡ˆ_hexoå›¾ç‰‡æ˜¾ç¤ºä¸å‡ºæ¥-CSDNåšå®¢\nå¯¹äºmarkdownæ¸²æŸ“å™¨çš„ä¼˜åŒ–\nå‚è€ƒé“¾æ¥ï¼ˆä¼˜é€‰ï¼‰ï¼šã€Hexoã€‘æ›´é«˜çº§çš„Markdownæ¸²æŸ“å™¨ | Everett Rain\nå‚è€ƒé“¾æ¥ï¼šã€Hexoã€‘é€‰æ‹©æ›´é«˜çº§çš„Markdownæ¸²æŸ“å™¨_hexo-renderer-marked-CSDNåšå®¢\nå¯¹äºæ¸²æŸ“å™¨ä¼˜åŒ–åï¼Œmathjaxç”¨ä¸äº†çš„è§£å†³æ–¹æ¡ˆ\n\n\nåœ¨ä¿®æ”¹äº†æ¸²æŸ“å™¨åï¼Œä¼šå‘ç°mathjaxç”¨ä¸äº†ï¼Œæ­¤æ—¶å°±éœ€è¦é’ˆå¯¹æ–°çš„æ¸²æŸ“å™¨å®‰è£…ä¸€ä¸ªæ’ä»¶\nå¦‚å›¾æ‰€ç¤ºï¼š\n\n\n\nè§£å†³æ–¹æ¡ˆï¼š\n\nä¸‹è½½ä¸€ä¸ªæ’ä»¶ï¼š\n\n$ npm install markdown-it-mathjax3\n\n\nåœ¨hexoçš„_config.ymlæ–‡ä»¶ä¸­ï¼ŒåŠ ä¸Šè¿™ä¸ªæ’ä»¶çš„åå­—\n\n\n\néšåå°±å¯ä»¥å•¦ï¼\na=b+c=d+e=f+g\n\n\n\nå¯¹äºgitéƒ¨ç½²çš„ç½‘ç»œé—®é¢˜\nå‚è€ƒèµ„æ–™ï¼šè§£å†³gitæŠ¥é”™ï¼šfatal: unable to access â€˜https://github.com/â€¦â€˜: Failed to connect to github.com port 443 a_git failed with a fatal error unable to access-CSDNåšå®¢\nç›´æ¥åœ¨gitä¸­è¾“å…¥ï¼š\ngit config --global --unset http.proxygit config --global --unset https.proxy\nç„¶åæŒ‚vpnå°±å¥½äº†ã€‚ï¼ˆç¨³å®šçš„vpnï¼Œæˆ–è€…ä¹Ÿå¯ä»¥ä¸æŒ‚ç›´æ¥ä¼ ï¼Œæ²¡å‡†ä¹Ÿèƒ½æˆï¼‰\nåœ¨è¾“å…¥å®Œç»ˆç«¯å‘½ä»¤åå¯ä»¥æ£€æŸ¥ä¸€ä¸‹æ˜¯å¦çœŸçš„åˆ é™¤äº†ï¼š\ngit config --global --list\nå¦‚æœè¾“å‡ºä¸­æ²¡æœ‰http.proxyå’Œhttps.proxyï¼Œåˆ™è¯æ˜æˆåŠŸã€‚\næµ‹è¯•åŒº\näºŒçº§æ ‡é¢˜æµ‹è¯•\nä¸‰çº§æ ‡é¢˜æµ‹è¯•\nå››çº§æ ‡é¢˜æµ‹è¯•\näº”çº§æ ‡é¢˜æµ‹è¯•\nå…­çº§æ ‡é¢˜æµ‹è¯•\ntest\ntest\ntest\ntest\ntest\n\n\nå¯¹äºä»£ç çš„æµ‹è¯•\n\n\n## æ­¤å¤„ä½œä¸ºä»£ç æµ‹è¯•import numpy as npa = np.array([1,2,3])\n\n\nå¯¹äºå…¬å¼çš„æµ‹è¯•\na+b=c\n\niâ„âˆ‚âˆ‚tÏˆ=âˆ’â„22mâˆ‡2Ïˆ+VÏˆâ€‹\tå¯¹äºå¥å­å†…éƒ¨çš„å…¬å¼ï¼šä¾‹å¦‚x+1=2\n\n\nå¯¹äºå›¾ç‰‡çš„æµ‹è¯•\n\n\næœ¬åœ°å›¾ç‰‡ï¼š\n\n\n\n\n\n.vobqbqjowuwc{zoom:33%;}\n\n\nâ€‹\tç½‘ç»œå›¾ç‰‡\n\n\n\n\n\nä¸Šä¼ å›¾åºŠçš„ç…§ç‰‡\n\n\n\n\næŠ˜å ï¼Œå±•å¼€å†…å®¹\n\n\nè¯­æ³•ï¼š\n\n\n+++ **ç‚¹å‡»æŠ˜å **è¿™æ˜¯è¢«éšè—çš„å†…å®¹+++\n\n\næ•ˆæœï¼š\n\n&nbsp;ç‚¹å‡»æŠ˜å è¿™æ˜¯è¢«éšè—çš„å†…å®¹\n\n\n\nè‡ªå®šä¹‰å®¹å™¨\n\n\næ•ˆæœ\n\næç¤º\nè¿™æ˜¯ä¸€ä¸ªæç¤º\n\n\næ³¨æ„\nè¿™æ˜¯ä¸€ä¸ªè­¦å‘Š\n\n\ndanger\nè¿™æ˜¯ä¸€ä¸ªå±é™©ä¿¡å·\n\n\nå¼•ç”¨æµ‹è¯•\n\n\næˆåŠŸ\nè¿™æ˜¯ä¸€ä¸ªæˆåŠŸä¿¡å·\n\n\n\nd\n\næç¤º\nè¿™æ˜¯ä¸€ä¸ªæç¤º\n\n\n\n\næç¤º\nè¿™æ˜¯ä¸€ä¸ªæç¤º\n\n\n\nè¯­æ³•ï¼š\n::: tips**æç¤º**è¿™æ˜¯ä¸€ä¸ªæç¤º:::::: warning**warning**è¿™æ˜¯ä¸€ä¸ªè­¦å‘Š:::::: danger**è­¦å‘Š**è¿™æ˜¯ä¸€ä¸ªå±é™©ä¿¡å·:::::: success**æˆåŠŸ**è¿™æ˜¯ä¸€ä¸ªæˆåŠŸä¿¡å·:::\n\n\næ–‡æœ¬é«˜äº®æµ‹è¯•ï¼š\n\nè¿™æ˜¯ä¸€å¥æµ‹è¯•çš„æ–‡æœ¬ã€‚\n\n\n\nHeâ€™s a good person\n\n\ntyporaå®¹å™¨æµ‹è¯•\n\nğŸ’¡ Tip\næµ‹è¯•\n\n\n\næµ‹è¯•\n\nğŸ“˜ Important\næµ‹è¯•\n\n\n\n\nåœ¨æµ‹è¯•\n\nâš ï¸ Warning\næµ‹è¯•è¡Œ1\næµ‹è¯•è¡Œ2\n\nd\n\ndddd\n\n\n\n\n\n\n\nd\n\nâ„¹ï¸ Note\nç¼©è¿›æµ‹è¯•\n\n\n\n\nğŸ’¡ Tip\næµ‹è¯•2\n\n\nå¼•ç”¨\n\n\nâ„¹ï¸ Note\nTest-Note\n\nå•¦å•¦å•¦å•¦æ¥å•¦\n\nğŸ“˜ Important\nImportant-test\n\nllalalalallala\n\nâš ï¸ Warning\nWarning-test\n\n\nâ— Caution\nCaution-test\n\næœ€åˆï¼Œæƒ³ä½¿ç”¨markdown-itçš„æ’ä»¶æ¯”å¦‚markdown-it-alertã€‚ä½†æ˜¯è²Œä¼¼æ¸²æŸ“å™¨åœ¨ç‰ˆæœ¬ä¸Šæœ‰å†²çªï¼Œå¹¶ä¸æ”¯æŒã€‚å› æ­¤ï¼Œä¾¿ä½¿ç”¨ä¸€ç§æŠ•æœºå–å·§çš„åŠæ³•ï¼ŒåŸºäºmarkdown-it-containeræ¥å®ç°ã€‚æ–¹å¼å¦‚ä¸‹ï¼š\nè¾“å…¥&gt; [!TIP]&gt; &gt; æµ‹è¯•2&gt; [!NOTE]&gt;&gt; Test-Note&gt; [!IMPORTANT]&gt;&gt; Important-test&gt; [!WARNING]&gt;&gt; Warning-test&gt; [!CAUTION]&gt;&gt; Caution-testè¾“å‡ºï¼š::: tip**ğŸ’¡ Tip**æµ‹è¯•2:::::: note**â„¹ï¸ Note**Test-Note:::::: important**ğŸ“˜ Important**Important-test:::::: warning**âš ï¸ Warning**Warning-test:::::: caution**â— Caution**Caution-test:::\nä¸ºäº†å®ç°è‡ªåŠ¨åŒ–çš„è„šæœ¬è½¬åŒ–ï¼Œä¸”ä¸å½±å“åŸå§‹çš„typoraä»£ç ï¼Œéœ€è¦åœ¨ Hexoæ ¹ç›®å½•/scripts (å¦‚æœæ²¡æœ‰è¿™ä¸ªæ–‡ä»¶å¤¹å°±æ–°å»ºä¸€ä¸ª) ä¸­æ–°å»ºä¸€ä¸ªå«admonition-convert.jsçš„æ–‡ä»¶ã€‚å°†ä¸‹é¢çš„ä»£ç æ‹·è´è¿›å»\nhexo.extend.filter.register(&#x27;before_post_render&#x27;, function(data) &#123;  // å®šä¹‰ç±»å‹æ˜ å°„è¡¨ï¼ŒåŒ…å«å¯¹åº”çš„å›¾æ ‡å’Œæ ‡é¢˜  const typeMap = &#123;    TIP:       &#123; icon: &#x27;ğŸ’¡&#x27;, title: &#x27;Tip&#x27; &#125;,    NOTE:      &#123; icon: &#x27;â„¹ï¸&#x27;, title: &#x27;Note&#x27; &#125;,    IMPORTANT: &#123; icon: &#x27;ğŸ“˜&#x27;, title: &#x27;Important&#x27; &#125;,    WARNING:   &#123; icon: &#x27;âš ï¸&#x27;, title: &#x27;Warning&#x27; &#125;,    CAUTION:   &#123; icon: &#x27;â—&#x27;, title: &#x27;Caution&#x27; &#125;  &#125;;    // åŒ¹é…æ‰€æœ‰æ”¯æŒçš„Admonitionç±»å‹ï¼Œå¤„ç†ç©ºè¡Œå’Œå¤šä¸ªè¿ç»­å—  const admonitionRegex = /(^&gt; \\[!([A-Z]+)\\])(\\r?\\n)(&gt; ?\\r?\\n)*((&gt; .*?\\r?\\n)*?)(?=\\r?\\n&gt; \\[!|$)/gm;    data.content = data.content.replace(admonitionRegex, (match, header, type, firstLineBreak, emptyLines, content) =&gt; &#123;    // æ£€æŸ¥ç±»å‹æ˜¯å¦åœ¨æ˜ å°„è¡¨ä¸­    if (!typeMap[type]) &#123;      return match; // ä¸åŒ¹é…çš„ç±»å‹ä¿æŒåŸæ ·    &#125;        // æ¸…ç†å†…å®¹ä¸­çš„&gt;å‰ç¼€å’Œå¤šä½™ç©ºè¡Œ    const cleanedContent = content      .replace(/^&gt; ?/gm, &#x27;&#x27;) // ç§»é™¤æ¯è¡Œå¼€å¤´çš„&gt;å’Œå¯èƒ½çš„ç©ºæ ¼      .replace(/^\\s+|\\s+$/g, &#x27;&#x27;); // ç§»é™¤é¦–å°¾ç©ºè¡Œ        // è·å–å¯¹åº”çš„å›¾æ ‡å’Œæ ‡é¢˜    const &#123; icon, title &#125; = typeMap[type];        // è½¬æ¢ä¸ºå¸¦å›¾æ ‡å’Œæ ‡é¢˜çš„Typoraé£æ ¼æç¤ºå—    return `::: $&#123;type.toLowerCase()&#125;**$&#123;icon&#125; $&#123;title&#125;**$&#123;cleanedContent&#125;:::`;  &#125;);    return data;&#125;);\néšåï¼Œæ¸²æŸ“å™¨å°±å¯ä»¥æ”¯æŒtyporaçš„alertå•¦\næµ‹è¯•æµç¨‹å›¾\ngraph TD\n    A[èµ·ç‚¹] --> B[æ­¥éª¤1]\n    B --> C[æ­¥éª¤2]\n    C --> D[æ­¥éª¤3]\n    D --> E[ç»ˆç‚¹]\næµ‹è¯•è¡¨æ ¼\n\n\n\n1\n2\n3\n\n\n\n\n4\n+âˆ\n6\n\n\n7\n8\n9\n\n\n10\n11\n12\n\n\n\n\n\nç¼©è¿›è¡¨æ ¼\n\n\n\n\n\næ—¶ä»£\n2\nTime\n\n\n\n\næ—¶ä»£\n+âˆ\nTime\n\n\n7\n8\n9\n\n\n10\n11\n12\n\n\n\n\n\nè¿›ä¸€æ­¥ç¼©è¿›çš„è¡¨æ ¼\n\nè¿›ä¸€æ­¥ç¼©è¿›\n\n\n\n\n\n\n1\n2\n3\n\n\n\n\n4\n+âˆ\n6\n\n\n7\n8\n9\n\n\n10\n11\n12\n\n\n\n\n\nå†è¿›ä¸€æ­¥\n\nå†è¿›\n\n\n\n\n\n\n1\n2\n3\n\n\n\n\n4\n+âˆ\n6\n\n\n7\n8\n9\n\n\n10\n11\n12\n\n\n\næµ‹è¯•æœ‰åºæ— åºåˆ—è¡¨\n\n\n1\n\n1\n\n\n\nä¸€ä¸ªäºº\n\nä¸€ä¸ªäºº\n\n\n\none person\n\none person\n\n\n\n\n\n33324325123\n\n\n234843929823409\n\n\nä¸€ç¾¤äºº\n\nä¸€ç¾¤äºº\n\n\n\na group of people\n\na group of people\n\na antehr\n\n\n\n\n\n","categories":["Hexo","è§£å†³æ–¹æ¡ˆ"],"tags":["Hexo"]},{"title":"æ·±åº¦å­¦ä¹ ï¼šæ·±åº¦ç”Ÿæˆæ¨¡å‹","url":"/2025/02/10/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%EF%BC%9A%E6%89%A9%E6%95%A3%E6%A8%A1%E5%9E%8B/","content":"\n\nåˆ¤åˆ«å¼æ¨¡å‹å’Œç”Ÿæˆå¼æ¨¡å‹\n\nåˆ¤åˆ«å¼ï¼šå­¦ä¹ ç±»åˆ«çš„è¾¹ç•Œï¼ˆæ¯”å¦‚CNNã€RNNï¼‰\nç”Ÿæˆå¼ï¼šå­¦ä¹ æ•°æ®çš„åˆ†å¸ƒï¼ˆVAEã€GANï¼‰ã€‚æ¯”åˆ¤åˆ«å¼æ›´åŠ å¤æ‚\n\n\n\nè’™ç‰¹å¡æ´›æ–¹æ³•\né‡‡æ · sampling\n\n\né‡‡æ ·å¯ä»¥å‡å°‘ç§¯åˆ†è¿ç®—é‡ï¼ˆå°¤å…¶æ˜¯æ¦‚ç‡å¯†åº¦å‡½æ•°ç‰¹åˆ«å¤æ‚çš„æ—¶å€™ï¼Œå¯èƒ½æ— æ³•æ±‚å‡ºè§£æè§£ï¼‰ï¼Œä¾¿äºç»Ÿè®¡æ¨æ–­å’Œæ¨¡å‹ä¼˜åŒ–ã€‚\n\n\nè’™ç‰¹å¡æ´›é‡‡æ ·ï¼šä»æ¦‚ç‡åˆ†å¸ƒä¸­æŠ½å‡ºæ ·æœ¬ï¼Œå¾—åˆ°åˆ†å¸ƒçš„è¿‘ä¼¼ã€‚æ ·æœ¬è¶Šå¤šï¼Œè¿‘ä¼¼çš„è¶Šå‡†\n\n\n\nâ€‹\tå…¬å¼è¯¦è§£ï¼šs=âˆ«p(x)f(x)dx=Ep[f(x)]æ˜¯ä¸€ä¸ªç§¯åˆ†å½¢å¼çš„æœŸæœ›è¡¨è¾¾å¼ã€‚å…¶ä¸­ p(x)æ˜¯æ¦‚ç‡å¯†åº¦å‡½æ•°ï¼ˆå½“ x æ˜¯è¿ç»­å‹éšæœºå˜é‡æ—¶ï¼‰ï¼Œf(x) æ˜¯å…³äºéšæœºå˜é‡xçš„å‡½æ•° ã€‚âˆ«p(x)f(x)dxè¡¨ç¤ºå¯¹f(x) å…³äºæ¦‚ç‡åˆ†å¸ƒp(x)æ±‚ç§¯åˆ†ï¼Œå¾—åˆ°çš„ç»“æœ så°±æ˜¯å‡½æ•° f(x)å…³äºåˆ†å¸ƒp(x)çš„æœŸæœ›Ep[f(x)] ã€‚åœ¨æ¦‚ç‡è®ºå’Œç»Ÿè®¡å­¦ä¸­ï¼ŒæœŸæœ›æ˜¯éšæœºå˜é‡å–å€¼çš„åŠ æƒå¹³å‡ï¼Œè¿™é‡Œçš„æƒé‡å°±æ˜¯æ¦‚ç‡å¯†åº¦ p(x)ã€‚\ns^n=1nâˆ‘i=1nf(x(i)) æ˜¯è’™ç‰¹å¡ç½—ä¼°è®¡çš„å½¢å¼ã€‚è¿™é‡Œ$ \\boldsymbol{x}^{(i)}æ˜¯ä»åˆ†å¸ƒæ˜¯ä»åˆ†å¸ƒp(\\boldsymbol{x})$ä¸­æŠ½å–çš„ç¬¬ iä¸ªæ ·æœ¬ ï¼Œnæ˜¯æ ·æœ¬æ•°é‡ã€‚é€šè¿‡æŠ½å–n ä¸ªæ ·æœ¬ï¼Œè®¡ç®— f(x) åœ¨è¿™äº›æ ·æœ¬ä¸Šçš„å–å€¼å¹¶æ±‚å¹³å‡ï¼Œæ¥è¿‘ä¼¼ä¸Šæ–¹å…¬å¼ä¸­çš„æœŸæœ› Ep[f(x)]ã€‚éšç€ nå¢å¤§ï¼Œæ ¹æ®å¤§æ•°å®šå¾‹ï¼Œs^n ä¼šä¾æ¦‚ç‡æ”¶æ•›åˆ° Ep[f(x)]ï¼Œå³ limnâ†’âˆs^n=Ep[f(x)]ã€‚ è’™ç‰¹å¡ç½—æ–¹æ³•å¸¸ç”¨äº==éš¾ä»¥ç›´æ¥è®¡ç®—ç§¯åˆ†ï¼ˆæœŸæœ›ï¼‰==çš„æƒ…å†µï¼Œé€šè¿‡é‡‡æ ·å’Œå¹³å‡æ¥è·å¾—è¿‘ä¼¼è§£ã€‚\n\nå¯ä»¥çœ‹å‡ºï¼Œéšç€é‡‡æ ·æ•°çš„å¢åŠ ï¼Œé‡‡æ ·çš„ç²¾å‡†åº¦è¶Šæ¥è¶Šå¥½ã€‚åœ¨1000ä¸ªæ ·æœ¬çš„æ—¶å€™ï¼Œæ‹Ÿåˆå‡ºçš„æ›²çº¿æ›´æ¥è¿‘p(x)\nImportance Sampling\n\n\nç”¨äºä¼°è®¡éš¾ä»¥é‡‡æ ·çš„åˆ†å¸ƒçš„æœŸæœ›å€¼ã€‚ä¸¾ä¾‹æ¥çœ‹å°±æ˜¯f(x)åœ¨p(x)ä¸­çš„åˆ†å¸ƒå¯èƒ½ä¸æ˜¯å¾ˆå‡åŒ€ï¼Œåœ¨f(x)å–å€¼å¾ˆå°çš„æ—¶å€™åœ¨p(x)ä¸­çš„æ¦‚ç‡å¾ˆå¤§ï¼Œè€Œf(x)å–å€¼å¾ˆå¤§çš„æ—¶å€™åœ¨p(x)ä¸­çš„æ¦‚ç‡å¾ˆå°ï¼Œè¿™å°±ä¼šè®©æœŸæœ›çš„è®¡ç®—éš¾ä»¥æ”¶æ•›ã€‚\n\n\nç”¨æ˜“äºé‡‡æ ·çš„å‚è€ƒåˆ†å¸ƒç”Ÿæˆæ ·æœ¬ï¼Œç„¶åç”¨æƒé‡ç³»æ•°è°ƒæ•´ä¼°è®¡\n\næ­¤å¤„p(x)ä¸å¥½é‡‡æ ·ï¼Œå°±ç”¨å¦‚é«˜æ–¯åˆ†å¸ƒç­‰æ›´ä¸ºç®€å•çš„q(x)æ¥è¿‘ä¼¼\n\n\né©¬å°”ç§‘å¤«é“¾è’™ç‰¹å¡æ´›æ–¹æ³•\n\n\nMarkov Chain Monte Carlo (MCMC)\n\n\nä¸€ç§åŠ¨æ€çš„é‡‡æ ·æ–¹æ³•ã€‚\n\nä½¿ç”¨çŠ¶æ€è½¬ç§»çŸ©é˜µï¼Œä»èµ·å§‹çŠ¶æ€å¼€å§‹ï¼Œè½¬ç§»å‡ºä¸€ç³»åˆ—çš„çŠ¶æ€ï¼Œè¿™ä¸€ç³»åˆ—çš„çŠ¶æ€å°±æ˜¯æœ€åçš„é‡‡æ ·ç»“æœ\n\n.ombepndmxieo{zoom:150%;}\nçŠ¶æ€ä¸è½¬ç§»ï¼šå›¾ä¸­æœ‰ä¸¤ä¸ªçŠ¶æ€â€œSunnyï¼ˆæ™´å¤©ï¼‰â€å’Œâ€œRainyï¼ˆé›¨å¤©ï¼‰â€ ã€‚çŠ¶æ€ä¹‹é—´çš„å¸¦ç®­å¤´è¿çº¿å’Œæ•°å­—è¡¨ç¤ºçŠ¶æ€è½¬ç§»æ¦‚ç‡ã€‚æ¯”å¦‚ä»â€œSunnyâ€çŠ¶æ€åˆ°è‡ªèº«çš„è½¬ç§»æ¦‚ç‡æ˜¯(0.9)ï¼Œæ„å‘³ç€æ™´å¤©åè¿˜æ˜¯æ™´å¤©çš„æ¦‚ç‡ä¸º(0.9)ï¼›ä»â€œSunnyâ€åˆ°â€œRainyâ€çš„è½¬ç§»æ¦‚ç‡æ˜¯(0.1) ï¼Œå³æ™´å¤©è½¬é›¨å¤©æ¦‚ç‡ä¸º(0.1) ï¼›ä»â€œRainyâ€åˆ°â€œSunnyâ€æ¦‚ç‡æ˜¯(0.5)ï¼Œâ€œRainyâ€åˆ°è‡ªèº«æ¦‚ç‡æ˜¯(0.5) ã€‚\nçŠ¶æ€è½¬ç§»çŸ©é˜µï¼šå›¾ä¸­ä¸‹æ–¹çš„çŸ©é˜µ[0.90.10.50.5]å°±æ˜¯è¯¥é©¬å°”å¯å¤«é“¾çš„çŠ¶æ€è½¬ç§»çŸ©é˜µï¼ŒçŸ©é˜µçš„è¡Œå’Œåˆ—åˆ†åˆ«å¯¹åº”â€œSunnyâ€å’Œâ€œRainyâ€çŠ¶æ€ï¼ŒçŸ©é˜µå…ƒç´ pijè¡¨ç¤ºä»çŠ¶æ€iè½¬ç§»åˆ°çŠ¶æ€jçš„æ¦‚ç‡ã€‚\n::: tips\næç¤º\né©¬å°”å¯å¤«é“¾çš„æ ¸å¿ƒç‰¹æ€§æ˜¯æ— åæ•ˆæ€§ï¼Œå³ç³»ç»Ÿåœ¨æŸä¸ªæ—¶åˆ»çš„çŠ¶æ€è½¬ç§»åªå–å†³äºå½“å‰çŠ¶æ€ï¼Œä¸è¿‡å»çŠ¶æ€æ— å…³ã€‚\n:::\n\n\nå¥½å¤„ï¼šä¸ç”¨åœ¨æ•´ä¸ªåˆ†å¸ƒç©ºé—´ä¸­å‡åŒ€é‡‡æ ·ï¼Œå¯ä»¥è¿›ä¸€æ­¥æå‡é‡‡æ ·çš„æ•ˆç‡\n\n\nå˜åˆ†æ¨æ–­ï¼ˆVariational Inferenceï¼‰\nå‚æ•°ä¼°è®¡\n\n\næœºå™¨å­¦ä¹ çš„æœ¬è´¨ï¼šä»å·²æœ‰æ•°æ®ä¸­ä¼°è®¡å‡ºå‚æ•°\n\næ±‚è§£è¿‡ç¨‹ï¼šæœ€ä¼˜åŒ–ç†è®º\n\n\n\nä¸¤ä¸ªå­¦æ´¾ï¼š\n\n\nç¬¬ä¸€ä¸ªå­¦æ´¾ï¼ˆé¢‘ç‡å­¦æ´¾ï¼‰è®¤ä¸ºï¼Œæ•°æ®å°±æ˜¯å®¢è§‚å­˜åœ¨çš„ï¼Œä¸å—ä»»ä½•å½±å“ï¼Œæ‰€ä»¥ç›´æ¥æ±‚p(x|z)ï¼Œå³å…ˆéªŒæ¦‚ç‡ï¼ˆåœ¨ç»™å®šå‚æ•°zçš„å‰æä¸‹ï¼Œæ•°æ®ä¸ºxçš„æ¦‚ç‡ï¼‰ã€‚\né¢‘ç‡å­¦æ´¾é‡ç‚¹å…³æ³¨åŸºäºå·²æœ‰æ•°æ®å¯¹æ€»ä½“å‚æ•°è¿›è¡Œä¼°è®¡ã€‚æ¯”å¦‚ä¼°è®¡ä¸€æšç¡¬å¸æ­£é¢æœä¸Šçš„æ¦‚ç‡ï¼Œé€šè¿‡å¤§é‡æŠ›ç¡¬å¸è¯•éªŒï¼Œç”¨æ­£é¢æœä¸Šçš„é¢‘ç‡è¿‘ä¼¼è¿™ä¸ªæ¦‚ç‡ï¼Œä¸è€ƒè™‘å…¶ä»–æ½œåœ¨ â€œéšå˜é‡â€ ã€‚ä»–ä»¬è®¤ä¸ºå­˜åœ¨ä¸€ä¸ªçœŸå®å›ºå®šçš„å‚æ•°å€¼ï¼Œé€šè¿‡è¶³å¤Ÿå¤šæ•°æ®å’Œåˆé€‚ç»Ÿè®¡æ–¹æ³•å°±èƒ½é€¼è¿‘è¿™ä¸ªçœŸå€¼ã€‚\nå¸¸ç”¨æ–¹æ³•æœ‰æœ€å¤§ä¼¼ç„¶ä¼°è®¡(MLE)ï¼ˆL(Î¸|x)=P(X=x|Î¸)ï¼‰ã€‚æœ€å¤§ä¼¼ç„¶ä¼°è®¡æ ¹æ®æ ·æœ¬æ„é€ ä¼¼ç„¶å‡½æ•°ï¼Œæ‰¾åˆ°ä½¿ä¼¼ç„¶å‡½æ•°æœ€å¤§çš„å‚æ•°å€¼ï¼Œè®¤ä¸ºè¿™å°±æ˜¯å¯¹æ€»ä½“çœŸå®å‚æ•°çš„æœ€ä½³ä¼°è®¡ï¼Œå¼ºè°ƒæ•°æ®æœ¬èº«å¯¹å‚æ•°ä¼°è®¡çš„ä½œç”¨ï¼Œä¸ä¾èµ–å…ˆéªŒä¿¡æ¯ã€‚\n\n\nç¬¬äºŒä¸ªå­¦æ´¾ï¼ˆè´å¶æ–¯å­¦æ´¾ï¼‰è®¤ä¸ºï¼Œè§‚æµ‹åˆ°çš„æ•°æ®xä¼šå—åˆ°éšå˜é‡zçš„å½±å“ï¼Œæ‰€ä»¥ä»–ä»¬æ±‚çš„æ˜¯åéªŒæ¦‚ç‡p(z|x)ã€‚\n.vkmvefivlvkb{zoom:50%;}\nåŸºäºè´å¶æ–¯å®šç†ï¼ŒåéªŒæ¦‚ç‡p(Î¸|x)å¯ä»¥è¡¨ç¤ºä¸ºp(Î¸|x)=p(x|Î¸)p(Î¸)p(x)ï¼Œå…¶ä¸­Î¸æ˜¯å¾…ä¼°è®¡çš„å‚æ•°ï¼Œxæ˜¯è§‚æµ‹æ•°æ®ã€‚p(x|Î¸)æ˜¯ä¼¼ç„¶å‡½æ•°ï¼Œè¡¨ç¤ºåœ¨ç»™å®šå‚æ•°Î¸ä¸‹è§‚æµ‹åˆ°æ•°æ®xçš„æ¦‚ç‡ï¼›p(Î¸)æ˜¯å…ˆéªŒæ¦‚ç‡ï¼Œåæ˜ äº†åœ¨è§‚æµ‹æ•°æ®ä¹‹å‰å¯¹å‚æ•°Î¸çš„è®¤çŸ¥ï¼›p(x)æ˜¯è¯æ®å› å­ï¼Œé€šå¸¸ä½œä¸ºå½’ä¸€åŒ–å¸¸æ•°.\næœ€å¤§åéªŒä¼°è®¡å°±æ˜¯æ‰¾åˆ°ä½¿åéªŒæ¦‚ç‡p(Î¸|x)æœ€å¤§çš„å‚æ•°Î¸å€¼ï¼Œå³Î¸^MAP=argâ¡maxÎ¸p(Î¸|x)=argâ¡maxÎ¸p(x|Î¸)p(Î¸)p(x)ã€‚ç”±äºp(x)ä¸Î¸æ— å…³ï¼Œæ‰€ä»¥ç­‰ä»·äºÎ¸^MAP=argâ¡maxÎ¸p(x|Î¸)p(Î¸)ã€‚\nä¸æœ€å¤§ä¼¼ç„¶ä¼°è®¡çš„æ¯”è¾ƒ ï¼š\n\n\næœ€å¤§ä¼¼ç„¶ä¼°è®¡ï¼šåªè€ƒè™‘ä¼¼ç„¶å‡½æ•°p(x|Î¸)ï¼Œé€šè¿‡æ‰¾åˆ°ä½¿p(x|Î¸)æœ€å¤§çš„Î¸å€¼æ¥ä¼°è®¡å‚æ•°ï¼Œæ²¡æœ‰åˆ©ç”¨å…ˆéªŒä¿¡æ¯ã€‚å½“æ•°æ®é‡è¶³å¤Ÿå¤§æ—¶ï¼Œæœ€å¤§ä¼¼ç„¶ä¼°è®¡èƒ½å¾—åˆ°è¾ƒå¥½çš„ç»“æœï¼Œä½†åœ¨æ•°æ®é‡è¾ƒå°‘æ—¶ï¼Œå¯èƒ½ä¼šå‡ºç°è¿‡æ‹Ÿåˆæˆ–ä¸åˆç†çš„ä¼°è®¡ã€‚\n\n\næœ€å¤§åéªŒä¼°è®¡ï¼šç»¼åˆäº†ä¼¼ç„¶å‡½æ•°å’Œå…ˆéªŒæ¦‚ç‡ï¼Œåœ¨æ•°æ®é‡è¾ƒå°‘æ—¶ï¼Œå…ˆéªŒä¿¡æ¯å¯ä»¥èµ·åˆ°æ­£åˆ™åŒ–çš„ä½œç”¨ï¼Œå¸®åŠ©é¿å…è¿‡æ‹Ÿåˆï¼Œå¾—åˆ°æ›´ç¬¦åˆå®é™…æƒ…å†µçš„ä¼°è®¡ç»“æœã€‚ä¾‹å¦‚ï¼Œåœ¨ä¼°è®¡ä¸€ä¸ªç¡¬å¸æ­£é¢æœä¸Šçš„æ¦‚ç‡æ—¶ï¼Œå¦‚æœæ²¡æœ‰å…ˆéªŒä¿¡æ¯ï¼Œæœ€å¤§ä¼¼ç„¶ä¼°è®¡å¯èƒ½ä¼šæ ¹æ®å°‘é‡çš„æŠ•æ·ç»“æœç»™å‡ºä¸€ä¸ªæç«¯çš„ä¼°è®¡å€¼ã€‚ä½†å¦‚æœæœ‰ä¸€ä¸ªåˆç†çš„å…ˆéªŒï¼Œå¦‚è®¤ä¸ºç¡¬å¸å¤§è‡´æ˜¯å‡åŒ€çš„ï¼Œé‚£ä¹ˆæœ€å¤§åéªŒä¼°è®¡ä¼šåœ¨ä¼¼ç„¶å‡½æ•°å’Œå…ˆéªŒä¹‹é—´è¿›è¡Œå¹³è¡¡ï¼Œå¾—åˆ°ä¸€ä¸ªæ›´åˆç†çš„ä¼°è®¡ã€‚\n\n\n::: success\næ€è€ƒ\næ„Ÿè§‰è´å¶æ–¯å­¦æ´¾æ›´å±Œä¸€äº›ï¼Ÿæ¯•ç«Ÿäººå®¶æ˜¯ç›´æ¥ä»å½“å‰çš„æ•°æ®ä¸­ç›´æ¥æ¨ç®—å‡ºæ¥äº†é’ˆå¯¹å½“å‰æ•°æ®çš„ä¸€ç»„å‚æ•°ã€‚\n:::\n\n\n\n\né€šè¿‡ä¸¤ä¸ªå­¦æ´¾çš„è®²è§£ï¼Œå…¶å®å¯¹å½“å‰çš„æ·±åº¦å­¦ä¹ å¯ä»¥è¿›è¡Œä¸€ä¸ªåˆ†ç±»ã€‚å¯¹äºä¼ ç»Ÿçš„æ·±åº¦ç¥ç»ç½‘ç»œï¼ˆå¦‚CNNã€RNNç­‰ï¼‰æˆ–è€…ç§°åˆ¤åˆ«å¼æ¨¡å‹ï¼Œéƒ½æ˜¯åŸºäºé¢‘ç‡å­¦æ´¾çš„ã€‚å…¶æ˜¯é€šè¿‡æœ€ä¼˜åŒ–ç†è®ºæ‰¾åˆ°ä¸€ç»„å‚æ•°æ¥å°½å¯èƒ½ä½¿ç»“æœé€¼è¿‘çœŸå€¼ã€‚è€Œæ¦‚ç‡ç”Ÿæˆæ¨¡å‹ï¼Œéƒ½æ˜¯åŸºäºè´å¶æ–¯å­¦æ´¾çš„ã€‚\næ±‚è§£æœ€å¤§åéªŒä¼°è®¡\np(z|x)=p(x|z)p(z)p(x)=p(x|z)p(z)âˆ«p(z)p(x|z)dzå…¶ä¸­p(x|z)æ˜¯ä¼¼ç„¶å‡½æ•°(likelihood)ï¼Œp(z)æ˜¯å…ˆéªŒ(prior)ï¼Œp(x)æ˜¯evidenceã€‚evidenceä½¿ç”¨ç§¯åˆ†æ¥æ±‚è§£ï¼Œä½†æ˜¯å…¶å¾ˆå¤šæ—¶å€™éƒ½æ˜¯ä¸€ä¸ªä¸å¯ç§¯çš„ç§¯åˆ†ï¼Œæ— æ³•è·å¾—è§£æè§£ã€‚è¿™æ—¶å€™å¯ä»¥ä½¿ç”¨é©¬å°”ç§‘å¤«é“¾è’™ç‰¹å¡æ´›æ–¹æ³•ï¼ˆMCMCï¼‰æ¥å–æ ·ï¼Œè¿‘ä¼¼çš„æ±‚è§£p(z|x)ã€‚\nä½†æ˜¯MCMCæ–¹æ³•è®¡ç®—è¾ƒæ…¢ï¼Œå˜åˆ†æ¨æ–­æ›´é€‚åˆæ±‚è§£å¤§è§„æ¨¡æ¨æ–­ï¼Œé€‚åˆå¹¶è¡Œè®¡ç®—ã€‚\nå˜åˆ†æ¨æ–­çš„ç®—æ³•æ€è·¯\n\nå›¾ä¸­é»„è‰²çš„éƒ¨åˆ†å°±æ˜¯è¦æ±‚çš„ç›®æ ‡åˆ†å¸ƒï¼ˆp(z|x;Ï•)ï¼ŒÏ†æ˜¯å‚æ•°ï¼‰,çº¢è‰²çš„æ›²çº¿q(z;Î¸)ï¼ˆå¯ä»¥æ˜¯ä¸€ä¸ªå·²çŸ¥çš„é«˜æ–¯åˆ†å¸ƒï¼ŒÎ¸æ˜¯å‚æ•°ï¼‰æ˜¯ç”¨æ¥è¿‘ä¼¼ç›®æ ‡åˆ†å¸ƒçš„ã€‚ç°åœ¨åªéœ€è¦ç§»åŠ¨æ›²çº¿qï¼Œä½¿å…¶å°½å¯èƒ½å¤šçš„ä¸é»„è‰²éƒ¨åˆ†é‡å ã€‚ç»¿è‰²æ›²çº¿ä»£è¡¨äº†æœ€åé‡å æœ€å¤§çš„ä½ç½®ã€‚\nä¸Šé¢çš„æè¿°æ˜¯å®šæ€§çš„ï¼Œç”¨å®šé‡çš„æ•°å­¦è¯­è¨€æ¥æè¿°å°±æ˜¯æœ€å°åŒ–ä¸¤ä¸ªåˆ†å¸ƒä¹‹é—´çš„KLæ•£åº¦ï¼Œå…·ä½“å…¬å¼å¦‚ä¸‹ï¼š\nminÎ¸KL(q(z;Î¸)||p(z|x;Ï•))ç›®æ ‡å°±æ˜¯æ±‚å‡ºä¸€ä¸ªå‚æ•°Î¸ ï¼Œä¹Ÿå°±æ˜¯æ€ä¹ˆç§»åŠ¨qã€‚\n\n\nKLæ•£åº¦\n\n\næ¦‚å¿µï¼šç”¨äºè¡¡é‡ä¸¤ä¸ªåˆ†å¸ƒä¹‹é—´çš„è·ç¦»ï¼Œæ˜¯ä¸€ç§ç›¸å¯¹ç†µ\n\n\nç›¸å¯¹ç†µ = äº¤å‰ç†µ - ä¿¡æ¯ç†µ\n\n\n\nè®¡ç®—å…¬å¼ï¼š\nKL(q(z;Î¸)||p(z|x))]=Eq(z;Î¸)[logâ¡q(z;Î¸)p(z|x)]æ˜¯é’ˆå¯¹å‡å®šçš„å‡½æ•°qæ¥æ±‚æœŸæœ›ã€‚\n\n\n\n\nå…¬å¼æ¨å¯¼\nKL(q(z;Î¸)||p(z|x))]=Eq[logâ¡q(z)p(z|x)]=Eq[logâ¡q(z)âˆ’logâ¡p(z|x)]=Eq[logâ¡q(z)âˆ’logâ¡p(x|z)p(z)p(x)]=Eq[logâ¡q(z)]âˆ’Eq[logâ¡p(x|z)]âˆ’Eq[logâ¡p(z)]+Eq[logâ¡p(x)]=âˆ’Eq[logâ¡p(x|z)]+KL(q(z)||p(z))+Eq[logâ¡p(x)]æœ€åç»“æœé‡Œï¼ŒEq[logâ¡p(x)] æ˜¯ä¸€ä¸ªå¸¸æ•°ï¼ˆx æ˜¯å·²æœ‰çš„æ•°æ®ï¼Œæ‰€ä»¥ logâ¡p(x) æ˜¯å¸¸æ•°ï¼‰ã€‚å‰åŠéƒ¨åˆ† âˆ’Eq[logâ¡p(x|z)]+KL(q(z)||p(z)) ä¸ºè¯æ®ä¸‹ç•Œï¼ˆEvidence Lower Boundï¼‰,ç¼©å†™ä¸ºELBOï¼Œé‚£ä¹ˆæ•´ä¸ªå…¬å¼å¯ä»¥æ”¹å†™ä¸ºä¸‹é¢çš„æ ·å­ã€‚\nKL(q(z;Î¸)||p(z|x))]=âˆ’Eq[logâ¡p(x|z)]+KL(q(z)||p(z))+Eq[logâ¡p(x)]=âˆ’ELBO+Eq[logâ¡p(x)]è¿›ä¸€æ­¥ï¼Œæœ€å°åŒ–KLæ•£åº¦ï¼Œå°±æ˜¯åœ¨æœ€å¤§åŒ–ELBOã€‚å› ä¸ºEq[logâ¡p(x)]æ˜¯å®šå€¼ï¼Œè€ŒELBOå‰é¢åˆæœ‰ä¸€ä¸ªè´Ÿå·ã€‚æ‰€ä»¥å¼å­å¯è¿›ä¸€æ­¥æ”¹å†™æˆä¸‹é¢çš„æ ·å­ã€‚\nargâ¡minKL(q(z;Î¸)||p(z|x))]=argâ¡maxELBO=argâ¡maxEq[logâ¡p(x|z)]âˆ’KL(q(z)||p(z))\n\næ¡ˆä¾‹\nå‡è®¾æˆ‘ä»¬æœ‰è‹¥å¹²çš„æ•°æ®xï¼ˆä¸åŒé¢œè‰²çš„ç‚¹ï¼‰ï¼Œæˆ‘ä»¬éœ€è¦ä»è¿™äº›æ•°æ®ä¸­æ¨ç†å‡ºä¸€ä¸ªå‚æ•°zï¼Œé‚£ä¹ˆå°±æ˜¯åœ¨æ±‚æœ€å¤§åéªŒä¼°è®¡p(z|x)ã€‚å‡è®¾æˆ‘ä»¬æœ‰ä¸€ä¸ªé«˜æ–¯æ··åˆæ¨¡å‹q(z;Î¸)ï¼ˆç°è‰²é˜´å½±ï¼‰ï¼Œå®ƒå‡è®¾æ•°æ®æ˜¯ç”±å¤šä¸ªé«˜æ–¯åˆ†å¸ƒï¼ˆä¹Ÿå«æ­£æ€åˆ†å¸ƒ ï¼Œå³å½¢å¦‚f(x)=12Ï€Ïƒ2eâˆ’(xâˆ’Î¼)22Ïƒ2 ï¼Œå…¶ä¸­Î¼æ˜¯å‡å€¼ï¼ŒÏƒ2æ˜¯æ–¹å·®çš„åˆ†å¸ƒï¼‰æ··åˆè€Œæˆã€‚Î¸åŒ…å«äº†å¤šä¸ªé«˜æ–¯åˆ†å¸ƒçš„å‚æ•°ã€‚\n\n\nå¯ä»¥çœ‹åˆ°ï¼Œéšç€è¿­ä»£æ¬¡æ•°çš„å¢åŠ ï¼Œq(z)é€æ¸å¯ä»¥å‡†ç¡®çš„æ‹Ÿåˆå‡ºä¸åŒé¢œè‰²çš„ç‚¹ï¼Œä¹Ÿå°±æ˜¯æ‹Ÿåˆå‡ºäº†q(z|x)ã€‚\n\n\n\næ³¨æ„\nå°½ç®¡å˜åˆ†æ¨æ–­ï¼ˆVIï¼‰åœ¨è¿è¡Œé€Ÿåº¦ä¸Šè¦æ¯”MCMCå¿«ï¼Œä½†æ˜¯å…¶éœ€è¦å…ˆå¯¹åéªŒåˆ†å¸ƒè¿›è¡Œä¸€ä¸ªå‡è®¾ï¼Œå³å…¶å¾—åˆ°çš„è¿‘ä¼¼è§£æ²¡æœ‰MCMCæ–¹æ³•é€šè¿‡é‡‡æ ·å¾—åˆ°çš„è¿‘ä¼¼è§£å‡†ç¡®ã€‚\n\nå˜åˆ†è‡ªç¼–ç å™¨ï¼ˆVariational Auto - Encoderï¼ŒVAEï¼‰\nè‡ªç¼–ç å™¨\n\nè¾“å…¥å±‚ï¼ˆè“è‰²çš„xï¼‰å’Œè¾“å‡ºå±‚(æ©™è‰²çš„x^)å…·æœ‰ç›¸åŒçš„ç»´åº¦ã€‚z=e(x)æ˜¯ä¸€ä¸ªé™ç»´åçš„éšè—å±‚ï¼Œå…¶ç»´åº¦æ˜æ˜¾æ˜¯å°äºè¾“å…¥å±‚ä¸è¾“å‡ºå±‚ã€‚\né€šè¿‡æœ€å°åŒ–è¾“å…¥æ•°æ®å’Œè§£ç æ•°æ®ä¹‹é—´çš„é‡æ„è¯¯å·®ï¼ˆloss=||xâˆ’x^||2=||xâˆ’d(z)||2=||xâˆ’d(e(x))||2ï¼‰æ¥å­¦ä¹ å‚æ•°ã€‚\nç›¸æ¯”è¾ƒäºPCAï¼Œè‡ªåŠ¨ç¼–ç å™¨å› æ·±åº¦ç¥ç»ç½‘ç»œæ‰€å¸¦æ¥çš„éçº¿æ€§è€Œå…·æœ‰æ›´å¼ºçš„é™ç»´èƒ½åŠ›ã€‚\n::: tips\næç¤º\nPCAé™ç»´åçš„å‘é‡æ˜¯ä¸¥æ ¼æ­£äº¤çš„ã€‚\n:::\nä½†æ˜¯ï¼Œè¦æ³¨æ„çš„ä¸€ç‚¹æ˜¯ï¼Œåœ¨é™ç»´çš„æ—¶å€™åº”è¯¥å°½é‡ç¡®ä¿é™ç»´åçš„æ•°æ®ä¿å­˜äº†å°½å¯èƒ½å¤šçš„åŸå§‹æ•°æ®ä¿¡æ¯ã€‚è‡ªç¼–ç å™¨åœ¨éšè—å±‚è¿‡äºå°æˆ–è€…ç¼–ç å™¨è¿‡äºæ·±çš„æ—¶å€™å¯èƒ½ä¼šå¯¼è‡´å› é™ç»´è€Œä¸¢å¤±å¤§é‡ä¿¡æ¯ï¼æ‰€ä»¥è¦é€‰æ‹©åˆé€‚æ·±åº¦çš„ç¼–ç å™¨ä¸åˆé€‚å°ºå¯¸çš„éšè—å±‚ã€‚\næ­¤å¤–ï¼Œå› ä¸ºè‡ªç¼–ç å™¨çš„è®­ç»ƒæ˜¯å°½å¯èƒ½å‡å°è¾“å…¥å±‚å’Œè¾“å‡ºå±‚ä¹‹é—´çš„è¯¯å·®ï¼Œæ‰€ä»¥å…¶å¹¶ä¸careé™ç»´åçš„éšè—å±‚ï¼ˆç»´åº¦å¯¹åº”äº†æ½œç©ºé—´ï¼‰æ˜¯å¦æœ‰æ„ä¹‰ã€‚è¿™å°†å¯¼è‡´ä»æ½œç©ºé—´é‡‡æ ·å¾—åˆ°çš„ç‚¹å¯èƒ½æ— æ³•å¯¹åº”åˆç†æœ‰æ„ä¹‰çš„æ•°æ®ã€‚å½“è®­ç»ƒå¥½åçš„è‡ªç¼–ç å™¨ç”¨äºæµ‹è¯•é›†æ—¶ï¼Œç”Ÿæˆçš„æ•°æ®å¯èƒ½ä¼šæ¯«æ— æ„ä¹‰ï¼ˆå¯ä»¥ç†è§£ä¸ºè¿‡æ‹Ÿåˆäºè®­ç»ƒæ•°æ®ï¼‰ã€‚\nVAEåŸºæœ¬æ€æƒ³\næ­¤æ–¹æ¡ˆå¼•å…¥äº†æ­£åˆ™åŒ–æ¥é¿å…è¿‡æ‹Ÿåˆï¼Œè¾“å…¥å±‚ç»è¿‡ç¼–ç åä¸å†æ˜¯ç‚¹ï¼Œè€Œæ˜¯æ¦‚ç‡åˆ†å¸ƒã€‚\n\nå›¾ä¸­çš„å·¦åˆ—æ˜¯ç”¨AEæ–¹æ¡ˆå®ç°çš„ç»“æœï¼Œå¯¹åº”äº†ç¬‘çš„ç¨‹åº¦ã€‚å³åˆ—æ˜¯VAEæ–¹æ¡ˆçš„ç»“æœï¼Œä¹Ÿæ˜¯å¯¹åº”äº†ç¬‘çš„ç¨‹åº¦ï¼Œä½†æ˜¯ç”¨æ¦‚ç‡åˆ†å¸ƒæ¥è¡¨è¾¾ã€‚\næ¦‚ç‡ç”Ÿæˆæ¨¡å‹\np(z|x)=p(x|z)p(z)p(x)é‡æ¸©ä¸€ä¸‹è´å¶æ–¯å…¬å¼ã€‚AEçš„æ–¹æ¡ˆæ˜¯åœ¨æ±‚MLEï¼ˆæœ€å¤§ä¼¼ç„¶ä¼°è®¡ï¼‰ã€‚è€ŒVAEæ–¹æ¡ˆæ˜¯åœ¨æ±‚MAPï¼ˆæœ€å¤§åéªŒä¼°è®¡ï¼‰ã€‚\nåœ¨æ±‚p(z|x)æœ€å¤§çš„æ—¶å€™å®é™…ä¸Šæ˜¯åœ¨æ±‚p(x|z)p(z)æœ€å¤§ï¼ˆå› ä¸ºp(x)ä¸zï¼‰æ— å…³ã€‚æ‰€ä»¥è¿›ä¸€æ­¥maxp(z|x)=maxp(x|z)p(z)ã€‚ä¸ºäº†æ–¹ä¾¿è®¡ç®—ï¼Œå¯¹p(x|z)p(z)å–å¯¹æ•°ï¼Œå¾—åˆ°lnâ¡p(x|z)p(z)=lnâ¡p(x|z)+lnâ¡p(z)ã€‚å½“p(z)ä¸ºæ‹‰æ™®æ‹‰æ–¯åˆ†å¸ƒæ¦‚ç‡å¯†åº¦å‡½æ•°æ—¶ï¼Œlnâ¡p(z)=âˆ’ln(2b)âˆ’|z|bï¼Œæ­¤æ—¶lnâ¡p(z) ç­‰ä»·äºå¼•å…¥äº†ä¸€ä¸ªä¸|z|ç›¸å…³çš„é¡¹ã€‚æ‰€ä»¥ï¼Œp(z)å¯ä»¥ç†è§£ä¸ºä¸€ä¸ªæ­£åˆ™é¡¹ï¼Œå…¶ç”¨æ¥é¿å…è¿‡æ‹Ÿåˆã€‚\n","categories":["Artificial Intelligence","Deep Learning"],"tags":["Deep Learning"]}]